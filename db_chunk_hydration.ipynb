{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83f47ba6",
   "metadata": {},
   "source": [
    "# Hydrate the `chunks` table\n",
    "\n",
    "This notebook chunks and inserts research papers into the database, for use in the baseline model. To make model comparisons apples-to-apples, we need to make sure that the papers represented in `chunks` are the same as those in `contributions`. So we:\n",
    "\n",
    "1. Load the research dataframe, which contains the full text of all papers\n",
    "1. Get a set of the DOI's in the `contributions` table to use as a control list\n",
    "1. Get the set of DOIS in the `chunks` table, and compute the set difference\n",
    "1. The remaining DOIs are in the research dataset but not yet in `chunks`, so chunk them and insert into the `chunks` table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "659ef850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Research len: 52618\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# examples_df = pd.read_json('data/dataset/nontrivial_llm.jsonl', lines=True)\n",
    "research_df = pd.read_json('data/preprocessed/research.jsonl', lines=True)\n",
    "\n",
    "# print(f\"Examples len: {len(examples_df)}\")\n",
    "print(f\"Research len: {len(research_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5797df3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================CONFIG=================================\n",
      "Database         User             Host                             Port            \n",
      "citelinedb       bbasseri         localhost                        5432            \n",
      "========================================================================\n",
      "Database version: ('PostgreSQL 17.5 (Homebrew) on aarch64-apple-darwin24.4.0, compiled by Apple clang version 17.0.0 (clang-1700.0.13.3), 64-bit',)\n",
      "Number of unique DOIs in chunks: 4864\n",
      "Number of unique DOIs in contributions: 6317\n",
      "Number of DOIs to chunk: 1453\n"
     ]
    }
   ],
   "source": [
    "from database.database import Database\n",
    "\n",
    "db = Database()\n",
    "db.test_connection()\n",
    "\n",
    "results = db.query(f\"SELECT DISTINCT doi FROM chunks\")\n",
    "existing_dois = {row[0] for row in results}\n",
    "print(f\"Number of unique DOIs in chunks: {len(existing_dois)}\")\n",
    "\n",
    "results = db.query(f\"SELECT DISTINCT doi FROM contributions\")\n",
    "contribution_dois = {row[0] for row in results}\n",
    "print(f\"Number of unique DOIs in contributions: {len(contribution_dois)}\")\n",
    "\n",
    "dois_to_chunk = set(contribution_dois) - set(existing_dois)\n",
    "print(f\"Number of DOIs to chunk: {len(dois_to_chunk)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65c98901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up splitter and embedder\n",
    "\n",
    "import torch\n",
    "from semantic_text_splitter import TextSplitter\n",
    "from Embedders import get_embedder\n",
    "\n",
    "splitter = TextSplitter(capacity=1500, overlap=150)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "embedder = get_embedder(\"BAAI/bge-large-en-v1.5\", device=device, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30dce439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=\"logs/chunk_hydration.log\",\n",
    "    filemode=\"w\",\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03891225",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing dois: 100%|██████████| 1453/1453 [52:51<00:00,  2.18s/it] \n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "def reconstruct_paper(example: pd.Series) -> str:\n",
    "    return f\"{example['title']}\\n\\nAbstract: {example['abstract']}\\n\\n{example['body']}\"\n",
    "\n",
    "for doi in tqdm(dois_to_chunk, desc=\"processing dois\"):\n",
    "    # Get the doi's full record from research df\n",
    "    record = None\n",
    "    try:\n",
    "        record = research_df[research_df['doi'] == doi].iloc[0]\n",
    "    except IndexError:\n",
    "        logging.error(f\"DOI {doi} not found in research_df\")\n",
    "        continue\n",
    "\n",
    "    paper = reconstruct_paper(record)\n",
    "    chunks = splitter.chunks(paper)\n",
    "    chunks = [chunk.strip().replace(\"\\x00\", \"\") for chunk in chunks if chunk.strip()]  # Remove null chars and empty chunks\n",
    "    embeddings = embedder(chunks)\n",
    "\n",
    "    for chunk, embedding in zip(chunks, embeddings):\n",
    "        with db.conn.cursor() as cursor:\n",
    "            cursor.execute(\n",
    "                f\"INSERT INTO chunks (embedding, text, doi, pubdate) VALUES (%s, %s, %s, %s)\",\n",
    "                (embedding, chunk, doi, record['pubdate'])\n",
    "            )\n",
    "    db.conn.commit()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d695347f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique DOIs in chunks after hydration: 6317\n",
      "Number of unique DOIs in contributions: 6317\n",
      "Same set of DOIS: True\n"
     ]
    }
   ],
   "source": [
    "# Check that the set of DOIs present in `chunks` is the same as those in `contributions`\n",
    "results = db.query(f\"SELECT DISTINCT doi FROM chunks\")\n",
    "chunk_dois = {row[0] for row in results}\n",
    "print(f\"Number of unique DOIs in chunks after hydration: {len(chunk_dois)}\")\n",
    "print(f\"Number of unique DOIs in contributions: {len(contribution_dois)}\")\n",
    "print(f\"Same set of DOIS: {chunk_dois == contribution_dois}\")\n",
    "assert chunk_dois == contribution_dois, \"DOIs in chunks do not match those in contributions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7baa16ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples len: 8959\n",
      "Examples after filtering: 8540\n"
     ]
    }
   ],
   "source": [
    "examples = pd.read_json('data/dataset/nontrivial_llm.jsonl', lines=True)\n",
    "print(f\"Examples len: {len(examples)}\")\n",
    "# Filter for only those examples where all citation_dois are in the contributions_dois\n",
    "examples = examples[examples['citation_dois'].apply(lambda x: all(doi in contribution_dois for doi in x))]\n",
    "print(f\"Examples after filtering: {len(examples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebf44e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples.to_json('data/dataset/nontrivial_no_longpapers.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a52bf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "citeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
