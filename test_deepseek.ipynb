{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34c4115b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================CONFIG=================================\n",
      "Database         User             Host                             Port            \n",
      "citelinedb       bbasseri         localhost                        5432            \n",
      "========================================================================\n",
      "Database version: ('PostgreSQL 17.5 (Homebrew) on aarch64-apple-darwin24.4.0, compiled by Apple clang version 17.0.0 (clang-1700.0.13.3), 64-bit',)\n"
     ]
    }
   ],
   "source": [
    "from database.database import Database\n",
    "\n",
    "\n",
    "db = Database()\n",
    "db.test_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58c7ee55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded samples: 10\n",
      "Embedder: BAAI/bge-small-en, device=mps, normalize=True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from Embedders import get_embedder\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "samples = pd.read_json(\"data/dataset/split/small_train.jsonl\", lines=True)\n",
    "bge_embedder = get_embedder(\"BAAI/bge-small-en\", device=device, normalize=True)\n",
    "print(f\"Loaded samples: {len(samples)}\")\n",
    "print(\"Embedder:\", bge_embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d02eee4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 citations, 27 results, 0.00% of citations in top-k\n",
      "1 citations, 35 results, 0.00% of citations in top-k\n",
      "4 citations, 19 results, 0.00% of citations in top-k\n",
      "3 citations, 32 results, 0.00% of citations in top-k\n",
      "1 citations, 31 results, 0.00% of citations in top-k\n",
      "1 citations, 27 results, 0.00% of citations in top-k\n",
      "2 citations, 22 results, 50.00% of citations in top-k\n",
      "1 citations, 19 results, 0.00% of citations in top-k\n",
      "1 citations, 26 results, 0.00% of citations in top-k\n",
      "1 citations, 17 results, 100.00% of citations in top-k\n"
     ]
    }
   ],
   "source": [
    "# Get the embedding of the sentence (no citation)\n",
    "for sample in samples.itertuples():\n",
    "    pubdate = sample.pubdate\n",
    "    query_vector = bge_embedder([sample.sent_no_cit])[0]\n",
    "\n",
    "    # Query the database for similar sentences\n",
    "    results = db.query_vector_column(\n",
    "        query_vector=query_vector,\n",
    "        table_name=\"lib\",\n",
    "        target_column=\"bge_norm\",\n",
    "        metric=\"vector_cosine_ops\",\n",
    "        pubdate=pubdate,\n",
    "        use_index=True,\n",
    "        top_k=40,\n",
    "        ef_search=40\n",
    "    )\n",
    "\n",
    "    result_dois = set(result.doi for result in results)\n",
    "    target_dois = set(sample.citation_dois)\n",
    "\n",
    "    all_citations_in_top_k = target_dois.issubset(result_dois)\n",
    "    pct_in_top_k = len(target_dois.intersection(result_dois)) / len(target_dois) if target_dois else 0\n",
    "    print(f\"{len(target_dois)} citations, {len(result_dois)} results, \"\n",
    "          f\"{pct_in_top_k:.2%} of citations in top-k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dd682f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = samples.iloc[9]\n",
    "print(f\"Sample 9: {sample.sent_no_cit}\")\n",
    "print(f\"Sample 9 DOIs: {sample.citation_dois}\")\n",
    "\n",
    "query_results = db.query_vector_column(\n",
    "    query_vector=bge_embedder([sample.sent_no_cit])[0],\n",
    "    table_name=\"lib\",\n",
    "    target_column=\"bge_norm\",\n",
    "    metric=\"vector_cosine_ops\",\n",
    "    pubdate=sample.pubdate,\n",
    "    use_index=True,\n",
    "    top_k=20,\n",
    "    ef_search=1000\n",
    ")\n",
    "print(f\"Sample 9 query results: {len(query_results)}\")\n",
    "unique_dois = set(result.doi for result in query_results)\n",
    "print(f\"Sample 9 unique DOIs in results: {len(unique_dois)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560ad61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique dois as a list (to preserve order)\n",
    "unique_dois_list = []\n",
    "for result in query_results:\n",
    "    if result.doi not in unique_dois_list:\n",
    "        unique_dois_list.append(result.doi)\n",
    "print(unique_dois_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32813a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the candidate papers for each doi\n",
    "candidates = [db.get_paper_by_doi(doi) for doi in unique_dois_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35228474",
   "metadata": {},
   "outputs": [],
   "source": [
    "from apis.openai_client import deepseek_citation_validator_using_openai\n",
    "\n",
    "validation_responses = deepseek_citation_validator_using_openai(query=sample.sent_no_cit, candidates=candidates)\n",
    "\n",
    "for res in validation_responses:\n",
    "    print(f\"Validation response: {res}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279d967a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = validation_responses[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0967bacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for val in vals:\n",
    "    print(val.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c33bf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unique_dois_list[:2])\n",
    "print(sample.citation_dois)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9cd715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from apis.openai_client import deepseek_citation_validator\n",
    "\n",
    "# deepseek_results = deepseek_citation_validator(query=sample.sent_no_cit, candidates=candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ace2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "assert \"DEEPSEEK_API_KEY\" in os.environ, \"Please set DEEPSEEK_API_KEY in your environment variables.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d4bbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.environ[\"DEEPSEEK_API_KEY\"],\n",
    "    base_url=\"https://api.deepseek.com\",\n",
    ")\n",
    "\n",
    "with open(\"llm/prompts/deepseek_citation_identification.txt\", \"r\") as f:\n",
    "    DEEPSEEK_CITATION_IDENTIFICATION_PROMPT = f.read()\n",
    "\n",
    "OUTPUT_FORMAT = {\n",
    "    \"type\": \"json_schema\",\n",
    "    \"json_schema\": {\n",
    "        \"type\": \"json_object\",\n",
    "        \"properties\": {\"should_cite\": {\"type\": \"boolean\"}},\n",
    "        \"required\": [\"should_cite\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "def ds_formatted(query, candidates):\n",
    "    prompts = [\n",
    "        DEEPSEEK_CITATION_IDENTIFICATION_PROMPT.format(sentence=query, paper=paper)\n",
    "        for paper in candidates\n",
    "    ]\n",
    "    results = [\n",
    "        client.chat.completions.create(\n",
    "            model=\"deepseek-chat\",\n",
    "            temperature=0.0,\n",
    "            messages=[{\"role\": \"system\", \"content\": prompt}],\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "            stream=False,\n",
    "        )\n",
    "        for prompt in prompts\n",
    "    ]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fb4983",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_results = ds_formatted(query=sample.sent_no_cit, candidates=candidates[:1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7d1c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trial_results[0].choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11db99ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "citeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
