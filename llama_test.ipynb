{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/sentence_segmented/Astro_Reviews.json: 996/996 have all required keys\n",
      "352\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import os\n",
    "import re\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from utils import load_dataset\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_ACCESS_TOKEN\")\n",
    "\n",
    "astro_reviews = load_dataset(\"data/sentence_segmented/Astro_Reviews.json\")\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available(\n",
    ") else 'mps' if torch.mps.is_available() else 'cpu'\n",
    "DEVICE = list(range(torch.cuda.device_count())) if DEVICE == 'cuda' else DEVICE\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc9a85fd59404cfa8515dfa3fee21bcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipe pad token id: 128009\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\npipe(messages) -> list of one item\\n[{'generated_text': [dicts]}]\\neach inner dict has {'content': ..., 'role': ...}\\nThe 'response' is then\\nresponse[0]['generated_text'][-1]['content']\\n\""
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-3.2-3B-Instruct\", device_map='auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\", device_map='auto', padding_side='left')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "pipe = pipeline(\"text-generation\",\n",
    "                # model=\"meta-llama/Llama-3.2-3B-Instruct\", \n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                max_new_tokens=100,\n",
    "                temperature=0.01,\n",
    "                # device=DEVICE,\n",
    "                device_map='auto',\n",
    "                token=HF_TOKEN)\n",
    "# pipe.tokenizer.pad_token_id = model.config.eos_token_id\n",
    "print(f\"Pipe pad token id: {pipe.tokenizer.pad_token_id}\")\n",
    "\"\"\"\n",
    "pipe(messages) -> list of one item\n",
    "[{'generated_text': [dicts]}]\n",
    "each inner dict has {'content': ..., 'role': ...}\n",
    "The 'response' is then\n",
    "response[0]['generated_text'][-1]['content']\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Extract and output ONLY the inline citations from the text below as a list of tuples\n",
    "- Each citation becomes a (string, int) tuple where the string is the first author's name and the int is the year\n",
    "- If there are no citations in the text, output []\n",
    "- Do not count citations 'in preparation' or lacking a year\n",
    "- Do not include any introductory text, explanations, or anything before or after the array\n",
    "\n",
    "Examples of inline citations:\n",
    "'''\n",
    "Sentence: \"Like Caffau et al. (2008a) , we have similar findings.\"\n",
    "Output: [('Caffau et al.', 2008)]\n",
    "\n",
    "Sentence: \"Methods for mixing below the convection zone are well understood ( Brun, Turck-Chièze Zahn 1999 , Charbonnel Talon 2005 ).\"\n",
    "Output: [('Brun', 1999), ('Charbonnel', 2005)]\n",
    "\n",
    "Sentence: \"Momentum balance gives an expression ( Fabian 1999 ; Di Matteo, Wilman Crawford 2002 ; King 2003 , 2005 )\"\n",
    "Output: [('Fabian', 1999), ('Di Matteo', 2002), ('King', 2003), ('King', 2005)]\n",
    "\n",
    "Sentence: \"In the early Universe, when the metal content was extremely low, enrichment by a single supernova could dominate preexisting metal contents (e.g., Audouse Silk 1995 ; Ryan, Norris Beers 1996 ).\"\n",
    "Output: [('Audouse', 1995), ('Ryan', 1996)]\n",
    "\n",
    "Sentence: \"This is consistent with previous results (Pereira et al., in preparation).\"\n",
    "Output: []\n",
    "'''\n",
    "\n",
    "Now extract the inline citations from the following text:\n",
    "'''\n",
    "{text}\n",
    "'''\n",
    "\n",
    "Output format: \n",
    "[('first author', year), ('first author', year), ...]\n",
    "\"\"\"\n",
    "text_1 = 'neglect the H collisions altogether based on the available atomic physics data for other elements, while others use the classical Drawin (1968) formula, possibly with a scaling factor S H that typically varies from 0 to 1. Holweger (2001) found log ε O = 8.71 ± 0.05 using the Holweger Müller (1974) model with granulation corrections'\n",
    "text_2 = 'AGN feedback features in many theoretical, numerical, and semianalytic simulations of galaxy growth and evolution (e.g., Kauffmann Haehnelt 2000 ; Granato et al. 2004 ; Di Matteo, Springel Hernquist 2005 ; Springel, Di Matteo Hernquist 2005 ; Bower et al. 2006 ; Croton et al. 2006 ; Hopkins et al. 2006 ; Ciotti, Ostriker Proga 2010 ; Scannapieco et al. 2012 ).'\n",
    "\n",
    "\n",
    "def format_prompt_for_pipe(prompt, text):\n",
    "    return [{\"role\": \"user\", \"content\": prompt.format(text=text)}]\n",
    "\n",
    "def get_pipe_response(pipe, prompt, text):\n",
    "    msg = format_prompt_for_pipe(prompt, text)\n",
    "    res = pipe(msg)\n",
    "    return res[0]['generated_text'][-1]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "LIST_PATTERN = re.compile(r\"\\[.*?\\]\")\n",
    "\n",
    "class ParseResponseError(Exception):\n",
    "    def __init__(self, match_group, exception):\n",
    "        super().__init__(f\"Error parsing response: {match_group}\")\n",
    "        self.match_group = match_group\n",
    "        self.exception = exception\n",
    "\n",
    "def parse_response(response):\n",
    "    match = re.search(LIST_PATTERN, response)\n",
    "    if not match:\n",
    "        return []\n",
    "    try:\n",
    "        lst = ast.literal_eval(match.group())\n",
    "        return lst\n",
    "    except Exception as e:\n",
    "        raise ParseResponseError(match.group(), e)\n",
    "\n",
    "def citations_from_sentence(sentence):\n",
    "    try:\n",
    "        res = parse_response(get_pipe_response(pipe, prompt, sentence))\n",
    "        if res == []:\n",
    "            with open('no_citation_sentences.csv', 'a') as f:\n",
    "                csv.writer(f).writerow([sentence])\n",
    "        else:\n",
    "            print('in nonempty branch')\n",
    "            with open('citation_sentences.csv', 'a') as f:\n",
    "                csv.writer(f).writerow([res, sentence])\n",
    "    except ParseResponseError as e:\n",
    "        print(e)\n",
    "        with open('error_citation_sentences.csv', 'a') as f:\n",
    "            csv.writer(f).writerow([e.match_group, sentence, e.exception])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in nonempty branch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in nonempty branch\n",
      "Error parsing response: [('']\n"
     ]
    }
   ],
   "source": [
    "# for sentence in astro_reviews[0]['body_sentences'][:100]:\n",
    "#     citations_from_sentence(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input'],\n",
      "    num_rows: 64\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "input_messages = [format_prompt_for_pipe(prompt, sentence) for sentence in astro_reviews[0]['body_sentences'][10:74]]\n",
    "input_messages.sort(key = lambda x: len(x[0]['content']))\n",
    "dataset = Dataset.from_list([{'input': msg} for msg in input_messages])\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = pipe(dataset['input'], batch_size=8, num_return_sequences=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pipe(dataset['input'], batch_size=16, num_return_sequences=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "citeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
