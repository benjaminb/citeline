{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "import requests\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so the user is asking how cold is too cold to jog. Hmm, I need to figure out what factors influence this and provide a clear answer.\n",
      "\n",
      "First, I should consider the main variables that affect whether jogging is possible in cold weather. Temperature isn't the only factor; windchill also plays a role because it makes it feel colder than the actual temperature. Then there's humidity since sweat can make the air feel more uncomfortable, especially if someone is sweating a lot.\n",
      "\n",
      "Age and fitness level are important too. Older people or those with pre-existing conditions might not handle cold as well, so even mild colds could be an issue. Similarly, someone who's less fit might get wind exhaustion easier in colder temps.\n",
      "\n",
      "Clothing plays a big role here. Lightweight clothing can trap heat differently than heavier fabrics. If the clothes are thin, they don't insulate well, making it harder to stay warm when it's cold. Also, moisture-wicking materials help keep you dry and prevent chafing, which is crucial in cold conditions.\n",
      "\n",
      "Hydration is another factor because staying too long in cold weather can lead to dehydrating, especially if someone is exercising. So drinking water before jogging helps maintain levels and prevents issues like cramps or heat exhaustion.\n",
      "\n",
      "Footwear matters as well. Properly fitting shoes with good insulation prevent the feet from getting cold, which can cause other problems like shin splints or knee issues during jogging.\n",
      "\n",
      "The intensity of the jog also affects how much heat is generated through sweating. If someone is running hard, they'll be generating more heat, so their body needs to work harder to keep warm, increasing the risk of hypothermia if it's too cold.\n",
      "\n",
      "Putting this all together, I can outline safe temperature ranges for different scenarios, like moderate and vigorous activity, and suggest tips on how to stay warm without being overexerted. It's important to balance the information so the user understands both the thresholds and practical advice.\n",
      "</think>\n",
      "\n",
      "How cold is too cold for jogging depends on several factors, including your age, fitness level, and tolerance of cold weather. However, here are some general guidelines:\n",
      "\n",
      "### Temperature Thresholds:\n",
      "- **Moderate Jogging:** Generally, moderate jogging (6-8 mph) can be done comfortably in temperatures ranging from 20°C (68°F) down to around -5°C (-20°F). At these lower temps, you may need to use bundling clothing or a windbreaker for extra warmth.\n",
      "- **Vigorous Jogging:** Exercising vigorously at higher speeds (8-10 mph) is typically advised only when temperatures are above 10°C (50°F), as the increased heart rate and breathing can lead to hypothermia if it's too cold.\n",
      "\n",
      "### Windchill and Humidity:\n",
      "- **Windchill:** Wind chill makes it feel colder than the actual temperature, so even a slight dip in temp can enhance discomfort.\n",
      "- **Humidity:** High humidity can make it harder to dry perspiration, leading to discomfort or heat exhaustion, especially when combined with cold temps.\n",
      "\n",
      "### Individual Factors:\n",
      "- **Age and Fitness:** Older individuals and those with pre-existing conditions may experience more significant issues at lower temperatures.\n",
      "- **Clothing:** Properly layering clothing is crucial. Lightweight fabrics trap less heat than heavier materials, while moisture-wicking fabrics prevent chafing and keep you dry.\n",
      "\n",
      "### Hydration and Footwear:\n",
      "- **Hydration:** Staying hydrated is essential to avoid dehydration, which can worsen cold weather exercise.\n",
      "- **Footwear:** Properly fitting shoes with good insulation help maintain warmth without causing issues like shin splints or knee problems.\n",
      "\n",
      "### Tips for Cold Jogging:\n",
      "- **Warm clothing:** Always wear layers, including a lightweight jacket and gloves.\n",
      "- **Hydration:** Drink water before jogging to stay hydrated.\n",
      "- **Footwear:** Choose comfortable, insulated shoes.\n",
      "- **Gradual adaptation:** If you're new to cold weather jogging, start at higher temps and gradually decrease as your body adapts.\n",
      "\n",
      "In summary, while moderate jogging can be done down to around -5°C (-20°F), vigorous exercise is best avoided in temperatures below 10°C (50°F) due to the increased risk of hypothermia. Always assess personal comfort and adjust accordingly.\n"
     ]
    }
   ],
   "source": [
    "def ask_ollama(prompt):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": \"deepseek-r1\",\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, json=payload)\n",
    "    result = response.json()\n",
    "    return result['response']\n",
    "\n",
    "print(ask_ollama(\"How cold is too cold to jog?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('testbody.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "result = ask_ollama('Organize the following text into an array of paragraphs:\\n\\n' + text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I need to figure out how to organize this big chunk of text about solar chemical composition into something more readable, probably using headings and sections. The original text is pretty dense with information, especially about the challenges in determining the Sun's elemental makeup.\n",
      "\n",
      "First, I should read through it carefully to understand the main points. It talks about recent studies comparing different models, the uncertainties due to assumptions like LTE or local thermodynamic equilibrium, and mentions specific elements like Mg where transition probabilities are uncertain. There’s also a lot about non-LTE effects and how they can impact abundance determinations.\n",
      "\n",
      "I notice sections with headings that might already be present, but maybe some need better structuring. The text starts by mentioning the Solar Neighborhood Study (SNS) and other projects comparing models. Then it dives into uncertainties from LTE assumptions, details on specific elements like Mg, and discusses future challenges such as non-LTE effects and better atomic data.\n",
      "\n",
      "I think organizing this into main sections would help. Perhaps starting with an introduction about the Sun's composition uncertainty, then sections on the SNS and model comparisons, uncertainties due to LTE, issues with transition probabilities, non-LTE effects, and future efforts. Each section can have subsections for more detail.\n",
      "\n",
      "Wait, in the original text, there’s a part about the Sun having a mean T of ~5780K but actual structure varying between 4900-6200K and a density gradient from 1 to ~150 g/cm³. That might fit under model uncertainties or perhaps an issue with local assumptions.\n",
      "\n",
      "Also, the text mentions that current models assume LTE except for strong lines, which is maybe a point in the LTE section.\n",
      "\n",
      "I should also consider including future efforts and challenges as their own sections since they’re comprehensive topics.\n",
      "\n",
      "So, putting it all together:\n",
      "\n",
      "1. Introduction to Solar Chemical Composition Uncertainty\n",
      "2. Solar Neighborhood Study (SNS) Comparisons\n",
      "   - Model Comparison with SNS\n",
      "   - Challenges in Determining Chemical Composition\n",
      "3. Uncertainties Due to Local Thermodynamic Equilibrium (LTE)\n",
      "4. Transition Probabilities and Atomic Data Issues\n",
      "5. Non-LTE Effects on Abundance Determinations\n",
      "6. Future Solar Studies and Needed Improvements\n",
      "\n",
      "Each of these sections can have subsections as needed, covering specific points like Mg's transition probabilities or detailed aspects of LTE assumptions.\n",
      "\n",
      "I should also make sure to reference the specific elements mentioned in the text, like Mg, Ca, Fe-peak elements, etc., to ensure all key points are covered.\n",
      "\n",
      "Now, considering the user’s previous query where they had a response with numbered sections and subsections, maybe I can follow a similar structure but perhaps more organized. Let me outline this step by step.\n",
      "</think>\n",
      "\n",
      "Certainly! Here's an organized presentation of the information about solar chemical composition uncertainty:\n",
      "\n",
      "---\n",
      "\n",
      "### Solar Chemical Composition Uncertainty: An Overview\n",
      "\n",
      "#### 1. Introduction to Solar Chemical Composition Uncertainty\n",
      "- **Overview**: The Sun's elemental composition is a cornerstone of astrophysics, yet uncertainties persist due to various modeling and observational challenges.\n",
      "- **Key Considerations**:\n",
      "  - Current models assume the Sun behaves as an ideal blackbody with uniform density and LTE (Local Thermodynamic Equilibrium).\n",
      "  - Observational diagnostics vary significantly between quiet and active solar regions.\n",
      "\n",
      "#### 2. Solar Neighborhood Study (SNS) Comparisons\n",
      "- **Comparative Analysis**: Recent studies like the SNS have shown discrepancies between models, such as differences in T_eff (~5780K) and density structures.\n",
      "- **Challenges**:\n",
      "  - The Sun's actual structure varies with a temperature gradient from ~4900K to ~6200K.\n",
      "  - Density increases from ~1g/cm³ at the core to ~150g/cm³ in the photosphere.\n",
      "\n",
      "#### 3. Uncertainties Due to Local Thermodynamic Equilibrium (LTE)\n",
      "- **LTE Assumptions**: Many models rely on LTE, which might not hold true under all solar conditions.\n",
      "- **Implications**:\n",
      "  - Departures from LTE can significantly affect abundance determinations, particularly in strong lines of elements like Mg.\n",
      "\n",
      "#### 4. Transition Probabilities and Atomic Data Issues\n",
      "- **Element-Specific Challenges**:\n",
      "  - **Mg**: Lines have transition probabilities of ~25–50%, necessitating better data.\n",
      "  - Other Elements: Transition probabilities are uncertain, impacting their abundance determinations.\n",
      "\n",
      "#### 5. Non-LTE Effects on Abundance Determinations\n",
      "- **Impact**: Departures from LTE can lead to corrections at the 0.05 dex level or more severe, affecting abundance calculations across elements.\n",
      "- **Current State**:\n",
      "  - Detailed non-LTE studies are scarce due to insufficient atomic data and computational demands.\n",
      "\n",
      "#### 6. Future Solar Studies and Needed Improvements\n",
      "- **Priorities**:\n",
      "  - Improve LTE assumptions where possible.\n",
      "  - Enhance atomic physicists' roles in providing accurate transition probabilities, especially for elements like Mg.\n",
      "  - Develop better models of the Sun's structure, including density variations and magnetic activity cycles.\n",
      "\n",
      "---\n",
      "\n",
      "This structured approach ensures clarity and comprehensiveness, addressing each aspect systematically.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['building_astrobert.pdf', 'Attention.pdf', 'gravitational_duals.pdf', 'BERT.pdf'])\n"
     ]
    }
   ],
   "source": [
    "filenames = os.listdir('data/pdf')\n",
    "papers = dict()\n",
    "\n",
    "for filename in filenames:\n",
    "    texts = []\n",
    "    with open(f'data/pdf/{filename}', 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "\n",
    "        for page_num in range(len(reader.pages)):\n",
    "            page = reader.pages[page_num]\n",
    "            text = page.extract_text()\n",
    "            texts.append(text)\n",
    "\n",
    "    full_text = '\\n'.join(texts)\n",
    "    papers[filename] = full_text\n",
    "print(papers.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have a research paper (or abstract) that I would like you to analyze. Your task is to extract and clearly list the paper's original results, claims, contributions, or innovations. Please focus on key points that highlight what is novel or significant about the research, avoiding general background information or restatements of existing knowledge.\n",
      "\n",
      "Provide the results/claims/contributions/innovations as a JSON string array, using concise and precise language. Ensure each item reflects the specific value the paper adds to its field.\n",
      "\n",
      "Example abstract:\n",
      "\"\"\"\n",
      "We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community\n",
      "\"\"\"\n",
      "\n",
      "Example output:\n",
      "[\n",
      "  \"Introduction of LLaMA, a collection of foundation language models ranging from 7B to 65B parameters.\",\n",
      "  \"Demonstration that state-of-the-art language models can be trained using publicly available datasets exclusively, avoiding proprietary and inaccessible datasets.\",\n",
      "  \"LLaMA-13B outperforms GPT-3 (175B) on most benchmarks.\",\n",
      "  \"LLaMA-65B is competitive with the best models, including Chinchilla-70B and PaLM-540B.\",\n",
      "  \"Release of all LLaMA models to the research community, promoting open research and accessibility.\"\n",
      "]\n",
      "\n",
      "Here is your text to analyze:\n",
      "BERT: Pre-training of Deep Bidirectional Transformers for\n",
      "Language Understanding\n",
      "Jacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\n",
      "Google AI Language\n",
      "fjacobdevlin,mingweichang,kentonl,kristout g@google.com\n",
      "Abstract\n",
      "We introduce a new language representa-\n",
      "tion model called BERT , which stands for\n",
      "Bidirectional Encoder Representations from\n",
      "Transformers. Unlike recent language repre-\n",
      "sentation models (Peters et al., 2018a; Rad-\n",
      "ford et al., 2018), BERT is designed to pre-\n",
      "train deep bidirectional representations from\n",
      "unlabeled text by jointly conditioning on both\n",
      "left and right context in all layers. As a re-\n",
      "sult, the pre-trained BERT model can be ﬁne-\n",
      "tuned with just one additional output layer\n",
      "to create state-of-the-art models for a wide\n",
      "range of tasks, such as question answering and\n",
      "language inference, without substantial task-\n",
      "speciﬁc architecture modiﬁcations.\n",
      "BERT is conceptually simple and empirically\n",
      "powerful. It obtains new state-of-the-art re-\n",
      "sults on eleven natural language processing\n",
      "tasks, including pushing the GLUE score to\n",
      "80.5% (7.7% point absolute improvement),\n",
      "MultiNLI accuracy to 86.7% (4.6% absolute\n",
      "improvement), SQuAD v1.1 question answer-\n",
      "ing Test F1 to 93.2 (1.5 point absolute im-\n",
      "provement) and SQuAD v2.0 Test F1 to 83.1\n",
      "(5.1 point absolute improvement).\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('prompts/abstract_to_claims_prompt.txt', 'r') as file:\n",
    "    template = file.read()\n",
    "prompt = template.format(text=papers['BERT.pdf'][:1306])\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Alright, I need to analyze the provided research paper abstract about BERT and extract its original results, claims, contributions, or innovations. Let's break it down step by step.\n",
      "\n",
      "First, the abstract introduces BERT as a new language representation model. It mentions that unlike previous models like those from Peters et al., Radford et al., BERT pre-trains deep bidirectional representations using unlabeled text. This joint conditioning on both left and right context in all layers is key. So, this is a significant innovation because it's simpler but more effective.\n",
      "\n",
      "Next, the abstract states that BERT can be fine-tuned with just an additional output layer for various tasks like question answering and NLP tasks without major changes. That's a clear contribution—it makes pre-training efficient and adaptable to different models.\n",
      "\n",
      "Then, it highlights BERT's performance improvements on several benchmarks. It mentions pushing the GLUE score by 7.7%, improving MultiNLI accuracy by 4.6%, SQuAD v1.1 F1 by 1.5 points, and SQuAD v2.0 F1 by 5.1 points. These are specific numerical improvements that clearly show BERT's effectiveness.\n",
      "\n",
      "Also, the paper notes that it achieves these results with state-of-the-art performance without relying on costly or inaccessible datasets, which is a significant point about accessibility.\n",
      "\n",
      "So, compiling all this, I'll list each key point as a separate JSON object within an array. Each should be concise and precise, highlighting what's novel or impactful without extra background.\n",
      "</think>\n",
      "\n",
      "```json\n",
      "[\n",
      "  \"Introduction of BERT, a new language representation model that pre-trains deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.\",\n",
      "  \"BERT can be fine-tuned with just one additional output layer to create state-of-the-art models for tasks such as question answering and language understanding without substantial task-specific architecture modifications.\",\n",
      "  \"BERT achieves significant performance improvements on several natural language processing benchmarks, including a 7.7% absolute improvement in the GLUE score, a 4.6% absolute improvement in MultiNLI accuracy, a 1.5 point absolute improvement in SQuAD v1.1 Test F1, and a 5.1 point absolute improvement in SQuAD v2.0 Test F1.\",\n",
      "  \"BERT demonstrates that it is possible to achieve state-of-the-art performance on these tasks without relying on expensive or inaccessible datasets.\"\n",
      "]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "response = ask_ollama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "[\n",
      "  \"Introduction of BERT, a new language representation model that pre-trains deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.\",\n",
      "  \"BERT can be fine-tuned with just one additional output layer to create state-of-the-art models for tasks such as question answering and language understanding without substantial task-specific architecture modifications.\",\n",
      "  \"BERT achieves significant performance improvements on several natural language processing benchmarks, including a 7.7% absolute improvement in the GLUE score, a 4.6% absolute improvement in MultiNLI accuracy, a 1.5 point absolute improvement in SQuAD v1.1 Test F1, and a 5.1 point absolute improvement in SQuAD v2.0 Test F1.\",\n",
      "  \"BERT demonstrates that it is possible to achieve state-of-the-art performance on these tasks without relying on expensive or inaccessible datasets.\"\n",
      "]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# For Deepseek-r1, the output has a <think>...</think> section we can trim\n",
    "def postprocess_deepseek(response):\n",
    "    return response.split('</think>')[1].strip()\n",
    "\n",
    "print(postprocess_deepseek(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The abstract introduces a new Transformer model that replaces traditional encoder-decoder architectures with solely attention mechanisms. This innovation allows for more efficient training, faster convergence, and improved performance in machine translation tasks compared to existing models like ensembles from literature. The model achieves state-of-the-art results on two translation benchmarks (WMT 2014 English-to-German and English-to-French) while requiring less training time. Additionally, it demonstrates versatility by successfully applying to English constituency parsing with both ample and limited data.\n",
      "\n",
      "```json\n",
      "[\n",
      "    \"Introducing the Transformer architecture, which replaces traditional encoder-decoder architectures with solely attention mechanisms.\",\n",
      "    \"Superior performance in machine translation tasks compared to existing models like ensembles from literature.\",\n",
      "    \"Achieves state-of-the-art single-model BLEU scores of 28.4 and 41.8 for English-to-German and English-to-French translations, respectively.\",\n",
      "    \"Faster training time due to more parallelizable architecture, achieving the best results in a fraction of the training costs of existing models.\",\n",
      "    \"Successful application to English constituency parsing tasks with both large and limited amounts of training data.\"\n",
      "]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "prompt = template.format(text=papers['Attention.pdf'][587:1737])\n",
    "print(postprocess_deepseek(ask_ollama(prompt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  \"Introduction of astroBERT, a deeply contextual language model trained on recent astronomy publications.\",\n",
      "  \"Application of machine learning and natural language processing techniques to enhance NASA's Astrophysics Data System (ADS) for semantic search.\",\n",
      "  \"Development of a named entity recognition tool within the context of astronomical datasets to improve discoverability.\",\n",
      "  \"Use of astroBERT to address ambiguity in search queries, such as distinguishing between various meanings of terms like 'Planck'.\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "prompt = template.format(text=papers['building_astrobert.pdf'][513:1427])\n",
    "print(postprocess_deepseek(ask_ollama(prompt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "[\n",
      "  \"Introduction of a novel method using physics-informed neural networks to solve an inverse problem in holography\",\n",
      "  \"Development of an algorithm that is both data-driven and informed by Einstein's equations for determining gravitational theories from a prescribed equation of state\",\n",
      "  \"Successful application of the approach to theories involving crossovers, first-order phase transitions, and second-order phase transitions\"\n",
      "]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "prompt = template.format(text=papers['gravitational_duals.pdf'][846:1653])\n",
    "print(postprocess_deepseek(ask_ollama(prompt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "citeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
