{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "import requests\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I'm trying to figure out how cold it's too cold to jog. I know that exercise is good for you in general, but when the weather gets really cold, I wonder if I should avoid going out for a run or bike ride. Let me think about this.\n",
      "\n",
      "First, I guess the main factors are comfort and health. If it's extremely cold, my body might not be able to generate enough heat through exercise, which could be bad for my muscles or heart. But how cold is too cold? I'm not sure what the exact temperatures are, but I know that below freezing can be a problem.\n",
      "\n",
      "I've heard that in the winter, people often use jackets and layers when exercising. Maybe that's why even if it's cold outside, you still go jogging inside if it's below a certain temperature. But how does that translate to outdoor activities?\n",
      "\n",
      "Then there are the health aspects. I remember reading somewhere that too cold can lead to cramps or even frostbite, which is really painful and can be dangerous. So maybe anything below 30°F (about -1°C) is too cold? Or is it lower than that?\n",
      "\n",
      "Also, personal tolerance plays a role. Some people might not mind being outside in very cold weather for exercise, while others might get cramps even at the same temperature because their bodies are more sensitive.\n",
      "\n",
      "Another thing to consider is the type of activity. Running and biking require more heat-generating muscles, so maybe they're more affected by cold than walking or swimming. For example, if it's below freezing when I'm running, my muscles might not warm up enough, leading to cramps or even frostbite on my extremities.\n",
      "\n",
      "I should also think about how long the run is and the weather conditions. If I go for a short run at low temperatures, maybe it's okay because there's less chance of cold accumulating in my body. But if it's a longer run when it's really freezing, that could be dangerous.\n",
      "\n",
      "So putting this all together, it seems like any temperature below 30°F (about -1°C) might not be ideal for jogging or biking, but personal comfort and health should guide individual decisions. Maybe as long as I'm wearing the right clothes and staying warm, I can handle a little colder weather without too many issues.\n",
      "\n",
      "I wonder if there are any guidelines from fitness communities or health organizations on this topic. That would help confirm my thoughts. Also, it's important to listen to my body and not push through if it feels uncomfortable.\n",
      "</think>\n",
      "\n",
      "The optimal temperature for jogging or biking is generally considered to be above 30°F (about -1°C). Below this threshold, the risk of discomfort such as cramps increases, and at very low temperatures (below freezing), frostbite can occur. Personal tolerance varies; some individuals may handle colder weather better than others due to differences in body composition or fitness levels.\n",
      "\n",
      "Key considerations include:\n",
      "\n",
      "1. **Clothing and Heat Retention**: Wear layers or jackets when the temperature drops below 30°F to maintain warmth, especially for activities that generate heat like jogging or biking.\n",
      "   \n",
      "2. **Weather Conditions**: The duration of exposure is important; short runs in cold weather may be manageable, but longer runs at freezing temperatures are riskier.\n",
      "\n",
      "3. **Personal Comfort and Health**: Listen to your body and adjust based on how you feel. If experiencing discomfort, it's better to stay home for that session.\n",
      "\n",
      "In summary, while some individuals can tolerate colder conditions for jogging or biking, the ideal temperature range is above 30°F, with clothing and personal health considerations playing significant roles in individual decision-making.\n"
     ]
    }
   ],
   "source": [
    "def ask_ollama(prompt):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": \"deepseek-r1\",\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, json=payload)\n",
    "    result = response.json()\n",
    "    return result['response']\n",
    "\n",
    "print(ask_ollama(\"How cold is too cold to jog?\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['building_astrobert.pdf', 'Attention.pdf', 'gravitational_duals.pdf', 'BERT.pdf'])\n"
     ]
    }
   ],
   "source": [
    "filenames = os.listdir('data/pdf')\n",
    "papers = dict()\n",
    "\n",
    "for filename in filenames:\n",
    "    texts = []\n",
    "    with open(f'data/pdf/{filename}', 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "\n",
    "        for page_num in range(len(reader.pages)):\n",
    "            page = reader.pages[page_num]\n",
    "            text = page.extract_text()\n",
    "            texts.append(text)\n",
    "\n",
    "    full_text = '\\n'.join(texts)\n",
    "    papers[filename] = full_text\n",
    "print(papers.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have a research paper (or abstract) that I would like you to analyze. Your task is to extract and clearly list the paper's original results, claims, contributions, or innovations. Please focus on key points that highlight what is novel or significant about the research, avoiding general background information or restatements of existing knowledge.\n",
      "\n",
      "Provide the results/claims/contributions/innovations as a JSON string array, using concise and precise language. Ensure each item reflects the specific value the paper adds to its field.\n",
      "\n",
      "Example abstract:\n",
      "\"\"\"\n",
      "We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community\n",
      "\"\"\"\n",
      "\n",
      "Example output:\n",
      "[\n",
      "  \"Introduction of LLaMA, a collection of foundation language models ranging from 7B to 65B parameters.\",\n",
      "  \"Demonstration that state-of-the-art language models can be trained using publicly available datasets exclusively, avoiding proprietary and inaccessible datasets.\",\n",
      "  \"LLaMA-13B outperforms GPT-3 (175B) on most benchmarks.\",\n",
      "  \"LLaMA-65B is competitive with the best models, including Chinchilla-70B and PaLM-540B.\",\n",
      "  \"Release of all LLaMA models to the research community, promoting open research and accessibility.\"\n",
      "]\n",
      "\n",
      "Here is your text to analyze:\n",
      "BERT: Pre-training of Deep Bidirectional Transformers for\n",
      "Language Understanding\n",
      "Jacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova\n",
      "Google AI Language\n",
      "fjacobdevlin,mingweichang,kentonl,kristout g@google.com\n",
      "Abstract\n",
      "We introduce a new language representa-\n",
      "tion model called BERT , which stands for\n",
      "Bidirectional Encoder Representations from\n",
      "Transformers. Unlike recent language repre-\n",
      "sentation models (Peters et al., 2018a; Rad-\n",
      "ford et al., 2018), BERT is designed to pre-\n",
      "train deep bidirectional representations from\n",
      "unlabeled text by jointly conditioning on both\n",
      "left and right context in all layers. As a re-\n",
      "sult, the pre-trained BERT model can be ﬁne-\n",
      "tuned with just one additional output layer\n",
      "to create state-of-the-art models for a wide\n",
      "range of tasks, such as question answering and\n",
      "language inference, without substantial task-\n",
      "speciﬁc architecture modiﬁcations.\n",
      "BERT is conceptually simple and empirically\n",
      "powerful. It obtains new state-of-the-art re-\n",
      "sults on eleven natural language processing\n",
      "tasks, including pushing the GLUE score to\n",
      "80.5% (7.7% point absolute improvement),\n",
      "MultiNLI accuracy to 86.7% (4.6% absolute\n",
      "improvement), SQuAD v1.1 question answer-\n",
      "ing Test F1 to 93.2 (1.5 point absolute im-\n",
      "provement) and SQuAD v2.0 Test F1 to 83.1\n",
      "(5.1 point absolute improvement).\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('prompts/abstract_to_claims_prompt.txt', 'r') as file:\n",
    "    template = file.read()\n",
    "prompt = template.format(text=papers['BERT.pdf'][:1306])\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Alright, I need to analyze the provided research paper abstract about BERT and extract its original results, claims, contributions, or innovations. Let's break it down step by step.\n",
      "\n",
      "First, the abstract introduces BERT as a new language representation model. It mentions that unlike previous models like those from Peters et al., Radford et al., BERT pre-trains deep bidirectional representations using unlabeled text. This joint conditioning on both left and right context in all layers is key. So, this is a significant innovation because it's simpler but more effective.\n",
      "\n",
      "Next, the abstract states that BERT can be fine-tuned with just an additional output layer for various tasks like question answering and NLP tasks without major changes. That's a clear contribution—it makes pre-training efficient and adaptable to different models.\n",
      "\n",
      "Then, it highlights BERT's performance improvements on several benchmarks. It mentions pushing the GLUE score by 7.7%, improving MultiNLI accuracy by 4.6%, SQuAD v1.1 F1 by 1.5 points, and SQuAD v2.0 F1 by 5.1 points. These are specific numerical improvements that clearly show BERT's effectiveness.\n",
      "\n",
      "Also, the paper notes that it achieves these results with state-of-the-art performance without relying on costly or inaccessible datasets, which is a significant point about accessibility.\n",
      "\n",
      "So, compiling all this, I'll list each key point as a separate JSON object within an array. Each should be concise and precise, highlighting what's novel or impactful without extra background.\n",
      "</think>\n",
      "\n",
      "```json\n",
      "[\n",
      "  \"Introduction of BERT, a new language representation model that pre-trains deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.\",\n",
      "  \"BERT can be fine-tuned with just one additional output layer to create state-of-the-art models for tasks such as question answering and language understanding without substantial task-specific architecture modifications.\",\n",
      "  \"BERT achieves significant performance improvements on several natural language processing benchmarks, including a 7.7% absolute improvement in the GLUE score, a 4.6% absolute improvement in MultiNLI accuracy, a 1.5 point absolute improvement in SQuAD v1.1 Test F1, and a 5.1 point absolute improvement in SQuAD v2.0 Test F1.\",\n",
      "  \"BERT demonstrates that it is possible to achieve state-of-the-art performance on these tasks without relying on expensive or inaccessible datasets.\"\n",
      "]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "response = ask_ollama(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "[\n",
      "  \"Introduction of BERT, a new language representation model that pre-trains deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.\",\n",
      "  \"BERT can be fine-tuned with just one additional output layer to create state-of-the-art models for tasks such as question answering and language understanding without substantial task-specific architecture modifications.\",\n",
      "  \"BERT achieves significant performance improvements on several natural language processing benchmarks, including a 7.7% absolute improvement in the GLUE score, a 4.6% absolute improvement in MultiNLI accuracy, a 1.5 point absolute improvement in SQuAD v1.1 Test F1, and a 5.1 point absolute improvement in SQuAD v2.0 Test F1.\",\n",
      "  \"BERT demonstrates that it is possible to achieve state-of-the-art performance on these tasks without relying on expensive or inaccessible datasets.\"\n",
      "]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# For Deepseek-r1, the output has a <think>...</think> section we can trim\n",
    "def postprocess_deepseek(response):\n",
    "    return response.split('</think>')[1].strip()\n",
    "\n",
    "print(postprocess_deepseek(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The abstract introduces a new Transformer model that replaces traditional encoder-decoder architectures with solely attention mechanisms. This innovation allows for more efficient training, faster convergence, and improved performance in machine translation tasks compared to existing models like ensembles from literature. The model achieves state-of-the-art results on two translation benchmarks (WMT 2014 English-to-German and English-to-French) while requiring less training time. Additionally, it demonstrates versatility by successfully applying to English constituency parsing with both ample and limited data.\n",
      "\n",
      "```json\n",
      "[\n",
      "    \"Introducing the Transformer architecture, which replaces traditional encoder-decoder architectures with solely attention mechanisms.\",\n",
      "    \"Superior performance in machine translation tasks compared to existing models like ensembles from literature.\",\n",
      "    \"Achieves state-of-the-art single-model BLEU scores of 28.4 and 41.8 for English-to-German and English-to-French translations, respectively.\",\n",
      "    \"Faster training time due to more parallelizable architecture, achieving the best results in a fraction of the training costs of existing models.\",\n",
      "    \"Successful application to English constituency parsing tasks with both large and limited amounts of training data.\"\n",
      "]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "prompt = template.format(text=papers['Attention.pdf'][587:1737])\n",
    "print(postprocess_deepseek(ask_ollama(prompt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  \"Introduction of astroBERT, a deeply contextual language model trained on recent astronomy publications.\",\n",
      "  \"Application of machine learning and natural language processing techniques to enhance NASA's Astrophysics Data System (ADS) for semantic search.\",\n",
      "  \"Development of a named entity recognition tool within the context of astronomical datasets to improve discoverability.\",\n",
      "  \"Use of astroBERT to address ambiguity in search queries, such as distinguishing between various meanings of terms like 'Planck'.\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "prompt = template.format(text=papers['building_astrobert.pdf'][513:1427])\n",
    "print(postprocess_deepseek(ask_ollama(prompt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "[\n",
      "  \"Introduction of a novel method using physics-informed neural networks to solve an inverse problem in holography\",\n",
      "  \"Development of an algorithm that is both data-driven and informed by Einstein's equations for determining gravitational theories from a prescribed equation of state\",\n",
      "  \"Successful application of the approach to theories involving crossovers, first-order phase transitions, and second-order phase transitions\"\n",
      "]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "prompt = template.format(text=papers['gravitational_duals.pdf'][846:1653])\n",
    "print(postprocess_deepseek(ask_ollama(prompt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "citeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
