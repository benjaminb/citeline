{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name '__file__' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m time\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Add the parent directory to sys.path\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;18;43m__file__\u001b[39;49m))))\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# fmt: off\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n",
            "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import psycopg2\n",
        "from psycopg2.extras import execute_values\n",
        "import sys\n",
        "import torch\n",
        "from collections import namedtuple, defaultdict\n",
        "from dotenv import load_dotenv\n",
        "from pgvector.psycopg2 import register_vector\n",
        "from semantic_text_splitter import TextSplitter\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed\n",
        "from time import time\n",
        "\n",
        "# Add the parent directory to sys.path\n",
        "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
        "# fmt: off\n",
        "from utils import load_dataset\n",
        "from embedding_functions import get_embedding_fn \n",
        "# fmt: on\n",
        "\n",
        "PGVECTOR_DISTANCE_METRICS = {\n",
        "    'vector_l2_ops': '<->',\n",
        "    'vector_ip_ops': '<#>',\n",
        "    'vector_cosine_ops': '<=>',\n",
        "}\n",
        "\n",
        "\n",
        "class Database:\n",
        "    def __init__(self, db_params):\n",
        "        self.db_params = db_params\n",
        "        self.device = 'cuda' if torch.cuda.is_available(\n",
        "        ) else 'mps' if torch.mps.is_available() else 'cpu'\n",
        "\n",
        "    def __remove_nul_chars(self, s: str) -> str:\n",
        "        \"\"\"Remove NUL characters from a string, which PostgreSQL does not like\"\"\"\n",
        "        return s.replace('\\x00', '')\n",
        "\n",
        "    def _insert_chunk(self, text: str, doi: str):\n",
        "        text = self.__remove_nul_chars(text)\n",
        "        conn = psycopg2.connect(**self.db_params)\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute(\n",
        "            \"INSERT INTO chunks (text, doi) VALUES (%s, %s);\", (text, doi))\n",
        "        conn.commit()\n",
        "        cursor.close()\n",
        "        conn.close()\n",
        "\n",
        "    def _chunk_record(self, record: dict, max_length: int, overlap: int):\n",
        "        splitter = TextSplitter(capacity=max_length, overlap=overlap)\n",
        "        full_text = record['title'] + '\\n\\nABSTRACT:\\n' + \\\n",
        "            record['abstract'] + '\\n\\n' + record['body']\n",
        "        chunks = splitter.chunks(full_text)\n",
        "        doi = record['doi'][0]\n",
        "        return [(chunk, doi) for chunk in chunks]\n",
        "\n",
        "    def chunk_and_insert_records(self, path: str, max_length: int = 1500, overlap: int = 150):\n",
        "        records = load_dataset(path)\n",
        "        all_chunks = []\n",
        "\n",
        "        # Use ProcessPoolExecutor to parallelize chunking\n",
        "        with ProcessPoolExecutor(max_workers=os.cpu_count()) as process_executor:\n",
        "            futures = [process_executor.submit(\n",
        "                self._chunk_record, record, max_length, overlap) for record in records]\n",
        "            for future in tqdm(as_completed(futures), total=len(futures), desc=\"Chunking records\"):\n",
        "                all_chunks.extend(future.result())  # Collect all chunks\n",
        "\n",
        "        # Use ThreadPoolExecutor to parallelize inserting chunks\n",
        "        with ThreadPoolExecutor(max_workers=os.cpu_count()) as thread_executor:\n",
        "            futures = [thread_executor.submit(\n",
        "                self._insert_chunk, chunk, doi) for chunk, doi in all_chunks]\n",
        "            for future in tqdm(as_completed(futures), total=len(futures), desc=\"Inserting chunks\"):\n",
        "                future.result()  # This will raise any exceptions caught during processing\n",
        "\n",
        "    def create_vector_table(self, name, dim, embedder):\n",
        "        \"\"\"\n",
        "        1. Creates a vector table \n",
        "        2. Creates indexes for all distance metrics\n",
        "        3. Batch embeds all records in the `chunks` table using the given embedder\n",
        "\n",
        "        available distance metrics:\n",
        "        vector_l2_ops, vector_ip_ops, vector_cosine_ops, vector_l1_ops, bit_hamming_ops, bit_jaccard_ops\n",
        "        \"\"\"\n",
        "        conn = psycopg2.connect(**self.db_params)\n",
        "        register_vector(conn)\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute(\n",
        "            f\"\"\"CREATE TABLE {name} (\n",
        "                id SERIAL PRIMARY KEY, \n",
        "                embedding VECTOR({dim}), \n",
        "                chunk_id INTEGER REFERENCES chunks(id)\n",
        "                );\n",
        "            \"\"\")\n",
        "        conn.commit()\n",
        "        print(f\"Created table {name}\")\n",
        "\n",
        "        # Create indexes\n",
        "        for metric in PGVECTOR_DISTANCE_METRICS:\n",
        "            cursor.execute(\n",
        "                f\"CREATE INDEX ON {name} USING hnsw (embedding {metric})\")\n",
        "        conn.commit()\n",
        "\n",
        "        # Get all chunks for embedding\n",
        "        ids_and_chunks = self._get_all_chunks(cursor)\n",
        "        print(f\"Embedding {len(ids_and_chunks)} chunks...\")\n",
        "\n",
        "        # Embed an insert in batches\n",
        "        batch_size = 16\n",
        "        num_batches = len(ids_and_chunks) // batch_size\n",
        "        for i in tqdm(range(num_batches), desc=\"Inserting embeddings\", leave=False):\n",
        "            # Prepare batch\n",
        "            batch = ids_and_chunks[i * batch_size:(i + 1) * batch_size]\n",
        "            ids, texts = list(zip(*batch))\n",
        "            embeddings = embedder(texts)\n",
        "            data = [(embedding, id_num)\n",
        "                    for embedding, id_num in zip(embeddings, ids)]\n",
        "\n",
        "            # Insert\n",
        "            execute_values(\n",
        "                cursor, f\"INSERT INTO {name} (embedding, chunk_id) VALUES %s;\", data)\n",
        "            conn.commit()\n",
        "        cursor.close()\n",
        "\n",
        "    def _get_all_chunks(self, cursor, columns: list[str] = ['id', 'text']) -> list[dict]:\n",
        "        cursor.execute(f\"SELECT id, text FROM chunks;\")\n",
        "        return cursor.fetchall()\n",
        "\n",
        "    def query_vector_table(self, target_table, query_vector, metric, top_k=5):\n",
        "        \"\"\"\n",
        "        target_table: name of the vector table\n",
        "        query_vector: the vector to query\n",
        "        metric: a key in PGVECTOR_DISTANCE_METRICS to resolve the distance operator\n",
        "        top_k: number of results to return\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # Resolve the distance operator\n",
        "        assert metric in PGVECTOR_DISTANCE_METRICS, f\"Invalid metric: {metric}. I don't have that metric in the PGVECTOR_DISTANCE_METRICS dictionary\"\n",
        "        operator = PGVECTOR_DISTANCE_METRICS[metric]\n",
        "\n",
        "        conn = psycopg2.connect(**self.db_params)\n",
        "        register_vector(conn)\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute(\n",
        "            f\"\"\"\n",
        "            SELECT {target_table}.chunk_id, chunks.doi, chunks.text, {target_table}.embedding {operator} %s AS distance \n",
        "            FROM {target_table} \n",
        "            JOIN chunks ON {target_table}.chunk_id = chunks.id\n",
        "            ORDER BY embedding {operator} %s DESC \n",
        "            LIMIT %s;\n",
        "            \"\"\",\n",
        "            (query_vector, query_vector, top_k)\n",
        "        )\n",
        "        results = cursor.fetchall()\n",
        "        cursor.close()\n",
        "\n",
        "        # Define the named tuple\n",
        "        QueryResult = namedtuple(\n",
        "            'QueryResult', ['chunk_id', 'doi', 'text', 'distance'])\n",
        "        return [QueryResult(*result) for result in results]\n",
        "\n",
        "    def test_connection(self):\n",
        "        conn = psycopg2.connect(**self.db_params)\n",
        "        cursor = conn.cursor()\n",
        "\n",
        "        # Execute a simple query\n",
        "        cursor.execute(\"SELECT version();\")\n",
        "        db_version = cursor.fetchone()\n",
        "        print(f\"Database version: {db_version}\")\n",
        "\n",
        "        cursor.close()\n",
        "        conn.close()\n",
        "\n",
        "load_dotenv('../.env')\n",
        "db_params = {\n",
        "    'dbname': 'citeline_db',\n",
        "    'user': os.getenv('DB_USER'),\n",
        "    'password': os.getenv('DB_PASSWORD'),\n",
        "    'host': os.getenv('DB_HOST'),\n",
        "    'port': os.getenv('DB_PORT')\n",
        "}\n",
        "db = Database(db_params)\n",
        "print(\"Testing db connection...\")\n",
        "db.test_connection()\n",
        "print(\"=====================================\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "citeline",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
