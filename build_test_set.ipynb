{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Earth_Science_Reviews', 'Planetary_Reviews', 'Astro_Reviews'])\n",
      "Journal keys: dict_keys(['documents', 'metadatas', 'ids'])\n",
      "dict_keys(['bibcode', 'abstract', 'aff', 'author', 'bibstem', 'doctype', 'doi', 'id', 'keyword', 'pubdate', 'title', 'read_count', 'reference', 'citation_count', 'citation', 'body'])\n",
      "['10.1146/annurev.astro.46.060407.145222', '10.48550/arXiv.0909.0948']\n",
      "<class 'list'>\n",
      "['1929ApJ....70...11R', '1956RvMP...28...53S', '1958ZA.....46..108B']\n",
      "2984\n",
      "dict_keys(['bibcode', 'abstract', 'aff', 'author', 'bibstem', 'doctype', 'doi', 'id', 'keyword', 'pubdate', 'title', 'read_count', 'reference', 'citation_count', 'citation', 'body'])\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "\n",
    "PATH_TO_DATA = 'data/processed_for_chroma/reviews'\n",
    "FILENAMES = os.listdir(PATH_TO_DATA)\n",
    "review_data = dict()\n",
    "\n",
    "for filename in FILENAMES:\n",
    "    with open(f'{PATH_TO_DATA}/{filename}', 'r') as file:\n",
    "        review_data[os.path.splitext(os.path.basename(filename))[0]] = json.load(file)\n",
    "\n",
    "# This one is missing the doi key\n",
    "del review_data['Earth_Science_Reviews']['metadatas'][292]\n",
    "del review_data['Earth_Science_Reviews']['documents'][292]\n",
    "del review_data['Earth_Science_Reviews']['ids'][292]\n",
    "\n",
    "# postprocessing\n",
    "for journal in review_data:\n",
    "    for i, d in enumerate(review_data[journal]['metadatas']):\n",
    "        # Convert stringified list to list\n",
    "        d['reference'] = json.loads(d['reference'])\n",
    "        d['doi'] = json.loads(d['doi'])\n",
    "\n",
    "print(review_data.keys())\n",
    "print(f\"Journal keys: {review_data['Astro_Reviews'].keys()}\")\n",
    "paper = review_data['Astro_Reviews']['metadatas'][0]\n",
    "print(paper.keys())\n",
    "print(paper['doi'])\n",
    "print(type(paper['doi']))\n",
    "print(paper['reference'][:3])\n",
    "all_reviews = [\n",
    "    record for journal in review_data for record in review_data[journal]['metadatas']]\n",
    "print(len(all_reviews))\n",
    "print(all_reviews[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Earth_Science_Research', 'Planetary_Research', 'Astro_Research'])\n",
      "All data: 5968\n"
     ]
    }
   ],
   "source": [
    "PATH_TO_DATA = 'data/processed_for_chroma/research'\n",
    "FILENAMES = os.listdir(PATH_TO_DATA)\n",
    "research_data = dict()\n",
    "\n",
    "for filename in FILENAMES:\n",
    "    with open(f'{PATH_TO_DATA}/{filename}', 'r') as file:\n",
    "        research_data[os.path.splitext(os.path.basename(filename))[\n",
    "            0]] = json.load(file)\n",
    "\n",
    "\n",
    "# postprocessing\n",
    "for journal in research_data:\n",
    "    for i, d in enumerate(research_data[journal]['metadatas']):\n",
    "        if not 'doi' in d:\n",
    "            del research_data[journal]['metadatas'][i]\n",
    "            del research_data[journal]['documents'][i]\n",
    "            del research_data[journal]['ids'][i]\n",
    "\n",
    "for journal in research_data:\n",
    "    for d in research_data[journal]['metadatas']:\n",
    "\n",
    "        # Convert stringified list to list\n",
    "        d['reference'] = json.loads(d['reference'])\n",
    "        d['doi'] = json.loads(d['doi'])\n",
    "\n",
    "print(research_data.keys())\n",
    "research_data['Astro_Research'].keys()\n",
    "\n",
    "all_data = all_reviews + \\\n",
    "    [record for journal in review_data for record in review_data[journal]['metadatas']]\n",
    "print(f\"All data: {len(all_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many bibcodes are there in this paper's references that start with 1929 and end with 'R'?\n",
    "import re\n",
    "# pattern = r'^2000.*B$'\n",
    "# matches = [s for s in paper['reference'] if re.match(pattern, s)]\n",
    "# print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re.compile(\"([A-Z][a-zA-ZÀ-ÖØ-öø-ÿ-]*(?:'[A-Z][a-zA-ZÀ-ÖØ-öø-ÿ-]*)?(?:,?\\\\s[A-Z][a-zA-ZÀ-ÖØ-öø-ÿ-]*(?:'[A-Z][a-zA-ZÀ-ÖØ-öø-ÿ-]*)?)*(?: et al.?)?)\\\\s*\\\\(?(\\\\d{4}[a-z]?)\\\\)?\")\n",
      "Results: 1\n",
      "1 ('Delbouille et al.', '1981')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the patterns\n",
    "lastname = r\"[A-Z][a-zA-ZÀ-ÖØ-öø-ÿ-]*(?:'[A-Z][a-zA-ZÀ-ÖØ-öø-ÿ-]*)?\"\n",
    "year = r\"\\(?(\\d{4}[a-z]?)\\)?\"\n",
    "name_sep = r\",?\\s\"\n",
    "INLINE_CITATION_PATTERN = fr\"({lastname}(?:{name_sep}{lastname})*(?: et al.?)?)\\s*{year}\"\n",
    "\n",
    "# Compile the regex pattern\n",
    "inline_regex = re.compile(INLINE_CITATION_PATTERN)\n",
    "\n",
    "print(inline_regex)\n",
    "\n",
    "test = \" Delbouille et al. 1981 the future (Section 5). 2. INGREDIENTS FOR SOLAR ABUNDANCE ANALYSIS 2.1. Observations Analyses of th\"\n",
    "\n",
    "# Find all matches using the compiled pattern\n",
    "matches = inline_regex.finditer(test)\n",
    "results = [match for match in matches]\n",
    "print(f\"Results: {len(results)}\")\n",
    "\n",
    "# Print the groups of each match\n",
    "for i, result in enumerate(results):\n",
    "    print(i+1, result.groups())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the inline citations from a paper's body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_inline_citations(record: dict) -> list[tuple[str, str]]:\n",
    "#     return [match.groups() for match in inline_regex.finditer(record['body'])]\n",
    "\n",
    "# # inline_citations = [match.groups() for match in inline_regex.finditer(paper['body'])]\n",
    "# inline_citations = get_inline_citations(paper)\n",
    "# print(f\"Results: {len(inline_citations)}\")\n",
    "# pprint(inline_citations[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to resolve incline citation to bibcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each result, get the first letter of the first author's last name\n",
    "# get the year\n",
    "def bibcode_regex(author: str, year: str):\n",
    "    \"\"\"\n",
    "    Given first author and year, return a regex pattern for the\n",
    "    corresponding bibcode\n",
    "    \"\"\"\n",
    "    initial = author[0]\n",
    "    year = year[:4] # cut off any letters at the end\n",
    "    pattern = fr'^{year}.*{initial}$'\n",
    "    return re.compile(pattern)\n",
    "\n",
    "def bibcode_matches(inline_citation: tuple[str, str], references: list[str]) -> int:\n",
    "    \"\"\"\n",
    "    Given an inline citation and a list of references, return the number of\n",
    "    references that match the inline citation's bibcode regex pattern\n",
    "    \"\"\"\n",
    "    pattern = bibcode_regex(*inline_citation)\n",
    "    return [s for s in references if pattern.match(s)]\n",
    "\n",
    "# def make_citation_bibcode_list(inline_citations: list[tuple[str, str]], references: list[str]) -> list[tuple[tuple[str, str], str]]:\n",
    "#     \"\"\"\n",
    "#     Given a paper's list of inline citations and list of references, return a list of\n",
    "#     tuples where the first element is the inline citation and the second element\n",
    "#     is the corresponding bibcode from the references list where there is exactly one match\n",
    "#     \"\"\"\n",
    "#     return [(citation, matches[0]) for citation in inline_citations \n",
    "#             if len((matches := bibcode_matches(citation, references))) == 1]\n",
    "\n",
    "# usable_citations = make_citation_bibcode_list(inline_citations, paper['reference'])\n",
    "# print(f\"Results: {len(usable_citations)}\")\n",
    "# print(usable_citations[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_all_bibcodes_from_file(path: str) -> list[str]:\n",
    "#     with open(path, 'r') as file:\n",
    "#         return json.load(file)\n",
    "\n",
    "# bibcodes = get_all_bibcodes_from_file('data/bibcodes.json')\n",
    "# print(f\"Results: {len(bibcodes)}\")\n",
    "# print(bibcodes[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def resolve_inline_references(records, bibcodes):\n",
    "#     in_dataset, out_of_dataset = [], []\n",
    "#     for record in tqdm(records, desc='Processing records'):\n",
    "#         usable_citations = make_citation_bibcode_list(get_inline_citations(record), record['reference'])\n",
    "#         for citation in usable_citations:\n",
    "#             # Construct the citation dictionary\n",
    "#             inline_citation, bibcode = citation\n",
    "#             cite_dict = {'source_bibcode': record['bibcode'],\n",
    "#                          'inline_citation': inline_citation, \n",
    "#                          'reference_bibcode': bibcode}\n",
    "\n",
    "#             # Determine if the referenced bibcode is in the dataset or not\n",
    "#             in_dataset.append(cite_dict) if bibcode in bibcodes else out_of_dataset.append(cite_dict)\n",
    "#     return in_dataset, out_of_dataset\n",
    "\n",
    "# have, dont_have = resolve_inline_references(review_data['Astro_Reviews']['metadatas'][:1], bibcodes)\n",
    "# print(f\"In dataset: {len(have)}\")\n",
    "# print(f\"Out of dataset: {len(dont_have)}\")\n",
    "# pprint(dont_have[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysbd\n",
    "\n",
    "def get_inline_citations(text: str) -> list[tuple[str, str]]:\n",
    "    return [match.groups() for match in inline_regex.finditer(text)]\n",
    "\n",
    "def sentence_to_example(record, sentence):\n",
    "    \"\"\"\n",
    "    Takes all the inline citations of a sentence and if it can resolve them to dois\n",
    "    then it returns the \"\"\"\n",
    "    def citation_to_doi(citation):\n",
    "        \"\"\"\n",
    "        Takes a single inline citation as tuple of (author, year) and determines if there is a unique\n",
    "        matching bibcode in the record's references. If so, it continues to look for a unique\n",
    "        doi matching that bibcode in the entire dataset. It returns the doi if resolved, otherwise None.\n",
    "        \"\"\"\n",
    "        bibcodes = bibcode_matches(citation, record['reference'])\n",
    "        if len(bibcodes) != 1:\n",
    "            return None\n",
    "        matching_dois = [record['doi'][0]\n",
    "                         for record in all_data if record['bibcode'] == bibcodes[0]]\n",
    "        if len(matching_dois) != 1:\n",
    "            return None\n",
    "        return matching_dois[0]\n",
    "    \n",
    "    inline_citations = get_inline_citations(sentence)\n",
    "    citation_dois = []\n",
    "    for citation in inline_citations:\n",
    "        if not (doi := citation_to_doi(citation)):\n",
    "            break\n",
    "        citation_dois.append(doi)\n",
    "\n",
    "    # If all citations resolved to dois, return the example\n",
    "    if len(inline_citations) != len(citation_dois):\n",
    "        return None\n",
    "    return {\n",
    "            'source_doi': record['doi'][0],\n",
    "            'sentence': sentence,\n",
    "            'citation_dois': citation_dois\n",
    "           }\n",
    "\n",
    "\n",
    "def create_examples_from_record(record):\n",
    "    splitter = pysbd.Segmenter(language=\"en\", clean=False)\n",
    "    sentences = [s for s in splitter.segment(record['body']) if len(s) > 40]\n",
    "    return [\n",
    "        example for sentence in sentences if (example := sentence_to_example(record, sentence))\n",
    "    ]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/test_set.jsonl', 'a') as file:\n",
    "    for journal in review_data:\n",
    "        for record in review_data[journal]['metadatas'][:10]:\n",
    "            examples = create_examples_from_record(record)\n",
    "            for example in examples:\n",
    "                json.dump(example, file)\n",
    "                file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "citeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
