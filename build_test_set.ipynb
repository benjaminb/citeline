{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "from utils import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/json/Astro_Reviews.json: 996/1000 have all required keys\n",
      "dict_keys(['bibcode', 'abstract', 'aff', 'author', 'bibstem', 'doctype', 'doi', 'id', 'keyword', 'pubdate', 'title', 'read_count', 'reference', 'citation_count', 'citation', 'body'])\n",
      "data/json/Astro_Research.json: 981/1000 have all required keys\n",
      "data/json/doi_articles.json: 1898/1898 have all required keys\n",
      "data/json/salvaged_articles.json: 50021/72374 have all required keys\n"
     ]
    }
   ],
   "source": [
    "PATH_TO_DATA = 'data/json/Astro_Reviews.json'\n",
    "astro_reviews = load_dataset(PATH_TO_DATA)\n",
    "print(astro_reviews[0].keys())\n",
    "\n",
    "astro_research = load_dataset('data/json/Astro_Research.json')\n",
    "doi_articles = load_dataset('data/json/doi_articles.json')\n",
    "salvaged_articles = load_dataset('data/json/salvaged_articles.json')\n",
    "\n",
    "all_records = astro_reviews + astro_research + doi_articles + salvaged_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "# Define bibcode and citation patterns\n",
    "lastname = r\"[A-Z][a-zA-ZÀ-ÖØ-öø-ÿ-]*(?:'[A-Z][a-zA-ZÀ-ÖØ-öø-ÿ-]*)?\"\n",
    "year = r\"\\(?(\\d{4}[a-z]?)\\)?\"\n",
    "name_sep = r\",?\\s\"\n",
    "INLINE_CITATION_PATTERN = fr\"({lastname}(?:{name_sep}{lastname})*(?: et al.?)?)\\s*{year}\"\n",
    "\n",
    "# Compile the regex pattern\n",
    "INLINE_REGEX = re.compile(INLINE_CITATION_PATTERN)\n",
    "\n",
    "# test = \" Delbouille et al. 1981 the future (Section 5). 2. INGREDIENTS FOR SOLAR ABUNDANCE ANALYSIS 2.1. Observations Analyses of th\"\n",
    "\n",
    "# # Find all matches using the compiled pattern\n",
    "# matches = inline_regex.finditer(test)\n",
    "# results = [match for match in matches]\n",
    "# print(f\"Results: {results}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to resolve incline citation to bibcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bibcode_regex(author: str, year: str):\n",
    "    \"\"\"\n",
    "    Given first author and year, return a regex pattern for the\n",
    "    corresponding bibcode\n",
    "    \"\"\"\n",
    "    initial = author[0]\n",
    "    year = year[:4] # cut off any letters at the end\n",
    "    pattern = fr'^{year}.*{initial}$'\n",
    "    return re.compile(pattern)\n",
    "\n",
    "def bibcode_matches(inline_citation: tuple[str, str], references: list[str]) -> int:\n",
    "    \"\"\"\n",
    "    Given an inline citation and a list of references, return the references\n",
    "    h the inline citation's bibcode regex pattern\n",
    "    \"\"\"\n",
    "    pattern = bibcode_regex(*inline_citation)\n",
    "    return [s for s in references if pattern.match(s)]\n",
    "\n",
    "# def make_citation_bibcode_list(inline_citations: list[tuple[str, str]], references: list[str]) -> list[tuple[tuple[str, str], str]]:\n",
    "#     \"\"\"\n",
    "#     Given a paper's list of inline citations and list of references, return a list of\n",
    "#     tuples where the first element is the inline citation and the second element\n",
    "#     is the corresponding bibcode from the references list where there is exactly one match\n",
    "#     \"\"\"\n",
    "#     return [(citation, matches[0]) for citation in inline_citations \n",
    "#             if len((matches := bibcode_matches(citation, references))) == 1]\n",
    "\n",
    "# usable_citations = make_citation_bibcode_list(inline_citations, paper['reference'])\n",
    "# print(f\"Results: {len(usable_citations)}\")\n",
    "# print(usable_citations[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysbd\n",
    "\n",
    "def get_inline_citations(text: str) -> list[tuple[str, str]]:\n",
    "    return [match.groups() for match in INLINE_REGEX.finditer(text)]\n",
    "\n",
    "def sentence_to_example(record, sentence, all_records):\n",
    "    \"\"\"\n",
    "    Takes all the inline citations of a sentence and if it can resolve them to dois\n",
    "    then it returns the \"\"\"\n",
    "    def citation_to_doi(citation):\n",
    "        \"\"\"\n",
    "        Takes a single inline citation as tuple of (author, year) and determines if there is a unique\n",
    "        matching bibcode in the record's references. If so, it continues to look for a unique\n",
    "        doi matching that bibcode in the entire dataset. It returns the doi if resolved, otherwise None.\n",
    "        \"\"\"\n",
    "        bibcodes = bibcode_matches(citation, record['reference'])\n",
    "        if len(bibcodes) != 1:\n",
    "            return None\n",
    "        \n",
    "        # Take the bibcode and look for a unique corresponding doi\n",
    "        matching_dois = [record['doi'][0]\n",
    "                         for record in all_records if record['bibcode'] == bibcodes[0]]\n",
    "        if len(matching_dois) != 1:\n",
    "            return None\n",
    "        return matching_dois[0]\n",
    "    \n",
    "    inline_citations = get_inline_citations(sentence)\n",
    "    citation_dois = []\n",
    "    for citation in inline_citations:\n",
    "        if not (doi := citation_to_doi(citation)):\n",
    "            break\n",
    "        citation_dois.append(doi)\n",
    "\n",
    "    # If all citations resolved to dois, return the example\n",
    "    # TODO: is this too strict?\n",
    "    if len(inline_citations) != len(citation_dois):\n",
    "        return None\n",
    "    return {\n",
    "            'source_doi': record['doi'][0],\n",
    "            'sentence': sentence,\n",
    "            'citation_dois': citation_dois\n",
    "           }\n",
    "\n",
    "\n",
    "def create_examples_from_record(record):\n",
    "    splitter = pysbd.Segmenter(language=\"en\", clean=False)\n",
    "    sentences = [s for s in splitter.segment(record['body']) if len(s) > 40]\n",
    "    return [\n",
    "        example for sentence in sentences if (example := sentence_to_example(record, sentence))\n",
    "    ]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_salvageable_bibcodes(record, sentence, all_bibcodes):\n",
    "    \"\"\"\n",
    "    Similar logic to sentence_to_example, except this function, when it finds an inline citation\n",
    "    that resolves to a single bibcode in a record/sentence, but it CANNOT find the corresponding\n",
    "    doi in the entire dataset (i.e. the bibcode is not in the dataset), it returns the bibcode\n",
    "    \"\"\"\n",
    "    def citation_to_missing_bibcode(citation):\n",
    "        \"\"\"\n",
    "        Takes a single inline citation as tuple of (author, year) and determines if there is a unique\n",
    "        matching bibcode in the record's references. If so, it continues to look for a unique\n",
    "        doi matching that bibcode in the entire dataset. It returns the doi if resolved, otherwise None.\n",
    "        \"\"\"\n",
    "        bibcodes = bibcode_matches(citation, record['reference'])\n",
    "        if len(bibcodes) != 1:\n",
    "            return None\n",
    "        \n",
    "        bibcode = bibcodes[0]\n",
    "        \n",
    "        # The inline citation corresponds to a unique bibcode, but that bibcode is not in the dataset\n",
    "        if bibcode not in all_bibcodes:\n",
    "            return bibcode\n",
    "        \n",
    "        matching_dois = [record['doi'][0]\n",
    "                         for record in all_records if record['bibcode'] == bibcode]\n",
    "        if len(matching_dois) != 1:\n",
    "            return None\n",
    "        return matching_dois[0]\n",
    "\n",
    "    inline_citations = get_inline_citations(sentence)\n",
    "    return [bibcode for citation in inline_citations if (bibcode := citation_to_missing_bibcode(citation))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_bibcodes = [record['bibcode'] for record in all_records]\n",
    "len(all_bibcodes) == len(all_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 996/996 [1:16:50<00:00,  4.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salvageable bibcodes: 45216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pysbd\n",
    "\n",
    "salvageable_bibcodes = set()\n",
    "splitter = pysbd.Segmenter(language=\"en\", clean=False)\n",
    "for record in tqdm(astro_reviews):\n",
    "    sentences = [s for s in splitter.segment(record['body']) if len(s) > 40]\n",
    "    for sentence in sentences:\n",
    "        salvageable_bibcodes.update(find_salvageable_bibcodes(record, sentence, all_bibcodes))\n",
    "\n",
    "print(f\"Salvageable bibcodes: {len(salvageable_bibcodes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('more_salvageable_bibcodes.txt', 'w') as f:\n",
    "    for bibcode in salvageable_bibcodes:\n",
    "        f.write(f\"{bibcode}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/json/Earth_Science_Reviews.json: 994/1000 have all required keys\n",
      "data/json/Earth_Science_Research.json: 1000/1000 have all required keys\n",
      "data/json/Planetary_Reviews.json: 994/1000 have all required keys\n",
      "data/json/Planetary_Research.json: 964/1000 have all required keys\n"
     ]
    }
   ],
   "source": [
    "earth_reviews = load_dataset('data/json/Earth_Science_Reviews.json')\n",
    "earth_research = load_dataset('data/json/Earth_Science_Research.json')\n",
    "planet_reviews = load_dataset('data/json/Planetary_Reviews.json')\n",
    "planet_research = load_dataset('data/json/Planetary_Research.json')\n",
    "\n",
    "all_records = astro_reviews + astro_research + earth_reviews + earth_research + planet_reviews + planet_research\n",
    "all_bibcodes = [record['bibcode'] for record in all_records]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1988/1988 [2:44:51<00:00,  4.98s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salvageable bibcodes: 74365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for record in tqdm(earth_reviews + planet_reviews):\n",
    "    sentences = [s for s in splitter.segment(record['body']) if len(s) > 40]\n",
    "    for sentence in sentences:\n",
    "        salvageable_bibcodes.update(find_salvageable_bibcodes(record, sentence, all_bibcodes))\n",
    "\n",
    "print(f\"Salvageable bibcodes: {len(salvageable_bibcodes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/salvageable_bibcodes.txt', 'w') as file:\n",
    "    for bibcode in salvageable_bibcodes:\n",
    "        file.write(f\"{bibcode}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/test_set.jsonl', 'a') as file:\n",
    "    for journal in review_data:\n",
    "        for record in review_data[journal]['metadatas'][:10]:\n",
    "            examples = create_examples_from_record(record)\n",
    "            for example in examples:\n",
    "                json.dump(example, file)\n",
    "                file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sentences = []\n",
    "num_examples = []\n",
    "examples_with_empty_labels = []\n",
    "total_examples = 0\n",
    "for i, record in tqdm(enumerate(review_data['Astro_Reviews']['metadatas'][:100])):\n",
    "    splitter = pysbd.Segmenter(language=\"en\", clean=False)\n",
    "    sentences = [s for s in splitter.segment(record['body']) if len(s) > 40]\n",
    "    num_sentences.append(len(sentences))\n",
    "    examples = [example for sentence in sentences if (example := sentence_to_example(record, sentence))]\n",
    "    num_examples.append(len(examples))\n",
    "    examples_with_empty_labels.append(len([e for e in examples if e['citation_dois'] == []]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of examples: {sum(num_examples)}\")\n",
    "print(f\"Total number of examples with no citations: {sum(examples_with_empty_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45216"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(salvageable_bibcodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74365"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/salvageable_bibcodes.txt', 'r') as file:\n",
    "    previous_salvageable_bibcodes = set(file.read().splitlines())\n",
    "\n",
    "len(previous_salvageable_bibcodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10858"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(salvageable_bibcodes.intersection(previous_salvageable_bibcodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "citeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
