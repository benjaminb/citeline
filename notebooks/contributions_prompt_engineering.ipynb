{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9346f4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<citeline.database.milvusdb.MilvusDB object at 0x17bbbb2d0>\n",
      "Qwen/Qwen3-Embedding-0.6B, device=mps, normalize=True, dim=1024\n",
      "QueryExpander(name=add_prev_3, data_length=2980)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:04<00:00,  4.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collections:\n",
      " - astrobert_chunks: 460801 entities\n",
      " - astrobert_contributions: 89860 entities\n",
      " - bge_chunks: 460801 entities\n",
      " - bge_contributions: 89860 entities\n",
      " - nasa_chunks: 460801 entities\n",
      " - nasa_contributions: 89860 entities\n",
      " - qwen06_chunks: 460801 entities\n",
      " - qwen06_contributions: 89860 entities\n",
      " - qwen06_findings_v2: 4342 entities\n",
      " - qwen8b_contributions: 89860 entities\n",
      " - specter_chunks: 460801 entities\n",
      " - specter_contributions: 89860 entities\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_doi</th>\n",
       "      <th>sent_original</th>\n",
       "      <th>sent_no_cit</th>\n",
       "      <th>sent_idx</th>\n",
       "      <th>citation_dois</th>\n",
       "      <th>pubdate</th>\n",
       "      <th>resolved_bibcodes</th>\n",
       "      <th>sent_cit_masked</th>\n",
       "      <th>vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.1146/annurev-astro-081710-102521</td>\n",
       "      <td>Their abundance is important because molecular...</td>\n",
       "      <td>In this limit, the important reactions are dis...</td>\n",
       "      <td>318</td>\n",
       "      <td>[10.1046/j.1365-8711.2002.04940.x]</td>\n",
       "      <td>20110901</td>\n",
       "      <td>[2002MNRAS.329...18F]</td>\n",
       "      <td>Their abundance is important because molecular...</td>\n",
       "      <td>[-0.036558926, -0.016381508, -0.0110567855, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.1146/annurev-astro-081817-051826</td>\n",
       "      <td>It is important to point out that the fraction...</td>\n",
       "      <td>Gesicki et al. (2014) derived masses and ages ...</td>\n",
       "      <td>231</td>\n",
       "      <td>[10.1051/0004-6361/201220678, 10.1051/0004-636...</td>\n",
       "      <td>20180901</td>\n",
       "      <td>[2013A&amp;A...549A.147B, 2017A&amp;A...605A..89B]</td>\n",
       "      <td>It is important to point out that the fraction...</td>\n",
       "      <td>[-0.016821573, -0.014033405, -0.008635518, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.1007/s00159-010-0029-x</td>\n",
       "      <td>How could the seed massive black holes have gr...</td>\n",
       "      <td>This argument is particularly important at ear...</td>\n",
       "      <td>259</td>\n",
       "      <td>[10.1086/422910, 10.1086/427065, 10.1086/50744...</td>\n",
       "      <td>20100701</td>\n",
       "      <td>[2004ApJ...613...36H, 2005ApJ...620...59S, 200...</td>\n",
       "      <td>How could the seed massive black holes have gr...</td>\n",
       "      <td>[-0.018864796, -0.06410703, -0.0063532703, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.1146/annurev.aa.31.090193.003441</td>\n",
       "      <td>Nature has somehow solved this problem in doub...</td>\n",
       "      <td>These models can reproduce the observed spectr...</td>\n",
       "      <td>457</td>\n",
       "      <td>[10.1086/161053]</td>\n",
       "      <td>19930101</td>\n",
       "      <td>[1983ApJ...269..423R]</td>\n",
       "      <td>Nature has somehow solved this problem in doub...</td>\n",
       "      <td>[0.044838704, -0.013867467, -0.007506692, 0.04...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.1007/s00159-012-0055-y</td>\n",
       "      <td>However, a similar linewidth–size scaling law ...</td>\n",
       "      <td>Size, internal velocity dispersion and column ...</td>\n",
       "      <td>306</td>\n",
       "      <td>[10.1051/0004-6361:20020629]</td>\n",
       "      <td>20121101</td>\n",
       "      <td>[2002A&amp;A...390..307O]</td>\n",
       "      <td>However, a similar linewidth–size scaling law ...</td>\n",
       "      <td>[-0.009350214, -0.056160886, -0.00880815, -0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            source_doi  \\\n",
       "0  10.1146/annurev-astro-081710-102521   \n",
       "1  10.1146/annurev-astro-081817-051826   \n",
       "2            10.1007/s00159-010-0029-x   \n",
       "3  10.1146/annurev.aa.31.090193.003441   \n",
       "4            10.1007/s00159-012-0055-y   \n",
       "\n",
       "                                       sent_original  \\\n",
       "0  Their abundance is important because molecular...   \n",
       "1  It is important to point out that the fraction...   \n",
       "2  How could the seed massive black holes have gr...   \n",
       "3  Nature has somehow solved this problem in doub...   \n",
       "4  However, a similar linewidth–size scaling law ...   \n",
       "\n",
       "                                         sent_no_cit  sent_idx  \\\n",
       "0  In this limit, the important reactions are dis...       318   \n",
       "1  Gesicki et al. (2014) derived masses and ages ...       231   \n",
       "2  This argument is particularly important at ear...       259   \n",
       "3  These models can reproduce the observed spectr...       457   \n",
       "4  Size, internal velocity dispersion and column ...       306   \n",
       "\n",
       "                                       citation_dois   pubdate  \\\n",
       "0                 [10.1046/j.1365-8711.2002.04940.x]  20110901   \n",
       "1  [10.1051/0004-6361/201220678, 10.1051/0004-636...  20180901   \n",
       "2  [10.1086/422910, 10.1086/427065, 10.1086/50744...  20100701   \n",
       "3                                   [10.1086/161053]  19930101   \n",
       "4                       [10.1051/0004-6361:20020629]  20121101   \n",
       "\n",
       "                                   resolved_bibcodes  \\\n",
       "0                              [2002MNRAS.329...18F]   \n",
       "1         [2013A&A...549A.147B, 2017A&A...605A..89B]   \n",
       "2  [2004ApJ...613...36H, 2005ApJ...620...59S, 200...   \n",
       "3                              [1983ApJ...269..423R]   \n",
       "4                              [2002A&A...390..307O]   \n",
       "\n",
       "                                     sent_cit_masked  \\\n",
       "0  Their abundance is important because molecular...   \n",
       "1  It is important to point out that the fraction...   \n",
       "2  How could the seed massive black holes have gr...   \n",
       "3  Nature has somehow solved this problem in doub...   \n",
       "4  However, a similar linewidth–size scaling law ...   \n",
       "\n",
       "                                              vector  \n",
       "0  [-0.036558926, -0.016381508, -0.0110567855, 0....  \n",
       "1  [-0.016821573, -0.014033405, -0.008635518, 0.0...  \n",
       "2  [-0.018864796, -0.06410703, -0.0063532703, 0.0...  \n",
       "3  [0.044838704, -0.013867467, -0.007506692, 0.04...  \n",
       "4  [-0.009350214, -0.056160886, -0.00880815, -0.0...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "from citeline.database.milvusdb import MilvusDB\n",
    "from citeline.embedders import Embedder\n",
    "from citeline.query_expander import get_expander\n",
    "\n",
    "db = MilvusDB()\n",
    "print(db)\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "# Setup: load embedder, expander, dataset, db collection\n",
    "embedder = Embedder.create(\"Qwen/Qwen3-Embedding-0.6B\", device=\"mps\", normalize=True)\n",
    "print(embedder)\n",
    "\n",
    "expander = get_expander(\"add_prev_3\", path_to_data=\"../data/preprocessed/reviews.jsonl\")\n",
    "print(expander)\n",
    "\n",
    "sample = pd.read_json(\"../data/dataset/nontrivial_100.jsonl\", lines=True)\n",
    "sample = sample.sample(20, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Apply query expansion and embed the queries\n",
    "sample[\"sent_no_cit\"] = expander(sample)\n",
    "sample[\"vector\"] = sample.progress_apply(lambda row: embedder([row[\"sent_no_cit\"]])[0], axis=1)\n",
    "\n",
    "db.list_collections()\n",
    "db.client.load_collection(\"qwen06_contributions\")\n",
    "\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36c36451",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 187.29it/s]\n",
      "100%|██████████| 20/20 [00:01<00:00, 15.97it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_doi</th>\n",
       "      <th>sent_original</th>\n",
       "      <th>sent_no_cit</th>\n",
       "      <th>sent_idx</th>\n",
       "      <th>citation_dois</th>\n",
       "      <th>pubdate</th>\n",
       "      <th>resolved_bibcodes</th>\n",
       "      <th>sent_cit_masked</th>\n",
       "      <th>vector</th>\n",
       "      <th>target_similarities</th>\n",
       "      <th>hard_dois</th>\n",
       "      <th>hard_similarities</th>\n",
       "      <th>old_margins</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.1146/annurev-astro-081710-102521</td>\n",
       "      <td>Their abundance is important because molecular...</td>\n",
       "      <td>In this limit, the important reactions are dis...</td>\n",
       "      <td>318</td>\n",
       "      <td>[10.1046/j.1365-8711.2002.04940.x]</td>\n",
       "      <td>20110901</td>\n",
       "      <td>[2002MNRAS.329...18F]</td>\n",
       "      <td>Their abundance is important because molecular...</td>\n",
       "      <td>[-0.036558926, -0.016381508, -0.0110567855, 0....</td>\n",
       "      <td>[0.6072211140974748]</td>\n",
       "      <td>[10.1088/0004-637X/703/2/1416, 10.1111/j.1365-...</td>\n",
       "      <td>[0.6299331188201904, 0.5999367833137512]</td>\n",
       "      <td>[-0.022712004722715617]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.1146/annurev-astro-081817-051826</td>\n",
       "      <td>It is important to point out that the fraction...</td>\n",
       "      <td>Gesicki et al. (2014) derived masses and ages ...</td>\n",
       "      <td>231</td>\n",
       "      <td>[10.1051/0004-6361/201220678, 10.1051/0004-636...</td>\n",
       "      <td>20180901</td>\n",
       "      <td>[2013A&amp;A...549A.147B, 2017A&amp;A...605A..89B]</td>\n",
       "      <td>It is important to point out that the fraction...</td>\n",
       "      <td>[-0.016821573, -0.014033405, -0.008635518, 0.0...</td>\n",
       "      <td>[0.5379291839783749, 0.5663974073131282]</td>\n",
       "      <td>[10.1093/mnras/stx373, 10.1051/0004-6361:20021...</td>\n",
       "      <td>[0.6454614996910095, 0.6320533156394958]</td>\n",
       "      <td>[-0.10753231571263466, -0.07906409237788137]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.1007/s00159-010-0029-x</td>\n",
       "      <td>How could the seed massive black holes have gr...</td>\n",
       "      <td>This argument is particularly important at ear...</td>\n",
       "      <td>259</td>\n",
       "      <td>[10.1086/422910, 10.1086/427065, 10.1086/50744...</td>\n",
       "      <td>20100701</td>\n",
       "      <td>[2004ApJ...613...36H, 2005ApJ...620...59S, 200...</td>\n",
       "      <td>How could the seed massive black holes have gr...</td>\n",
       "      <td>[-0.018864796, -0.06410703, -0.0063532703, 0.0...</td>\n",
       "      <td>[0.7070825246559626, 0.6650816675743063, 0.662...</td>\n",
       "      <td>[10.1111/j.1365-2966.2006.10467.x, 10.1111/j.1...</td>\n",
       "      <td>[0.7770616412162781, 0.7540676593780518]</td>\n",
       "      <td>[-0.06997911656031552, -0.11197997364197176, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.1146/annurev.aa.31.090193.003441</td>\n",
       "      <td>Nature has somehow solved this problem in doub...</td>\n",
       "      <td>These models can reproduce the observed spectr...</td>\n",
       "      <td>457</td>\n",
       "      <td>[10.1086/161053]</td>\n",
       "      <td>19930101</td>\n",
       "      <td>[1983ApJ...269..423R]</td>\n",
       "      <td>Nature has somehow solved this problem in doub...</td>\n",
       "      <td>[0.044838704, -0.013867467, -0.007506692, 0.04...</td>\n",
       "      <td>[0.5737327576399296]</td>\n",
       "      <td>[10.1086/164480, 10.1086/155083]</td>\n",
       "      <td>[0.6610962748527527, 0.6198835372924805]</td>\n",
       "      <td>[-0.0873635172128231]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.1007/s00159-012-0055-y</td>\n",
       "      <td>However, a similar linewidth–size scaling law ...</td>\n",
       "      <td>Size, internal velocity dispersion and column ...</td>\n",
       "      <td>306</td>\n",
       "      <td>[10.1051/0004-6361:20020629]</td>\n",
       "      <td>20121101</td>\n",
       "      <td>[2002A&amp;A...390..307O]</td>\n",
       "      <td>However, a similar linewidth–size scaling law ...</td>\n",
       "      <td>[-0.009350214, -0.056160886, -0.00880815, -0.0...</td>\n",
       "      <td>[0.4668131439478381]</td>\n",
       "      <td>[10.1086/169766, 10.1086/177465]</td>\n",
       "      <td>[0.6350903511047363, 0.5991689562797546]</td>\n",
       "      <td>[-0.1682772071568982]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            source_doi  \\\n",
       "0  10.1146/annurev-astro-081710-102521   \n",
       "1  10.1146/annurev-astro-081817-051826   \n",
       "2            10.1007/s00159-010-0029-x   \n",
       "3  10.1146/annurev.aa.31.090193.003441   \n",
       "4            10.1007/s00159-012-0055-y   \n",
       "\n",
       "                                       sent_original  \\\n",
       "0  Their abundance is important because molecular...   \n",
       "1  It is important to point out that the fraction...   \n",
       "2  How could the seed massive black holes have gr...   \n",
       "3  Nature has somehow solved this problem in doub...   \n",
       "4  However, a similar linewidth–size scaling law ...   \n",
       "\n",
       "                                         sent_no_cit  sent_idx  \\\n",
       "0  In this limit, the important reactions are dis...       318   \n",
       "1  Gesicki et al. (2014) derived masses and ages ...       231   \n",
       "2  This argument is particularly important at ear...       259   \n",
       "3  These models can reproduce the observed spectr...       457   \n",
       "4  Size, internal velocity dispersion and column ...       306   \n",
       "\n",
       "                                       citation_dois   pubdate  \\\n",
       "0                 [10.1046/j.1365-8711.2002.04940.x]  20110901   \n",
       "1  [10.1051/0004-6361/201220678, 10.1051/0004-636...  20180901   \n",
       "2  [10.1086/422910, 10.1086/427065, 10.1086/50744...  20100701   \n",
       "3                                   [10.1086/161053]  19930101   \n",
       "4                       [10.1051/0004-6361:20020629]  20121101   \n",
       "\n",
       "                                   resolved_bibcodes  \\\n",
       "0                              [2002MNRAS.329...18F]   \n",
       "1         [2013A&A...549A.147B, 2017A&A...605A..89B]   \n",
       "2  [2004ApJ...613...36H, 2005ApJ...620...59S, 200...   \n",
       "3                              [1983ApJ...269..423R]   \n",
       "4                              [2002A&A...390..307O]   \n",
       "\n",
       "                                     sent_cit_masked  \\\n",
       "0  Their abundance is important because molecular...   \n",
       "1  It is important to point out that the fraction...   \n",
       "2  How could the seed massive black holes have gr...   \n",
       "3  Nature has somehow solved this problem in doub...   \n",
       "4  However, a similar linewidth–size scaling law ...   \n",
       "\n",
       "                                              vector  \\\n",
       "0  [-0.036558926, -0.016381508, -0.0110567855, 0....   \n",
       "1  [-0.016821573, -0.014033405, -0.008635518, 0.0...   \n",
       "2  [-0.018864796, -0.06410703, -0.0063532703, 0.0...   \n",
       "3  [0.044838704, -0.013867467, -0.007506692, 0.04...   \n",
       "4  [-0.009350214, -0.056160886, -0.00880815, -0.0...   \n",
       "\n",
       "                                 target_similarities  \\\n",
       "0                               [0.6072211140974748]   \n",
       "1           [0.5379291839783749, 0.5663974073131282]   \n",
       "2  [0.7070825246559626, 0.6650816675743063, 0.662...   \n",
       "3                               [0.5737327576399296]   \n",
       "4                               [0.4668131439478381]   \n",
       "\n",
       "                                           hard_dois  \\\n",
       "0  [10.1088/0004-637X/703/2/1416, 10.1111/j.1365-...   \n",
       "1  [10.1093/mnras/stx373, 10.1051/0004-6361:20021...   \n",
       "2  [10.1111/j.1365-2966.2006.10467.x, 10.1111/j.1...   \n",
       "3                   [10.1086/164480, 10.1086/155083]   \n",
       "4                   [10.1086/169766, 10.1086/177465]   \n",
       "\n",
       "                          hard_similarities  \\\n",
       "0  [0.6299331188201904, 0.5999367833137512]   \n",
       "1  [0.6454614996910095, 0.6320533156394958]   \n",
       "2  [0.7770616412162781, 0.7540676593780518]   \n",
       "3  [0.6610962748527527, 0.6198835372924805]   \n",
       "4  [0.6350903511047363, 0.5991689562797546]   \n",
       "\n",
       "                                         old_margins  \n",
       "0                            [-0.022712004722715617]  \n",
       "1       [-0.10753231571263466, -0.07906409237788137]  \n",
       "2  [-0.06997911656031552, -0.11197997364197176, -...  \n",
       "3                              [-0.0873635172128231]  \n",
       "4                              [-0.1682772071568982]  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_hard_records(example: pd.Series, n: int = 2) -> list[str]:\n",
    "    \"\"\"\n",
    "    Overfetches 3*n most similar records (bc if two reps from same doc are in top n, we won't have n distinct non-target dois)\n",
    "\n",
    "    Returns:\n",
    "      A list of doi's, ordered by their max similarity to the query\n",
    "    \"\"\"\n",
    "    results = db.search(\n",
    "        collection_name=\"qwen06_contributions\",\n",
    "        query_records=[example.to_dict()],\n",
    "        query_vectors=[example.vector],\n",
    "        limit=3 * n,\n",
    "    )\n",
    "    results = results[0]  # db.search operates on lists of queries; we only need the first result\n",
    "\n",
    "    # Filter results to non-targets only\n",
    "    target_dois = set(example.citation_dois)\n",
    "    non_target_results = [r for r in results if r[\"doi\"] not in target_dois]\n",
    "    return non_target_results[:n]\n",
    "\n",
    "\n",
    "def get_similarity_to_targets(example: pd.Series) -> list[float]:\n",
    "    \"\"\"\n",
    "    For each target doi in the example, computes the max similarity between the example and any record with that doi.\n",
    "\n",
    "    Returns a list of scores in order of example.citation_dois\n",
    "    \"\"\"\n",
    "    similarities = []\n",
    "    for target_doi in example.citation_dois:\n",
    "        results = db.select_by_doi(doi=target_doi, collection_name=\"qwen06_contributions\")\n",
    "        target_vectors = np.array(results[\"vector\"].tolist())\n",
    "        similarity_scores = np.dot(example.vector, target_vectors.T)\n",
    "        similarities.append(np.max(similarity_scores))\n",
    "    return similarities\n",
    "\n",
    "\n",
    "def compute_margins(df: pd.DataFrame, target_col: str, hard_col: str, margin_col_name: str) -> None:\n",
    "    \"\"\"\n",
    "    For each row in the DataFrame, computes the margin between each target similarity and the hardest non-target similarity.\n",
    "\n",
    "    Args:\n",
    "      df: DataFrame containing the data\n",
    "      target_col: Name of the column with list of target similarities\n",
    "      hard_col: Name of the column with list of hard non-target similarities\n",
    "      margin_col_name: Name of the column to store the computed margins\n",
    "\n",
    "    Returns:\n",
    "      None (modifies df in place)\n",
    "    \"\"\"\n",
    "    df[margin_col_name] = None\n",
    "    for idx, row in df.iterrows():\n",
    "        target_similarities = row[target_col]\n",
    "        hardest_nontarget_similarity = max(row[hard_col])\n",
    "        margins = [target_sim - hardest_nontarget_similarity for target_sim in target_similarities]\n",
    "        df.at[idx, margin_col_name] = margins\n",
    "\n",
    "\n",
    "# Compute target and hard similarities, then the margins\n",
    "sample[\"target_similarities\"] = sample.progress_apply(get_similarity_to_targets, axis=1)\n",
    "sample[\"hard_dois\"] = None\n",
    "sample[\"hard_similarities\"] = None\n",
    "for idx, example in tqdm(sample.iterrows(), total=len(sample)):\n",
    "    hard_records = get_hard_records(example, n=2)\n",
    "    sample.at[idx, \"hard_dois\"] = [r[\"doi\"] for r in hard_records]\n",
    "    sample.at[idx, \"hard_similarities\"] = [r[\"metric\"] for r in hard_records]\n",
    "\n",
    "compute_margins(sample, target_col=\"target_similarities\", hard_col=\"hard_similarities\", margin_col_name=\"old_margins\")\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "877f8099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    28.000000\n",
       "mean     -0.072390\n",
       "std       0.063403\n",
       "min      -0.168277\n",
       "25%      -0.114911\n",
       "50%      -0.083214\n",
       "75%      -0.031465\n",
       "max       0.116795\n",
       "Name: old_margins, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "margins = pd.to_numeric(sample.explode(column=\"old_margins\")[\"old_margins\"], errors=\"coerce\").dropna()\n",
    "margins.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7096c976",
   "metadata": {},
   "source": [
    "## Process the dois\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08364f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOI's to process: 67\n",
      "Loaded 67 research papers\n",
      "Aperture Synthesis Maps of HDO Emission in Orion-KL\n",
      "\n",
      "The 1<SUB>10</SUB>-1<SUB>11</SUB> transition of deuterated water has been mapped toward the Kleinmann-Low nebula in Orion with the Hat Creek millimeter interferometer. The synthesized beamwidth is 3arcsec.4. The \"hot core\", \"plateau\", and \"compact ridge\" emission regions can be identified on the maps. The compact ridge appears to consist of streamers of gas which connect to the hot core clump and are directed away from the source IRc 2. The HD\n"
     ]
    }
   ],
   "source": [
    "dois_to_process = set(doi for dois in sample.citation_dois for doi in dois).union(\n",
    "    doi for dois in sample.hard_dois for doi in dois\n",
    ")\n",
    "print(f\"DOI's to process: {len(dois_to_process)}\")\n",
    "\n",
    "# Load research papers so we can get full text by doi\n",
    "research = pd.read_json(\"../data/research_used.jsonl\", lines=True)\n",
    "research = research[research[\"doi\"].isin(dois_to_process)].reset_index(drop=True)\n",
    "print(f\"Loaded {len(research)} research papers\")\n",
    "\n",
    "\n",
    "def doi_to_paper(doi: str) -> str:\n",
    "    record = research[research[\"doi\"] == doi].iloc[0]\n",
    "    return record[\"title\"] + \"\\n\\n\" + record[\"abstract\"] + \"\\n\\n\" + record[\"body\"]\n",
    "\n",
    "# Test:\n",
    "doi = list(dois_to_process)[0]\n",
    "print(doi_to_paper(doi)[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6c893fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"greeting\": \"Hello!\",\n",
      "    \"farewell\": \"Goodbye!\"\n",
      "}\n",
      "{'greeting': 'Hello!', 'farewell': 'Goodbye!'}\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "\n",
    "def bind_client(func):\n",
    "    \"\"\"\n",
    "    Decorator to bind OpenAI client to a function that will provide DeepSeek API access\n",
    "    \"\"\"\n",
    "    client = OpenAI(api_key=os.getenv(\"DEEPSEEK_API_KEY\"), base_url=\"https://api.deepseek.com\")\n",
    "\n",
    "    def wrapper(*args, **kwargs):\n",
    "        return func(client, *args, **kwargs)\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "@bind_client\n",
    "def deepseek(client, prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Sends a prompt to the DeepSeek API (using DeepSeek-V3.1 non-thinking model)\n",
    "\n",
    "    Expects a prompt that will instruct the model to respond with a JSON object.\n",
    "    However, the function returns the raw string response, to allow for validation and\n",
    "    error handling in multiple passes without losing the original response\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"deepseek-chat\",\n",
    "        messages=[{\"role\": \"system\", \"content\": prompt}],\n",
    "        stream=False,\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "response = deepseek(\"Respond with a JSON object with keys 'greeting' and 'farewell'\")\n",
    "print(response)\n",
    "print(json.loads(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fbc6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 63/67 [22:26<01:28, 22.11s/it]"
     ]
    }
   ],
   "source": [
    "with open(\"../src/citeline/llm/prompts/original_contributions_v2.txt\", \"r\") as f:\n",
    "    prompt_template = f.read()\n",
    "\n",
    "with open(\"new_findings.jsonl\", \"w\") as f:\n",
    "    for doi in tqdm(dois_to_process):\n",
    "        paper = doi_to_paper(doi)\n",
    "        prompt = prompt_template.format(paper=paper)\n",
    "        try:\n",
    "            response = deepseek(prompt)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing doi {doi}: {e}\")\n",
    "            continue\n",
    "        try:\n",
    "            data = json.loads(response)\n",
    "            data[\"doi\"] = doi\n",
    "            f.write(json.dumps(data) + \"\\n\")\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Failed to decode JSON for doi {doi}. Response was:\\n{response}\")\n",
    "            with open(\"failed_dois.txt\", \"a\") as f_fail:\n",
    "                f_fail.write(doi + \"\\n\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091ef5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_findings = pd.read_json(\"new_findings.jsonl\", lines=True)\n",
    "print(f\"Loaded {len(new_findings)} new findings\")\n",
    "\n",
    "new_findings_exploded = new_findings.explode(\"findings\")\n",
    "new_findings_exploded[\"vector\"] = embedder(new_findings_exploded[\"findings\"].tolist()).tolist()\n",
    "new_findings_exploded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ca6b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get new similarity to target\n",
    "sample['new_target_similarities'] = None\n",
    "sample['new_hard_similarities'] = None\n",
    "\n",
    "def get_vectors_by_doi(doi: str) -> np.ndarray:\n",
    "    return np.array(new_findings_exploded[new_findings_exploded[\"doi\"] == doi][\"vector\"].tolist())\n",
    "\n",
    "\n",
    "for idx, row in sample.iterrows():\n",
    "    # For each target doi, compute the max similarity wrt the new embeddings\n",
    "    query_vector = row['vector']\n",
    "    new_similarities = []\n",
    "    for target_doi in row['citation_dois']:\n",
    "        target_vectors = get_vectors_by_doi(target_doi)\n",
    "        new_similarities.append(np.max(np.dot(query_vector, target_vectors.T)))\n",
    "    sample.at[idx, 'new_target_similarities'] = new_similarities\n",
    "\n",
    "    # Collect all the hard vectors, compute the hard similarities\n",
    "    new_hard_similarities = []\n",
    "    for doi in row['hard_dois']:\n",
    "        candidate_vectors = get_vectors_by_doi(doi)\n",
    "        new_hard_similarities.append(np.max(np.dot(query_vector, candidate_vectors.T)))\n",
    "    sample.at[idx, 'new_hard_similarities'] = new_hard_similarities\n",
    "\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759b90a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_margins(sample, target_col=\"new_target_similarities\", hard_col=\"new_hard_similarities\", margin_col_name=\"new_margins\")\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52a80ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_margin_diffs(df: pd.DataFrame, new_col: str, ref_col: str) -> pd.Series:\n",
    "    new_values = df[new_col].explode().tolist()\n",
    "    ref_values = df[ref_col].explode().tolist()\n",
    "    diffs = [new - ref for new, ref in zip(new_values, ref_values)]\n",
    "    return pd.Series(diffs)\n",
    "\n",
    "diffs = compute_margin_diffs(sample, new_col=\"new_margins\", ref_col=\"old_margins\")\n",
    "print(diffs.describe())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b9faca",
   "metadata": {},
   "source": [
    "## Error analysis\n",
    "\n",
    "Let's look at where the new margin is still negative (the target document vectors aren't as close to the query as the hard examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bcf288",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_rows = sample[sample['new_margins'].apply(lambda margins: any(margin < 0 for margin in margins))]\n",
    "error_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e6baf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_error_row(idx: int) -> None:\n",
    "\n",
    "    example = error_rows.iloc[idx]\n",
    "    print(\"Original sentence:\")\n",
    "    pprint(example['sent_original'])\n",
    "\n",
    "    hardest_idx = np.argmax(example['new_hard_similarities'])\n",
    "    hard_doi = example['hard_dois'][hardest_idx]\n",
    "    hard_findings = new_findings_exploded[new_findings_exploded['doi'] == hard_doi]\n",
    "    hard_vectors = np.array(hard_findings['vector'].tolist())\n",
    "    hard_similarities = np.dot(example['vector'], hard_vectors.T)\n",
    "    hardest_indices = np.argsort(-hard_similarities)[:3]\n",
    "    for idx in hardest_indices:\n",
    "        print(f\"Similarity: {hard_similarities[idx]:.4f}, DOI: {hard_findings.iloc[idx]['doi']}\")\n",
    "        pprint(hard_findings.iloc[idx]['findings'])\n",
    "        print(\"-----\")\n",
    "\n",
    "analyze_error_row(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b519056e",
   "metadata": {},
   "source": [
    "### Revision 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340fee24",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../src/citeline/llm/prompts/original_contributions_v2.txt\", \"r\") as f:\n",
    "    prompt_template = f.read()\n",
    "\n",
    "NEW_FINDINGS_FILENAME = \"new_findings_v2.jsonl\"\n",
    "\n",
    "with open(NEW_FINDINGS_FILENAME, \"w\") as f:\n",
    "    for doi in tqdm(dois_to_process):\n",
    "        paper = doi_to_paper(doi)\n",
    "        prompt = prompt_template.format(paper=paper)\n",
    "        try:\n",
    "            response = deepseek(prompt)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing doi {doi}: {e}\")\n",
    "            continue\n",
    "        try:\n",
    "            data = json.loads(response)\n",
    "            data[\"doi\"] = doi\n",
    "            f.write(json.dumps(data) + \"\\n\")\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Failed to decode JSON for doi {doi}. Response was:\\n{response}\")\n",
    "            with open(\"failed_dois.txt\", \"a\") as f_fail:\n",
    "                f_fail.write(doi + \"\\n\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6486a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_findings = pd.read_json(NEW_FINDINGS_FILENAME, lines=True)\n",
    "print(f\"Loaded {len(new_findings)} new findings\")\n",
    "\n",
    "new_findings_exploded = new_findings.explode(\"findings\")\n",
    "new_findings_exploded[\"vector\"] = embedder(new_findings_exploded[\"findings\"].tolist()).tolist()\n",
    "new_findings_exploded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2e262d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save previous iteration and reset df for new results\n",
    "sample_old = sample.copy()\n",
    "\n",
    "# Get new similarity to target\n",
    "sample[\"new_target_similarities\"] = None\n",
    "sample[\"new_hard_similarities\"] = None\n",
    "\n",
    "for idx, row in sample.iterrows():\n",
    "    # For each target doi, compute the max similarity wrt the new embeddings\n",
    "    query_vector = row[\"vector\"]\n",
    "    new_similarities = []\n",
    "    for target_doi in row[\"citation_dois\"]:\n",
    "        target_vectors = get_vectors_by_doi(target_doi)\n",
    "        new_similarities.append(np.max(np.dot(query_vector, target_vectors.T)))\n",
    "    sample.at[idx, \"new_target_similarities\"] = new_similarities\n",
    "\n",
    "    # Collect all the hard vectors, compute the hard similarities\n",
    "    new_hard_similarities = []\n",
    "    for doi in row[\"hard_dois\"]:\n",
    "        candidate_vectors = get_vectors_by_doi(doi)\n",
    "        new_hard_similarities.append(np.max(np.dot(query_vector, candidate_vectors.T)))\n",
    "    sample.at[idx, \"new_hard_similarities\"] = new_hard_similarities\n",
    "\n",
    "compute_margins(\n",
    "    sample, target_col=\"new_target_similarities\", hard_col=\"new_hard_similarities\", margin_col_name=\"new_margins\"\n",
    ")\n",
    "sample.head()\n",
    "\n",
    "diffs = compute_margin_diffs(sample, new_col=\"new_margins\", ref_col=\"old_margins\")\n",
    "print(diffs.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4c79d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_rows = sample[sample[\"new_margins\"].apply(lambda margins: any(margin < 0 for margin in margins))]\n",
    "print(f\"Number of rows with negative new margins: {len(error_rows)}\")\n",
    "error_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208546b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the target contributions for an error row\n",
    "idx = 0\n",
    "analyze_error_row(idx)\n",
    "\n",
    "def print_target_contributions(idx: int) -> None:\n",
    "    row = error_rows.iloc[idx]\n",
    "    print(\"Original sentence:\")\n",
    "    print(row[\"sent_original\"])\n",
    "\n",
    "    target_dois = row[\"citation_dois\"]\n",
    "    target_records = {doi: new_findings_exploded[new_findings_exploded[\"doi\"] == doi]['findings'] for doi in target_dois}\n",
    "    pprint(\"Target findings:\")\n",
    "    for doi in target_records:\n",
    "        print(f\"DOI: {doi}\")\n",
    "        for i, finding in enumerate(target_records[doi]):\n",
    "            print(f\"{i}: {finding}\")\n",
    "        print(\"-----\")\n",
    "print(f\"Sentence in context:\\n{error_rows.iloc[idx]['sent_no_cit']}\")\n",
    "print_target_contributions(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d080fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_rows.iloc[idx]['sent_no_cit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b939606d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_vector = embedder(\n",
    "    [\n",
    "        \"Deep optical images shows a faint elliptical ring structure orbiting the spiral galaxy NGC 5907\",\n",
    "    ]\n",
    ")[0]\n",
    "# query_vector = error_rows.iloc[0][\"vector\"]\n",
    "query_vector = embedder(\n",
    "    [\n",
    "        \"However, deep optical images of a number of spiral galaxies, such as NGC 253, M 83, M 104, NGC 2855, (Malin and Hadley 1997) and NGC 5907 (), do show unusual, faint features in their surroundings.\",\n",
    "    ]\n",
    ")[0]\n",
    "print(f\"Cosine similarity: {query_vector.dot(target_vector):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683b60cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_vector = embedder([\"Most extended and complete luminosity function obtained for Galactic bulge to date\"])[0]\n",
    "print(f\"Cosine similarity: {np.dot(hard_vector, query_vector):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e14855",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in new_findings_exploded[new_findings_exploded[\"doi\"] == \"10.1086/164480\"].iterrows():\n",
    "    print(f\"Finding {i}:\")\n",
    "    pprint(row[\"findings\"])\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d0ecd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "citeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
