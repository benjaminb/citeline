{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9346f4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from citeline.database.milvusdb import MilvusDB\n",
    "from citeline.embedders import Embedder\n",
    "from citeline.query_expander import QueryExpander\n",
    "\n",
    "db = MilvusDB()\n",
    "print(db)\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "# Setup: load embedder, expander, dataset, db collection\n",
    "embedder = Embedder.create(\"Qwen/Qwen3-Embedding-0.6B\", device=\"mps\", normalize=True)\n",
    "print(embedder)\n",
    "\n",
    "expander = QueryExpander(\"add_prev_3\", reference_data_path=\"../data/preprocessed/reviews.jsonl\")\n",
    "print(expander)\n",
    "\n",
    "sample = pd.read_json(\"../data/dataset/nontrivial_100.jsonl\", lines=True)\n",
    "# sample = sample.sample(96, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Apply query expansion and embed the queries\n",
    "sample[\"sent_no_cit\"] = expander(sample)\n",
    "sample[\"vector\"] = sample.progress_apply(lambda row: embedder([row[\"sent_no_cit\"]])[0], axis=1)\n",
    "\n",
    "db.list_collections()\n",
    "# db.client.load_collection(\"qwen06_contributions\")\n",
    "db.client.load_collection(\"qwen06_chunks\")\n",
    "\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c745673",
   "metadata": {},
   "source": [
    "### Data Patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34fa4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This row's sentence concats a body sentence with a figure caption, which the dataset builder agent should not have included\n",
    "row = sample.iloc[17]\n",
    "print(row['sent_original'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e01a453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This sentence ('Kartaltepe et al. (2012) find that a sample ...) actually got linked to the wrong paper, Kocevski et al 2012\n",
    "print(sample.iloc[26][\"sent_original\"])\n",
    "print(sample.at[26, \"citation_dois\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccbcddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Its true target, \"10.48550/arXiv.1110.4057\", not in the db\n",
    "sample.drop(index=15, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c36451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hard_records(example: pd.Series, n: int = 2) -> list[str]:\n",
    "    \"\"\"\n",
    "    Overfetches 3*n most similar records (bc if two reps from same doc are in top n, we won't have n distinct non-target dois)\n",
    "\n",
    "    Returns:\n",
    "      A list of doi's, ordered by their max similarity to the query\n",
    "    \"\"\"\n",
    "    results = db.search(\n",
    "        collection_name=\"qwen06_contributions\",\n",
    "        query_records=[example.to_dict()],\n",
    "        query_vectors=[example.vector],\n",
    "        limit=10 * n,\n",
    "    )\n",
    "    results = results[0]  # db.search operates on lists of queries; we only need the first result\n",
    "\n",
    "    # Filter results to non-targets only\n",
    "    target_dois = set(example.citation_dois)\n",
    "    non_target_results = [r for r in results if r[\"doi\"] not in target_dois]\n",
    "    return non_target_results[:n]\n",
    "\n",
    "\n",
    "def get_similarity_to_targets(example: pd.Series) -> list[float]:\n",
    "    \"\"\"\n",
    "    For each target doi in the example, computes the max similarity between the example and any record with that doi.\n",
    "\n",
    "    Returns a list of scores in order of example.citation_dois\n",
    "    \"\"\"\n",
    "    similarities = []\n",
    "    for target_doi in example.citation_dois:\n",
    "        try:\n",
    "            results = db.select_by_doi(doi=target_doi, collection_name=\"qwen06_contributions\")\n",
    "            target_vectors = np.array(results[\"vector\"].tolist())\n",
    "            similarity_scores = np.dot(example.vector, target_vectors.T)\n",
    "            similarities.append(np.max(similarity_scores))\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing DOI {target_doi}: {e}\")\n",
    "    return similarities\n",
    "\n",
    "\n",
    "def compute_margins(df: pd.DataFrame, target_col: str, hard_col: str, margin_col_name: str) -> None:\n",
    "    \"\"\"\n",
    "    For each row in the DataFrame, computes the margin between each target similarity and the hardest non-target similarity.\n",
    "\n",
    "    Args:\n",
    "      df: DataFrame containing the data\n",
    "      target_col: Name of the column with list of target similarities\n",
    "      hard_col: Name of the column with list of hard non-target similarities\n",
    "      margin_col_name: Name of the column to store the computed margins\n",
    "\n",
    "    Returns:\n",
    "      None (modifies df in place)\n",
    "    \"\"\"\n",
    "\n",
    "    df[margin_col_name] = None\n",
    "    for idx, row in df.iterrows():\n",
    "        target_similarities = row[target_col]\n",
    "        hardest_nontarget_similarity = max(row[hard_col])\n",
    "        margins = [target_sim - hardest_nontarget_similarity for target_sim in target_similarities]\n",
    "        df.at[idx, margin_col_name] = margins\n",
    "\n",
    "\n",
    "# Compute target and hard similarities, then the margins\n",
    "sample[\"target_similarities\"] = sample.progress_apply(get_similarity_to_targets, axis=1)\n",
    "sample[\"hard_dois\"] = None\n",
    "sample[\"hard_similarities\"] = None\n",
    "for idx, example in tqdm(sample.iterrows(), total=len(sample)):\n",
    "    hard_records = get_hard_records(example, n=2)\n",
    "    sample.at[idx, \"hard_dois\"] = [r[\"doi\"] for r in hard_records]\n",
    "    sample.at[idx, \"hard_similarities\"] = [r[\"metric\"] for r in hard_records]\n",
    "\n",
    "compute_margins(sample, target_col=\"target_similarities\", hard_col=\"hard_similarities\", margin_col_name=\"old_margins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877f8099",
   "metadata": {},
   "outputs": [],
   "source": [
    "margins = pd.to_numeric(sample.explode(column=\"old_margins\")[\"old_margins\"], errors=\"coerce\").dropna()\n",
    "margins.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41778895",
   "metadata": {},
   "outputs": [],
   "source": [
    "for stat, num in margins.describe().items():\n",
    "    print(round(num, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51c0ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "margins.to_pickle(\"margins_v1_n100.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7096c976",
   "metadata": {},
   "source": [
    "## Process the dois\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08364f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dois_to_process = set(doi for dois in sample.citation_dois for doi in dois).union(\n",
    "    doi for dois in sample.hard_dois for doi in dois\n",
    ")\n",
    "print(f\"DOI's to process: {len(dois_to_process)}\")\n",
    "\n",
    "# Load research papers so we can get full text by doi\n",
    "research = pd.read_json(\"../data/research_used.jsonl\", lines=True)\n",
    "research = research[research[\"doi\"].isin(dois_to_process)].reset_index(drop=True)\n",
    "print(f\"Loaded {len(research)} research papers\")\n",
    "\n",
    "\n",
    "def doi_to_paper(doi: str) -> str:\n",
    "    record = research[research[\"doi\"] == doi].iloc[0]\n",
    "    return record[\"title\"] + \"\\n\\n\" + record[\"abstract\"] + \"\\n\\n\" + record[\"body\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c893fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepSeek client\n",
    "from citeline.apis.deepseek import deepseek_formatted as deepseek\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d78804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI, Gemini Clients\n",
    "\n",
    "# from citeline.apis.openai_client import openai_llm_client\n",
    "\n",
    "# openai_llm = openai_llm_client(model=\"gpt-5-nano\")\n",
    "\n",
    "\n",
    "# from google import genai\n",
    "# from google.genai import types\n",
    "# from citeline.llm.models import Findings\n",
    "\n",
    "# client = genai.Client()\n",
    "# with open(\"../src/citeline/llm/prompts/original_contributions_gemini_v3.txt\", \"r\") as f:\n",
    "#     prompt_template = f.read()\n",
    "\n",
    "\n",
    "# def gemini(paper: str) -> str:\n",
    "#     raw_text = \"\"\n",
    "#     try:\n",
    "#         response = client.models.generate_content(\n",
    "#             model=\"gemini-2.5-flash\",\n",
    "#             config=types.GenerateContentConfig(\n",
    "#                 temperature=0.0,\n",
    "#                 system_instruction=prompt_template,\n",
    "#                 response_mime_type=\"application/json\",\n",
    "#                 response_schema=Findings,\n",
    "#             ),\n",
    "#             contents=paper,\n",
    "#         )\n",
    "#         return response.parsed\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error during Gemini call: {e}\")\n",
    "#         return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90094a6b",
   "metadata": {},
   "source": [
    "### Set the filename for this run's generated findings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd7a2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_FINDINGS_FILENAME = \"findings_v3_reasoner_n100.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fbc6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../src/citeline/llm/prompts/original_contributions_v3.txt\", \"r\") as f:\n",
    "    prompt_template = f.read()\n",
    "\n",
    "# llm_function = deepseek\n",
    "# llm_function = openai_llm\n",
    "\n",
    "# Check if any dois have already been processed\n",
    "try:\n",
    "    with open(NEW_FINDINGS_FILENAME, \"r\") as f:\n",
    "        processed_dois = {json.loads(line)[\"doi\"] for line in f if line.strip()}\n",
    "        dois_to_process -= processed_dois\n",
    "    print(f\"Already processed {len(processed_dois)} DOI's.\")\n",
    "except FileNotFoundError:\n",
    "    processed_dois = set()\n",
    "    print(\"No existing findings file found. Starting fresh.\")\n",
    "\n",
    "with open(NEW_FINDINGS_FILENAME, \"a\") as f:\n",
    "    for doi in tqdm(dois_to_process):\n",
    "        paper = doi_to_paper(doi)\n",
    "        prompt = prompt_template.format(paper=paper)\n",
    "        try:\n",
    "            # response = llm_function(prompt)\n",
    "            response = deepseek(prompt, model=\"deepseek-reasoner\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing doi {doi}: {e}\")\n",
    "            continue\n",
    "        try:\n",
    "            data = json.loads(response)\n",
    "            data[\"doi\"] = doi\n",
    "            f.write(json.dumps(data) + \"\\n\")\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Failed to decode JSON for doi {doi}. Response was:\\n{response}\")\n",
    "            with open(\"failed_dois.txt\", \"a\") as f_fail:\n",
    "                f_fail.write(doi + \"\\n\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091ef5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_findings = pd.read_json(NEW_FINDINGS_FILENAME, lines=True)\n",
    "print(f\"Loaded {len(new_findings)} new findings\")\n",
    "\n",
    "new_findings_exploded = new_findings.explode(\"findings\")\n",
    "new_findings_exploded[\"vector\"] = embedder(new_findings_exploded[\"findings\"].tolist()).tolist()\n",
    "new_findings_exploded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ca6b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get new similarity to target\n",
    "sample[\"new_target_similarities\"] = None\n",
    "sample[\"new_hard_similarities\"] = None\n",
    "\n",
    "\n",
    "def get_vectors_by_doi(doi: str) -> np.ndarray:\n",
    "    return np.array(new_findings_exploded[new_findings_exploded[\"doi\"] == doi][\"vector\"].tolist())\n",
    "\n",
    "\n",
    "for idx, row in sample.iterrows():\n",
    "    # For each target doi, compute the max similarity wrt the new embeddings\n",
    "    query_vector = row[\"vector\"]\n",
    "    new_similarities = []\n",
    "    for target_doi in row[\"citation_dois\"]:\n",
    "        target_vectors = get_vectors_by_doi(target_doi)\n",
    "        new_similarities.append(np.max(np.dot(query_vector, target_vectors.T)))\n",
    "    sample.at[idx, \"new_target_similarities\"] = new_similarities\n",
    "\n",
    "    # Collect all the hard vectors, compute the hard similarities\n",
    "    new_hard_similarities = []\n",
    "    for doi in row[\"hard_dois\"]:\n",
    "        candidate_vectors = get_vectors_by_doi(doi)\n",
    "        new_hard_similarities.append(np.max(np.dot(query_vector, candidate_vectors.T)))\n",
    "    sample.at[idx, \"new_hard_similarities\"] = new_hard_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759b90a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_margins(\n",
    "    sample, target_col=\"new_target_similarities\", hard_col=\"new_hard_similarities\", margin_col_name=\"new_margins\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52a80ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_margin_diffs(df: pd.DataFrame, new_col: str, ref_col: str) -> pd.Series:\n",
    "    new_values = df[new_col].explode().tolist()\n",
    "    ref_values = df[ref_col].explode().tolist()\n",
    "    diffs = [new - ref for new, ref in zip(new_values, ref_values)]\n",
    "    return pd.Series(diffs)\n",
    "\n",
    "\n",
    "diffs = compute_margin_diffs(sample, new_col=\"new_margins\", ref_col=\"old_margins\")\n",
    "print(diffs.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045b6c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for stat, num in diffs.describe().items():\n",
    "    print(round(num, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78533109",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffs.to_pickle(\"margins_prompt3_reasoner_n100.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e594abd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1b9faca",
   "metadata": {},
   "source": [
    "## Error analysis\n",
    "\n",
    "Let's look at where the new margin is still negative (the target document vectors aren't as close to the query as the hard examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bcf288",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_rows = sample[sample[\"new_margins\"].apply(lambda margins: any(margin < 0 for margin in margins))]\n",
    "print(f\"{len(error_rows)} rows have at least one negative new margin\")\n",
    "\n",
    "error_margins = pd.to_numeric(error_rows.explode(column=\"new_margins\")[\"new_margins\"], errors=\"coerce\").dropna()\n",
    "print(error_margins.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538d3238",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e6baf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_error_row(idx: int) -> None:\n",
    "\n",
    "    example = error_rows.loc[idx]\n",
    "    margins = [round(float(margin), 4) for margin in example[\"new_margins\"]]\n",
    "    print(f\"Margins: {margins}\")\n",
    "    print(\"Original sentence:\")\n",
    "    print(example[\"sent_original\"])\n",
    "    print(\"\\nExpanded sentence:\")\n",
    "    print(example[\"sent_no_cit\"] + \"\\n\")\n",
    "\n",
    "    hardest_idx = np.argmax(example[\"new_hard_similarities\"])\n",
    "    hard_doi = example[\"hard_dois\"][hardest_idx]\n",
    "    hard_findings = new_findings_exploded[new_findings_exploded[\"doi\"] == hard_doi]\n",
    "    hard_vectors = np.array(hard_findings[\"vector\"].tolist())\n",
    "    hard_similarities = np.dot(example[\"vector\"], hard_vectors.T)\n",
    "    hardest_indices = np.argsort(-hard_similarities)[:3]\n",
    "    for idx in hardest_indices:\n",
    "        print(f\"Similarity: {hard_similarities[idx]:.4f}, DOI: {hard_findings.iloc[idx]['doi']}\")\n",
    "        pprint(hard_findings.iloc[idx][\"findings\"])\n",
    "        print(\"-----\")\n",
    "\n",
    "\n",
    "def print_target_contributions(idx: int) -> None:\n",
    "    row = error_rows.loc[idx]\n",
    "    print(\"Original sentence:\")\n",
    "    print(row[\"sent_original\"])\n",
    "\n",
    "    target_dois = row[\"citation_dois\"]\n",
    "    print(f\"Target DOIs: {target_dois}\")\n",
    "    target_records = {\n",
    "        doi: new_findings_exploded[new_findings_exploded[\"doi\"] == doi][\"findings\"] for doi in target_dois\n",
    "    }\n",
    "    pprint(\"Target findings:\")\n",
    "    for doi in target_records:\n",
    "        print(f\"DOI: {doi}\")\n",
    "        for i, finding in enumerate(target_records[doi]):\n",
    "            print(f\"{i}: {finding}\")\n",
    "        print(\"-----\")\n",
    "\n",
    "\n",
    "idx = 4\n",
    "\n",
    "print_target_contributions(idx)\n",
    "analyze_error_row(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73deab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "doi = \"10.1086/319733\"\n",
    "row = new_findings[new_findings[\"doi\"] == doi].iloc[0]\n",
    "target_texts = row[\"findings\"]\n",
    "target_vectors = embedder(target_texts)\n",
    "query_vector = embedder(\n",
    "    [\n",
    "        \"One of the most surprising observations to date are those of the narrow absorption lines in the quasar 3C 191, which show evidence for partial covering at a large distance (28 kpc) from the nucleus ( ); it is difficult to understand how the small clouds have maintained their integrity over the outflow timescale of ∼3 × 10 7 year.\"\n",
    "    ]\n",
    ")[0]\n",
    "\n",
    "cosine_similarities = np.dot(query_vector, target_vectors.T)\n",
    "tups = sorted(enumerate(cosine_similarities), key=lambda x: -x[1])\n",
    "for i, sim in tups:\n",
    "    print(f\"Finding {i}: similarity {sim:.4f}\")\n",
    "print(cosine_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8908fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_vector = embedder(\n",
    "    [\"Cosmic microwave background (CMB) anisotropy measurements indicate flat \" \"universe geometry.\"]\n",
    ")[0]\n",
    "# query_vector = error_rows.iloc[0][\"vector\"]\n",
    "query_vector = embedder(\n",
    "    [\n",
    "        \"This data provided the most convincing evidence then available for the Euclidean nature of the Universe; i.e. that the geometry is flat Fig. 16 The first measurement of polarization made of the CMBR, obtained by the DASI experiment at the South Pole \"\n",
    "    ]\n",
    ")[0]\n",
    "print(f\"Cosine similarity: {query_vector.dot(target_vector):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b519056e",
   "metadata": {},
   "source": [
    "### Revision 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340fee24",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../src/citeline/llm/prompts/original_contributions_v2.txt\", \"r\") as f:\n",
    "    prompt_template = f.read()\n",
    "\n",
    "NEW_FINDINGS_FILENAME = \"new_findings_v2.jsonl\"\n",
    "\n",
    "with open(NEW_FINDINGS_FILENAME, \"w\") as f:\n",
    "    for doi in tqdm(dois_to_process):\n",
    "        paper = doi_to_paper(doi)\n",
    "        prompt = prompt_template.format(paper=paper)\n",
    "        try:\n",
    "            response = deepseek(prompt)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing doi {doi}: {e}\")\n",
    "            continue\n",
    "        try:\n",
    "            data = json.loads(response)\n",
    "            data[\"doi\"] = doi\n",
    "            f.write(json.dumps(data) + \"\\n\")\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Failed to decode JSON for doi {doi}. Response was:\\n{response}\")\n",
    "            with open(\"failed_dois.txt\", \"a\") as f_fail:\n",
    "                f_fail.write(doi + \"\\n\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6486a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_findings = pd.read_json(NEW_FINDINGS_FILENAME, lines=True)\n",
    "print(f\"Loaded {len(new_findings)} new findings\")\n",
    "\n",
    "new_findings_exploded = new_findings.explode(\"findings\")\n",
    "new_findings_exploded[\"vector\"] = embedder(new_findings_exploded[\"findings\"].tolist()).tolist()\n",
    "new_findings_exploded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2e262d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save previous iteration and reset df for new results\n",
    "sample_old = sample.copy()\n",
    "\n",
    "# Get new similarity to target\n",
    "sample[\"new_target_similarities\"] = None\n",
    "sample[\"new_hard_similarities\"] = None\n",
    "\n",
    "for idx, row in sample.iterrows():\n",
    "    # For each target doi, compute the max similarity wrt the new embeddings\n",
    "    query_vector = row[\"vector\"]\n",
    "    new_similarities = []\n",
    "    for target_doi in row[\"citation_dois\"]:\n",
    "        target_vectors = get_vectors_by_doi(target_doi)\n",
    "        new_similarities.append(np.max(np.dot(query_vector, target_vectors.T)))\n",
    "    sample.at[idx, \"new_target_similarities\"] = new_similarities\n",
    "\n",
    "    # Collect all the hard vectors, compute the hard similarities\n",
    "    new_hard_similarities = []\n",
    "    for doi in row[\"hard_dois\"]:\n",
    "        candidate_vectors = get_vectors_by_doi(doi)\n",
    "        new_hard_similarities.append(np.max(np.dot(query_vector, candidate_vectors.T)))\n",
    "    sample.at[idx, \"new_hard_similarities\"] = new_hard_similarities\n",
    "\n",
    "compute_margins(\n",
    "    sample, target_col=\"new_target_similarities\", hard_col=\"new_hard_similarities\", margin_col_name=\"new_margins\"\n",
    ")\n",
    "sample.head()\n",
    "\n",
    "diffs = compute_margin_diffs(sample, new_col=\"new_margins\", ref_col=\"old_margins\")\n",
    "print(diffs.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4c79d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_rows = sample[sample[\"new_margins\"].apply(lambda margins: any(margin < 0 for margin in margins))]\n",
    "print(f\"Number of rows with negative new margins: {len(error_rows)}\")\n",
    "error_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208546b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the target contributions for an error row\n",
    "idx = 0\n",
    "analyze_error_row(idx)\n",
    "\n",
    "\n",
    "def print_target_contributions(idx: int) -> None:\n",
    "    row = error_rows.iloc[idx]\n",
    "    print(\"Original sentence:\")\n",
    "    print(row[\"sent_original\"])\n",
    "\n",
    "    target_dois = row[\"citation_dois\"]\n",
    "    target_records = {\n",
    "        doi: new_findings_exploded[new_findings_exploded[\"doi\"] == doi][\"findings\"] for doi in target_dois\n",
    "    }\n",
    "    pprint(\"Target findings:\")\n",
    "    for doi in target_records:\n",
    "        print(f\"DOI: {doi}\")\n",
    "        for i, finding in enumerate(target_records[doi]):\n",
    "            print(f\"{i}: {finding}\")\n",
    "        print(\"-----\")\n",
    "\n",
    "\n",
    "print(f\"Sentence in context:\\n{error_rows.iloc[idx]['sent_no_cit']}\")\n",
    "print_target_contributions(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d080fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_rows.iloc[idx][\"sent_no_cit\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b939606d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_vector = embedder(\n",
    "    [\n",
    "        \"Deep optical images shows a faint elliptical ring structure orbiting the spiral galaxy NGC 5907\",\n",
    "    ]\n",
    ")[0]\n",
    "# query_vector = error_rows.iloc[0][\"vector\"]\n",
    "query_vector = embedder(\n",
    "    [\n",
    "        \"However, deep optical images of a number of spiral galaxies, such as NGC 253, M 83, M 104, NGC 2855, (Malin and Hadley 1997) and NGC 5907 (), do show unusual, faint features in their surroundings.\",\n",
    "    ]\n",
    ")[0]\n",
    "print(f\"Cosine similarity: {query_vector.dot(target_vector):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683b60cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_vector = embedder([\"Most extended and complete luminosity function obtained for Galactic bulge to date\"])[0]\n",
    "print(f\"Cosine similarity: {np.dot(hard_vector, query_vector):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e14855",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in new_findings_exploded[new_findings_exploded[\"doi\"] == \"10.1086/164480\"].iterrows():\n",
    "    print(f\"Finding {i}:\")\n",
    "    pprint(row[\"findings\"])\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d0ecd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load pickle files\n",
    "margins_v1 = pd.read_pickle(\"margins_v1_n100.pkl\")\n",
    "margins_reasoner_v3 = pd.read_pickle(\"margins_prompt3_reasoner_n100.pkl\")\n",
    "margins_chat_v3 = pd.read_pickle(\"margins_prompt3_chat_n100.pkl\")\n",
    "\n",
    "plt.boxplot(\n",
    "    [\n",
    "        margins_v1,\n",
    "        margins_chat_v3,\n",
    "        margins_reasoner_v3,\n",
    "    ],\n",
    "    labels=[\"Prompt v1\\ndeepseek-r1\", \"Prompt v3\\ndeepseek-chat\", \"Prompt v3\\ndeepseek-reasoner\"],\n",
    ")\n",
    "plt.axhline(y=0, color=\"blue\", linestyle=\"--\", linewidth=1.0)\n",
    "plt.title(f\"Margin Differences (n={len(sample)})\")\n",
    "plt.ylabel(\"Margins\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7e9472",
   "metadata": {},
   "outputs": [],
   "source": [
    "for stat, num in margins_chat_v3.describe().items():\n",
    "    print(round(num,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9074b161",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "citeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
