{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be0ae21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a48e5db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This becomes the subdir under the dir with modelname; use it to indicate other experiment details\n",
    "# Besides model name, which indicates architecture / forward pass\n",
    "# So use it to indicate regularization, loss function, optimizer, etc.\n",
    "experiment_name = \"cos_adam\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37ca1b9",
   "metadata": {},
   "source": [
    "#### Set up datasets, get baseline stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6723a597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([ 0.0090,  0.0087, -0.0090,  ..., -0.0209,  0.0650, -0.0206]), tensor([-0.0260, -0.0282, -0.0108,  ...,  0.0060, -0.0049, -0.0267]), tensor([ 0.0264, -0.0285, -0.0133,  ..., -0.0481,  0.0206, -0.0428]))\n"
     ]
    }
   ],
   "source": [
    "class TripletDatasetHardandSoft(Dataset):\n",
    "    def __init__(self, path, hard_ratio=0.5):\n",
    "        with h5py.File(path, \"r\") as f:\n",
    "            self.anchors = torch.from_numpy(f[\"queries\"][:]).float()\n",
    "            self.positives = torch.from_numpy(f[\"positives\"][:]).float()\n",
    "            self.hard_negatives = torch.from_numpy(f[\"hard_negatives\"][:]).float()\n",
    "            self.soft_negatives = torch.from_numpy(f[\"soft_negatives\"][:]).float()\n",
    "        self.hard_ratio = hard_ratio\n",
    "\n",
    "    def __sample_row(self, rows):\n",
    "        idx = np.random.randint(len(rows))\n",
    "        return rows[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.anchors)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        anchor = self.anchors[idx]\n",
    "        positives = self.positives[idx]\n",
    "        positive = self.__sample_row(positives)\n",
    "\n",
    "        negatives = self.hard_negatives[idx] if np.random.rand() < self.hard_ratio else self.soft_negatives[idx]\n",
    "        negative = self.__sample_row(negatives)\n",
    "        return anchor, positive, negative\n",
    "    \n",
    "val_path = \"../data/dataset/val_triplet_ds_add_prev_2.h5\"\n",
    "ds = TripletDatasetHardandSoft(val_path, hard_ratio=0.5)\n",
    "res = ds[0]\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8897fd69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Hard Negatives ===\n",
      "Anchor-Positive: 0.6125 | Anchor-Negative: 0.6302\n",
      "Anchor-Positive: 0.6643 | Anchor-Negative: 0.6575\n",
      "Anchor-Positive: 0.7387 | Anchor-Negative: 0.6821\n",
      "Anchor-Positive: 0.6075 | Anchor-Negative: 0.6441\n",
      "Anchor-Positive: 0.5447 | Anchor-Negative: 0.6066\n",
      "==== Soft Negatives ===\n",
      "Anchor-Positive: 0.5755 | Anchor-Negative: 0.5111\n",
      "Anchor-Positive: 0.6525 | Anchor-Negative: 0.5508\n",
      "Anchor-Positive: 0.6919 | Anchor-Negative: 0.5183\n",
      "Anchor-Positive: 0.6075 | Anchor-Negative: 0.5597\n",
      "Anchor-Positive: 0.4826 | Anchor-Negative: 0.5432\n"
     ]
    }
   ],
   "source": [
    "# Test that hard negatives are actually hard\n",
    "ds.hard_ratio = 1.0\n",
    "print(\"==== Hard Negatives ===\")\n",
    "for idx in range(5):\n",
    "    anchor, positive, negative = ds[idx]\n",
    "    # Cosine of anchor to positive:\n",
    "    print(\n",
    "        f\"Anchor-Positive: {F.cosine_similarity(anchor, positive, dim=0):.4f} | Anchor-Negative: {F.cosine_similarity(anchor, negative, dim=0):.4f}\"\n",
    "    )\n",
    "\n",
    "ds.hard_ratio = 0.0\n",
    "print(\"==== Soft Negatives ===\")\n",
    "for idx in range(5):\n",
    "    anchor, positive, negative = ds[idx]\n",
    "    # Cosine of anchor to positive:\n",
    "    print(f\"Anchor-Positive: {F.cosine_similarity(anchor, positive, dim=0):.4f} | Anchor-Negative: {F.cosine_similarity(anchor, negative, dim=0):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f55f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, h5_path: str):\n",
    "        with h5py.File(h5_path, \"r\") as f:\n",
    "            self.triplets = torch.from_numpy(f[\"triplets\"][:]).float()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.triplets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        anchor = self.triplets[idx, 0, :]\n",
    "        positive = self.triplets[idx, 1, :]\n",
    "        negative = self.triplets[idx, 2, :]\n",
    "        return anchor, positive, negative\n",
    "\n",
    "\n",
    "train_dataset = TripletDataset(\"../src/citeline/nn/np_vectors_train_triplets.h5\")\n",
    "val_dataset = TripletDataset(\"../src/citeline/nn/np_vectors_val_triplets.h5\")\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False, drop_last=True)\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "\n",
    "# Check initial similarities in the raw data\n",
    "ap_distances = []\n",
    "an_distances = []\n",
    "margins = []\n",
    "for anchor, pos, neg in val_dataset:\n",
    "    ap_dist = 1 - F.cosine_similarity(anchor.unsqueeze(0), pos.unsqueeze(0)).item()\n",
    "    an_dist = 1 - F.cosine_similarity(anchor.unsqueeze(0), neg.unsqueeze(0)).item()\n",
    "    ap_distances.append(ap_dist)\n",
    "    an_distances.append(an_dist)\n",
    "    margins.append(an_dist - ap_dist)\n",
    "\n",
    "print(f\"=== BASELINE ===\")\n",
    "print(f\"Average Anchor-Positive distance: {np.mean(ap_distances):.4f} ± {np.std(ap_distances):.4f}\")\n",
    "print(f\"Average Anchor-Negative distance: {np.mean(an_distances):.4f} ± {np.std(an_distances):.4f}\")\n",
    "print(f\"Average margin in baseline data: {np.mean(margins):.4f}\")\n",
    "\n",
    "results['baseline'] = {\n",
    "    'distances_to_positive': ap_distances,\n",
    "    'distances_to_negative': an_distances,\n",
    "    'margins': margins\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fd2fc7",
   "metadata": {},
   "source": [
    "### 1. Define model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2f98eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingMapper(nn.Module):\n",
    "    def __init__(self, input_dim=1024, output_dim=1024, hidden_dim=256):  # Reduced from 512\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.final = nn.Linear(128, output_dim)\n",
    "        self.residual_scale = nn.Parameter(torch.tensor(0.01))\n",
    "\n",
    "        nn.init.xavier_uniform_(self.fc1.weight, gain=0.1)\n",
    "        nn.init.xavier_uniform_(self.final.weight, gain=0.1)\n",
    "        nn.init.zeros_(self.fc1.bias)\n",
    "        nn.init.zeros_(self.final.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = F.tanh(self.fc1(x))\n",
    "        y = F.tanh(self.final(y))\n",
    "        y = x + self.residual_scale * y  # Residual connection\n",
    "\n",
    "        y = F.normalize(y, p=2, dim=1)\n",
    "        return y\n",
    "\n",
    "# TODO: move this to an import\n",
    "model = EmbeddingMapper(input_dim=1024, output_dim=1024).to(device)\n",
    "results[\"model_name\"] = model.__class__.__name__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb34b53",
   "metadata": {},
   "source": [
    "### 2. Choose optimizer, learning rate, and weight decay (if any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3740092d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results['lr'] = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=results['lr'])\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=results['lr'], weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee99e641",
   "metadata": {},
   "source": [
    "### 3. Define loss function / loss function schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2a2221",
   "metadata": {},
   "outputs": [],
   "source": [
    "results['loss_function'] = 'CosineEmbeddingLoss with margin=0.1'\n",
    "cosine_loss = nn.CosineEmbeddingLoss(margin=0.1)\n",
    "\n",
    "loss_function = lambda a, p, n: cosine_loss(a, p, torch.ones(a.size(0)).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c098c5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline loss\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    for anchors, positives, negatives in val_dataloader:\n",
    "        anchors, positives, negatives = anchors.to(device), positives.to(device), negatives.to(device)\n",
    "        anchor_out = model(anchors)\n",
    "        val_loss = loss_function(anchor_out, positives, negatives)\n",
    "        val_losses.append(val_loss.item())\n",
    "\n",
    "    # Check that the untrained model isn't starting from a point where it's perturbing the data far away from positives\n",
    "    for name, dataset in [(\"Validation\", val_dataset)]:\n",
    "        ap_distances_after = []\n",
    "        an_distances_after = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for anchor, pos, neg in list(dataset):\n",
    "                anchor = anchor.to(device).unsqueeze(0)\n",
    "\n",
    "                anchor_emb = model(anchor)  # Only transform anchor\n",
    "\n",
    "                # Compare to raw positive/negative (no transformation)\n",
    "                ap_dist = 1 - F.cosine_similarity(anchor_emb, pos.unsqueeze(0).to(device)).item()\n",
    "                an_dist = 1 - F.cosine_similarity(anchor_emb, neg.unsqueeze(0).to(device)).item()\n",
    "\n",
    "                ap_distances_after.append(ap_dist)\n",
    "                an_distances_after.append(an_dist)\n",
    "\n",
    "        anchor_positive_mean_distance = np.mean(ap_distances_after)\n",
    "        anchor_negative_mean_distance = np.mean(an_distances_after)\n",
    "        ap_pct_diff = ((anchor_positive_mean_distance - np.mean(ap_distances)) / np.mean(ap_distances)) * 100\n",
    "        an_pct_diff = ((anchor_negative_mean_distance - np.mean(an_distances)) / np.mean(an_distances)) * 100\n",
    "        print(f\"=== BEFORE TRAINING ({name} set, Anchor mapped, Pos/Neg unchanged) ===\")\n",
    "        print(f\"Anchor-Positive distance: {anchor_positive_mean_distance:.4f}, {ap_pct_diff:+.4f}% vs. baseline\")\n",
    "        print(f\"Anchor-Negative distance: {anchor_negative_mean_distance:.4f}, {an_pct_diff:+.4f}% vs. baseline\")\n",
    "        print(f\"Average margin: {anchor_negative_mean_distance - anchor_positive_mean_distance:.4f}\")\n",
    "\n",
    "        if abs(ap_pct_diff) > 5.0:\n",
    "            print(\"WARNING: Untrained model is changing Anchor-Positive distances significantly!\")\n",
    "        if abs(an_pct_diff) > 5.0:\n",
    "            print(\"WARNING: Untrained model is changing Anchor-Negative distances significantly!\")\n",
    "            \n",
    "        # Box plot the results\n",
    "        results['untrained'] = {\n",
    "            'distances_to_positive': ap_distances_after,\n",
    "            'distances_to_negative': an_distances_after,\n",
    "            'margins': [an - ap for ap, an in zip(ap_distances_after, an_distances_after)]\n",
    "        }\n",
    "        # Box plot ap_distances against ap_distances_after\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        axes[0].boxplot([ap_distances, ap_distances_after], tick_labels=['Baseline', 'Untrained Model'])\n",
    "        axes[0].set_ylabel('Cosine Distance (Anchor to Positive)')\n",
    "        axes[0].set_title(f\"Anchor-Positive Distances ({name} set)\")\n",
    "\n",
    "        axes[1].boxplot([an_distances, an_distances_after], tick_labels=['Baseline', 'Untrained Model'])\n",
    "        axes[1].set_ylabel('Cosine Distance (Anchor to Negative)')\n",
    "        axes[1].set_title(f\"Anchor-Negative Distances ({name} set)\")\n",
    "\n",
    "        axes[2].boxplot([results['baseline']['margins'], results['untrained']['margins']], tick_labels=['Baseline', 'Untrained Model'])\n",
    "        axes[2].set_ylabel('Margin (Negative - Positive)')\n",
    "        axes[2].set_title(f\"Margins ({name} set)\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa000cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "pos_dist_weight = 1.0\n",
    "NUM_EPOCHS = 20\n",
    "min_val_loss = float(\"inf\")\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # ---- Training ----\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for batch_idx, (anchors, positives, negatives) in enumerate(train_dataloader):\n",
    "        anchors, positives, negatives = anchors.to(device), positives.to(device), negatives.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        anchor_out = model(anchors)\n",
    "\n",
    "        loss = loss_function(anchor_out, positives, negatives) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    train_loss_epoch = np.mean(train_losses)\n",
    "    train_loss_history.append(train_loss_epoch)\n",
    "\n",
    "    # ---- Validation ----\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        for anchors, positives, negatives in val_dataloader:\n",
    "            anchors, positives, negatives = anchors.to(device), positives.to(device), negatives.to(device)\n",
    "            anchor_out = model(anchors)\n",
    "\n",
    "            loss = loss_function(anchor_out, positives, negatives) \n",
    "\n",
    "            val_losses.append(loss.item())\n",
    "\n",
    "    val_loss_epoch = np.mean(val_losses)\n",
    "    val_loss_history.append(val_loss_epoch)\n",
    "    if val_loss_epoch < min_val_loss:\n",
    "        min_val_loss = val_loss_epoch\n",
    "        # Save the best model\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "\n",
    "    print(f\"Epoch {epoch}: Train Loss = {train_loss_epoch:.4f}, Val Loss = {val_loss_epoch:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958a09ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "results['training_loss'] = train_loss_history\n",
    "results['validation_loss'] = val_loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaad7394",
   "metadata": {},
   "outputs": [],
   "source": [
    "subdir = experiment_name + time.strftime(\"_%Y%m%d_%H%M%S\")\n",
    "save_dir = Path(model_name) / subdir\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save model trace\n",
    "sample = torch.randn(1, 1024).to(device)\n",
    "trace = torch.jit.trace(model, sample)\n",
    "torch.jit.save(trace, save_dir / f\"{results['model_name']}_traced.pth\")\n",
    "\n",
    "with open(save_dir / \"results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4af5e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot train and validation loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_loss_history, label=\"Train Loss\")\n",
    "plt.plot(val_loss_history, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "# Add a notation where the minimal val loss occurred and amount\n",
    "min_val_loss = min(val_loss_history)\n",
    "min_val_loss_epoch = val_loss_history.index(min_val_loss)\n",
    "\n",
    "# Mark the point\n",
    "plt.plot(min_val_loss_epoch, min_val_loss, \"ro\", markersize=8)\n",
    "\n",
    "# Add annotation with box\n",
    "plt.annotate(\n",
    "    f\"Min: {min_val_loss:.4f}\",\n",
    "    xy=(min_val_loss_epoch, min_val_loss),\n",
    "    xytext=(10, 10),  # Offset in points\n",
    "    textcoords=\"offset points\",\n",
    "    bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"orange\", alpha=0.3),\n",
    "    # arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3,rad=0\"),\n",
    ")\n",
    "\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.savefig(save_dir / \"loss_curve.png\")  # Save BEFORE show\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a416b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side by side boxplots of baseline distances to distances after training\n",
    "model.eval()\n",
    "for name, dataset in [(\"Validation\", val_dataset)]:\n",
    "    ap_distances_after = []\n",
    "    an_distances_after = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for anchor, pos, neg in list(dataset):\n",
    "            anchor = anchor.to(device).unsqueeze(0)\n",
    "\n",
    "            anchor_emb = model(anchor)  # Only transform anchor\n",
    "\n",
    "            # Compare to raw positive/negative (no transformation)\n",
    "            ap_dist = 1 - F.cosine_similarity(anchor_emb, pos.unsqueeze(0).to(device)).item()\n",
    "            an_dist = 1 - F.cosine_similarity(anchor_emb, neg.unsqueeze(0).to(device)).item()\n",
    "\n",
    "            ap_distances_after.append(ap_dist)\n",
    "            an_distances_after.append(an_dist)\n",
    "\n",
    "    anchor_positive_mean_distance = np.mean(ap_distances_after)\n",
    "    anchor_negative_mean_distance = np.mean(an_distances_after)\n",
    "    margins_trained = [an - ap for ap, an in zip(ap_distances_after, an_distances_after)]\n",
    "    print(f\"=== AFTER TRAINING ({name} set, Anchor mapped, Pos/Neg unchanged) ===\")\n",
    "    print(f\"Anchor-Positive distance: {anchor_positive_mean_distance:.4f} ± {np.std(ap_distances_after):.4f}\")\n",
    "    print(f\"Anchor-Negative distance: {anchor_negative_mean_distance:.4f} ± {np.std(an_distances_after):.4f}\")\n",
    "    print(f\"Average margin: {anchor_negative_mean_distance - anchor_positive_mean_distance:.4f}\")\n",
    "    print(f\"Learned margin mean: {np.mean(margins_trained):.4f} ± {np.std(margins_trained):.4f}\")\n",
    "    print(f\"Learned margin median: {np.median(margins_trained):.4f}\")\n",
    "    print(f\"Distribution of margins: min={np.min(margins_trained):.4f}, max={np.max(margins_trained):.4f}\")\n",
    "\n",
    "    # Box plot the results\n",
    "    results['trained'] = {\n",
    "        'distances_to_positive': ap_distances_after,\n",
    "        'distances_to_negative': an_distances_after,\n",
    "        'margins': [an - ap for ap, an in zip(ap_distances_after, an_distances_after)]\n",
    "    }\n",
    "    # Box plot ap_distances against ap_distances_after\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    axes[0].boxplot([results['baseline']['distances_to_positive'], results['trained']['distances_to_positive']], tick_labels=['Baseline', 'Trained Model'])\n",
    "    axes[0].set_ylabel('Cosine Distance')\n",
    "    axes[0].set_title(f\"Anchor-Positive Distances ({name} set)\")\n",
    "\n",
    "    axes[1].boxplot([results['baseline']['distances_to_negative'], results['trained']['distances_to_negative']], tick_labels=['Baseline', 'Trained Model'])\n",
    "    axes[1].set_ylabel('Cosine Distance')\n",
    "    axes[1].set_title(f\"Anchor-Negative Distances ({name} set)\")\n",
    "\n",
    "    axes[2].boxplot([results['baseline']['margins'], results['trained']['margins']], tick_labels=['Baseline', 'Trained Model'])\n",
    "    axes[2].set_ylabel('Distance(Negative) - Distance(Positive)')\n",
    "    axes[2].set_title(f\"Margins ({name} set)\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_dir / \"val_distances_boxplot.png\")  # Save BEFORE show\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564de696",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63437015",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "citeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
