{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================================CONFIG=================================\n",
            "Database         User             Host                             Port            \n",
            "citeline_db      bbasseri         localhost                        5432            \n",
            "========================================================================\n",
            "Database version: ('PostgreSQL 17.3 (Homebrew) on x86_64-apple-darwin23.6.0, compiled by Apple clang version 16.0.0 (clang-1600.0.26.6), 64-bit',)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from embedders import get_embedder\n",
        "from database.database import Database\n",
        "from time import time\n",
        "\n",
        "db = Database()\n",
        "db.test_connection()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time to embed vector {query_vector.shape}: 0.031116962432861328\n"
          ]
        }
      ],
      "source": [
        "embedder = get_embedder(model_name=\"BAAI/bge-small-en\", device=\"mps\")\n",
        "# example = {\n",
        "#     \"source_doi\": \"10.12942/lrsp-2011-4\",\n",
        "#     \"sent_original\": \"This is achieved by performing the inversion employing Artificial Neural Networks (ANNs; Carroll and Staude, 2001 , see Section 1.3) that have been previously trained with snapshots of MHD simulations, which are given in the z -scale. \",\n",
        "#     \"sent_no_cit\": \"This is achieved by performing the inversion employing Artificial Neural Networks (ANNs; Carroll and , see Section 1.3) that have been previously trained with snapshots of MHD simulations, which are given in the z -scale. \",\n",
        "#     \"sent_idx\": 182,\n",
        "#     \"citation_dois\": [\"10.1086/320984\"],\n",
        "#     # \"pubdate\": \"2011-12-31\"\n",
        "#     \"pubdate\": \"2025-04-01\"\n",
        "# }\n",
        "\n",
        "example = {\n",
        "    \"source_doi\": \"10.1016/0012-8252(86)90017-6\",\n",
        "    \"sent_original\": \"After Kohout et al., 1977. type of flow is involved in the formation of the Boulder Zone and subsurface dolomites. \",\n",
        "    \"sent_no_cit\": \"After . type of flow is involved in the formation of the Boulder Zone and subsurface dolomites. \",\n",
        "    \"sent_idx\": 477,\n",
        "    \"citation_dois\": [\"10.1016/0016-7037(77)90238-1\"],\n",
        "    \"pubdate\": \"1986-12-31\"\n",
        "}\n",
        "start = time()\n",
        "query_vector = embedder([example[\"sent_no_cit\"]])[0]\n",
        "print(\"Time to embed vector {query_vector.shape}:\", time() - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Session resources set for query optimization:\n",
            "\n",
            "                SET synchronous_commit = 'on';\n",
            "                -- SET wal_level = 'replica';\n",
            "                -- SET max_wal_size = 'DEFAULT';\n",
            "                SET maintenance_work_mem = '1GB';\n",
            "                -- SET random_page_cost = '1.1';\n",
            "                -- SET parallel_tuple_cost = '0.1';\n",
            "                -- SET parallel_setup_cost = '1000';\n",
            "                SET max_parallel_workers = '60';\n",
            "                SET work_mem = '1GB';\n",
            "                SET max_parallel_workers_per_gather = '60';\n",
            "                -- SET shared_buffers = '28GB';\n",
            "                SET effective_cache_size = '86GB';\n",
            "                -- SET effective_io_concurrency = '200';\n",
            "            \n",
            "  Query execution time: 5.66 seconds\n",
            "  Found 1000 results\n",
            "top_k: 1000\n",
            "Time take: 5.67 seconds\n",
            "Results: 1000\n"
          ]
        }
      ],
      "source": [
        "start = time()\n",
        "results = db.query_vector_column(\n",
        "    query_vector=query_vector, \n",
        "    target_column='bge', \n",
        "    target_table='lib', \n",
        "    use_index=True, \n",
        "    pubdate=example['pubdate'],\n",
        "    top_k=1000, probes=1472)\n",
        "print(f\"Time take: {time() - start:.2f} seconds\")\n",
        "print(f\"Results: {len(results)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "72\n"
          ]
        }
      ],
      "source": [
        "result_dois = [result.doi for result in results]\n",
        "print(result_dois.index(example['citation_dois'][0]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "'foo' is not in list",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mresult_dois\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfoo\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n",
            "\u001b[0;31mValueError\u001b[0m: 'foo' is not in list"
          ]
        }
      ],
      "source": [
        "print(result_dois.index('foo'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "small_bodies = research[research['body'].str.len() < 1000]\n",
        "small_bodies.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mid_bodies = research[(research['body'].str.len() >= 1000) & (research['body'].str.len() < 5000)]\n",
        "mid_bodies.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "research.iloc[10138]['pubdate']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# df = mid_bodies\n",
        "mid_bodies = research[(research['body'].str.len() >= 1000)\n",
        "                      & (research['body'].str.len() < 5000)]\n",
        "mid_bodies['pubdate'] = mid_bodies['pubdate'].str.replace(\n",
        "    r'-00', '-01', regex=True)\n",
        "# mid_bodies['pubdate'] = mid_bodies['pubdate'].str.replace(\n",
        "#     r'-00-', '-01', regex=True)\n",
        "mid_bodies['pubdate'] = pd.to_datetime(\n",
        "    mid_bodies['pubdate'], format='%Y-%m-%d', errors='coerce')\n",
        "mid_bodies['pubdate'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mid_bodies['pubdate'].dt.strftime('%Y-%m-%d').tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dupes = []\n",
        "for record in reviews.to_dict(orient='records'):\n",
        "    if record['doi'] in research.doi.values:\n",
        "        dupes.append(record)\n",
        "print(\"Done\")\n",
        "print(len(dupes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bert = EncoderEmbedder(model_name='bert-base-uncased', device='mps', normalize=False)\n",
        "bert.model.config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "from database.database import Database\n",
        "\n",
        "load_dotenv('.env', override=True)\n",
        "\n",
        "# Database setup\n",
        "db_params = {\n",
        "    'dbname': os.getenv('DB_NAME'),\n",
        "    'user': os.getenv('DB_USER'),\n",
        "    'password': os.getenv('DB_PASSWORD'),\n",
        "    'host': os.getenv('DB_HOST'),\n",
        "    'port': os.getenv('DB_PORT')\n",
        "}\n",
        "db = Database(db_params)\n",
        "db.test_connection()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from database.database import Database\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv()\n",
        "db_params = {\n",
        "    'dbname': os.getenv('DB_NAME'),\n",
        "    'user': os.getenv('DB_USER'),\n",
        "    'password': os.getenv('DB_PASSWORD'),\n",
        "    'host': os.getenv('DB_HOST'),\n",
        "    'port': os.getenv('DB_PORT'),\n",
        "}\n",
        "db = Database(db_params)\n",
        "\n",
        "db.test_connection()\n",
        "print(db.db_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import psycopg2\n",
        "from time import time\n",
        "conn = psycopg2.connect(**db.db_params)\n",
        "cursor = conn.cursor()\n",
        "\n",
        "cursor.execute('SELECT text FROM chunks;')\n",
        "embedding_times = []\n",
        "for i in range(30):\n",
        "    rows = [row[0] for row in cursor.fetchmany(1024)]\n",
        "    start = time()\n",
        "    embeddings = embedder(rows)\n",
        "    end = time()\n",
        "    embedding_times.append(end - start)\n",
        "    print(f'Batch {i+1} took {end - start:.2f} seconds. Shape: {embeddings.shape}')\n",
        "\n",
        "print(f'Average time: {sum(embedding_times) / len(embedding_times):.2f} seconds')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import psycopg2\n",
        "from time import time\n",
        "\n",
        "averages = []\n",
        "batch_size = 1\n",
        "while batch_size < 2_500_000:\n",
        "    try:\n",
        "        # Get chunks from the database\n",
        "        conn = psycopg2.connect(**db.db_params)\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute(\n",
        "            f\"SELECT text FROM chunks LIMIT {batch_size}\")\n",
        "        rows = cursor.fetchall()\n",
        "        conn.close()\n",
        "        chunks = [row[0] for row in rows]\n",
        "        print(f\"Got {len(chunks)} chunks\")\n",
        "\n",
        "        # Embed the chunks\n",
        "        start = time()\n",
        "        result = embedder(chunks)\n",
        "        duration = time() - start\n",
        "        print(f\"Result shape: {result.shape}\")\n",
        "        averages.append(duration/batch_size)\n",
        "        print(f\"Batch size {batch_size} took {duration} seconds ({duration/batch_size} per chunk)\")\n",
        "        batch_size *= 2\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(chunks[234])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = db.query_vector_table('bge', query_vector=embeddings[0], metric='vector_cosine_ops', top_k=5)\n",
        "for result in results:\n",
        "    print(result.similarity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ip_results = db.query_vector_table('bge', query_vector=embeddings[0], metric='vector_ip_ops', top_k=5)\n",
        "for result in ip_results:\n",
        "    print(result.similarity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from random import random\n",
        "\n",
        "plt.figure()\n",
        "x = [random() for _ in range(100)]\n",
        "y = [-x_i for x_i in x]\n",
        "plt.plot(x, y, marker='o', label='Average Score')\n",
        "plt.xlabel('Distance (n = 123)')\n",
        "plt.grid(True)\n",
        "plt.text(0.95, 0.05, \"n = 123\", horizontalalignment='right', verticalalignment='bottom')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "citeline",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
