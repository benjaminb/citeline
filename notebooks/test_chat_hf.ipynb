{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13ce81a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...qOCV\n"
     ]
    }
   ],
   "source": [
    "# Set up environment\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint, HuggingFacePipeline\n",
    "from langchain_core.messages import (\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    ")\n",
    "\"\"\"\n",
    "requirements: text-generation bitsandbytes\n",
    "\"\"\"\n",
    "\n",
    "# Ensure HF token is set in environment vars\n",
    "load_dotenv('.env')\n",
    "print(\"...\" + os.environ[\"HUGGINGFACE_API_TOKEN\"][-4:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8270adca",
   "metadata": {},
   "source": [
    "## Instantiate from Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "650897d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = HuggingFaceEndpoint(\n",
    "#     repo_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
    "#     task=\"text-generation\",\n",
    "#     max_new_tokens=512,\n",
    "#     do_sample=False,\n",
    "#     repetition_penalty=1.03,\n",
    "# )\n",
    "\n",
    "# chat_model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faa4faa",
   "metadata": {},
   "source": [
    "### Test run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cfc8a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You're a helpful assistant\"),\n",
    "    HumanMessage(content=\"What happens when an unstoppable force meets an immovable object?\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24406e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ai_msg = chat_model.invoke(messages)\n",
    "# print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ea5e124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(ai_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2afc027",
   "metadata": {},
   "source": [
    "The HuggingFaceEndpoint only returns the `AIMessage`, not the chat history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda1a6a8",
   "metadata": {},
   "source": [
    "## Instantiate from Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1dd07aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# llm = HuggingFacePipeline.from_model_id(\n",
    "#     model_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
    "#     task=\"text-generation\",\n",
    "#     pipeline_kwargs=dict(\n",
    "#         max_new_tokens=512,\n",
    "#         do_sample=False,\n",
    "#         # repetition_penalty=1.03,\n",
    "#     ),\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dde7f2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat_model = ChatHuggingFace(llm=llm)\n",
    "# ai_msg = chat_model.invoke(messages)\n",
    "# print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b988ee02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To get just the ai response, you must parse\n",
    "# if \"<|assistant|>\" in ai_msg.content:\n",
    "#     response = ai_msg.content.split(\"<|assistant|>\")[-1]\n",
    "#     print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b48382",
   "metadata": {},
   "source": [
    "## Instantiate with Pipeline and Quantization: only supported on CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a76633a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BitsAndBytesConfig\n",
    "\n",
    "# quantization_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=\"float16\",\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "# )\n",
    "\n",
    "# llm = HuggingFacePipeline.from_model_id(\n",
    "#     model_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
    "#     task=\"text-generation\",\n",
    "#     pipeline_kwargs=dict(\n",
    "#         max_new_tokens=512,\n",
    "#         do_sample=False,\n",
    "#         repetition_penalty=1.03,\n",
    "#         return_full_text=False,\n",
    "#     ),\n",
    "#     model_kwargs={\"quantization_config\": quantization_config},\n",
    "# )\n",
    "\n",
    "# chat_model = ChatHuggingFace(llm=llm)\n",
    "# ai_msg = chat_model.invoke(messages)\n",
    "# print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "405b59e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pydantic model for a field \"names\", which will be a list of strings\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class NameList(BaseModel):\n",
    "    names: list[str] = Field(\n",
    "        default=[],\n",
    "        description=\"A list of names\",\n",
    "        title=\"Names\",\n",
    "        example=[\"Smith\", \"Jones\", \"van der Merwe\"],\n",
    "    )\n",
    "    # Optional: Add an example to the schema\n",
    "    # This is not necessary for the output parser to work, but can be useful for documentation\n",
    "    # and validation purposes.\n",
    "\n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            \"example\": {\n",
    "                \"names\": [\"Smith\", \"Jones\", \"van der Merwe\"],\n",
    "            }\n",
    "        }\n",
    "\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "parser = PydanticOutputParser(pydantic_object=NameList)\n",
    "\n",
    "prompt = \"\"\"You're a helpful assistant that only returns JSON. \n",
    "Read the following text and write out a JSON object with a \"names\" key having a string array containing all the names of people in the text.\n",
    "\n",
    "<examples>\n",
    "Input: \"Smith and Jones went to the store\"\n",
    "Output: {\"names\": [\"Smith\", \"Jones\"]}\n",
    "\n",
    "Input: \"Smith and Jones went to the store. van der Merwe was there too.\"\n",
    "Output: {\"names\": [\"Smith\", \"Jones\", \"van der Merwe\"]}\n",
    "</examples>\n",
    "\n",
    "Input: \"See Dickenson 2017 or Brekki 2018 for more information on the topic\"\n",
    "Output: \n",
    "\"\"\"\n",
    "\n",
    "# Use the PydanticOutputParser to get the format instructions\n",
    "pd_prompt = f\"\"\"You're a helpful assistant that only returns JSON. \n",
    "Read the following text and write out a JSON object with a \"names\" key having a string array containing all the names of people in the text.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6de50c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = SystemMessage(content=pd_prompt)\n",
    "human_message = HumanMessage(\n",
    "    content=\"Write out the names from this text: See Dickenson 2017 or Brekki 2018 for more information on the topic\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "529b954f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ai_msg = chat_model.invoke([system_message, human_message])\n",
    "# print(ai_msg.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58fbcb9",
   "metadata": {},
   "source": [
    "## Pipeline for Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89216be0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c0cc344cb4f4e8587839debacaf5c31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer pad token: None\n"
     ]
    }
   ],
   "source": [
    "# Use AutoTokenizer and AutoModelForCausalLM to load the model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "print(f\"Tokenizer pad token: {tokenizer.pad_token}\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    # max_length=512,\n",
    "    do_sample=False,\n",
    "    top_p=1.0,\n",
    "    temperature=0.001,\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "chat_model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "450988fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminbasseri/miniforge3/envs/citeline/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.001` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You're a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What happens when an unstoppable force meets an immovable object?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "The concept of an unstoppable force meeting an immovable object is a classic thought experiment in philosophy, particularly\n"
     ]
    }
   ],
   "source": [
    "# Regular response\n",
    "ai_msg = chat_model.invoke(messages)\n",
    "ai_msg.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8e59c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminbasseri/miniforge3/envs/citeline/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.001` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "You're a helpful assistant that only returns JSON. \n",
      "Read the following text and write out a JSON object with a \"names\" key having a string array containing all the names of people in the text.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Write out the names from this text: See Dickenson 2017 or Brekki 2018 for more information on the topic<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "{\n",
      "  \"names\": [\"Dickenson\", \"Brekki\"]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# With expected formatted output\n",
    "ai_msg = chat_model.invoke([system_message, human_message])\n",
    "ai_msg.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a731147a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "{\n",
      "  \"names\": [\"Dickenson\", \"Brekki\"]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "if \"assistant<|end_header_id|>\" in ai_msg.content:\n",
    "    response = ai_msg.content.split(\"assistant<|end_header_id|>\")[-1]\n",
    "    print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "841806b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NameList(names=['Dickenson', 'Brekki'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.parse(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bd18f8",
   "metadata": {},
   "source": [
    "### Pydantic prompt instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a41a82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminbasseri/miniforge3/envs/citeline/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.001` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "Extract the names from the text and return them as JSON.\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"example\": {\"names\": [\"Smith\", \"Jones\", \"van der Merwe\"]}, \"properties\": {\"names\": {\"default\": [], \"description\": \"A list of names\", \"example\": [\"Smith\", \"Jones\", \"van der Merwe\"], \"items\": {\"type\": \"string\"}, \"title\": \"Names\", \"type\": \"array\"}}}\n",
      "```\n",
      "\n",
      "DO NOT write any code or any text except the JSON object.\n",
      "\n",
      "Now read the following text and write out the JSON object as described above:<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Text: 'See Dickenson 2017 or Brekki 2018 for more information on the topic'<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "{\"example\": {\"names\": [\"Dickenson\", \"Brekki\"]}, \"properties\": {\"\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Extract the names from the text and return them as JSON.\n",
    "{parser.get_format_instructions()}\n",
    "\n",
    "DO NOT write any code or any text except the JSON object.\n",
    "\n",
    "Now read the following text and write out the JSON object as described above:\n",
    "\"\"\"\n",
    "\n",
    "system_message = SystemMessage(content=prompt)\n",
    "human_message = HumanMessage(content=\"Text: 'See Dickenson 2017 or Brekki 2018 for more information on the topic'\")\n",
    "ai_msg = chat_model.invoke([system_message, human_message])\n",
    "ai_msg.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753b242d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "citeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
