{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Earth_Science_Reviews', 'Planetary_Reviews', 'Astro_Reviews'])\n",
      "Journal keys: dict_keys(['documents', 'metadatas', 'ids'])\n",
      "dict_keys(['bibcode', 'abstract', 'aff', 'author', 'bibstem', 'doctype', 'doi', 'id', 'keyword', 'pubdate', 'title', 'read_count', 'reference', 'citation_count', 'citation', 'body'])\n",
      "['10.1146/annurev.astro.46.060407.145222', '10.48550/arXiv.0909.0948']\n",
      "<class 'list'>\n",
      "['1929ApJ....70...11R', '1956RvMP...28...53S', '1958ZA.....46..108B']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "\n",
    "PATH_TO_DATA = 'data/processed_for_chroma/reviews'\n",
    "FILENAMES = os.listdir(PATH_TO_DATA)\n",
    "review_data = dict()\n",
    "\n",
    "for filename in FILENAMES:\n",
    "    with open(f'{PATH_TO_DATA}/{filename}', 'r') as file:\n",
    "        review_data[os.path.splitext(os.path.basename(filename))[0]] = json.load(file)\n",
    "\n",
    "# This one is missing the doi key\n",
    "del review_data['Earth_Science_Reviews']['metadatas'][292]\n",
    "del review_data['Earth_Science_Reviews']['documents'][292]\n",
    "del review_data['Earth_Science_Reviews']['ids'][292]\n",
    "\n",
    "# postprocessing\n",
    "for journal in review_data:\n",
    "    for i, d in enumerate(review_data[journal]['metadatas']):\n",
    "        # Convert stringified list to list\n",
    "        d['reference'] = json.loads(d['reference'])\n",
    "        d['doi'] = json.loads(d['doi'])\n",
    "\n",
    "print(review_data.keys())\n",
    "print(f\"Journal keys: {review_data['Astro_Reviews'].keys()}\")\n",
    "paper = review_data['Astro_Reviews']['metadatas'][0]\n",
    "print(paper.keys())\n",
    "print(paper['doi'])\n",
    "print(type(paper['doi']))\n",
    "print(paper['reference'][:3])\n",
    "all_reviews = [\n",
    "    record for journal in review_data for record in review_data[journal]['metadatas']]\n",
    "print(len(all_reviews))\n",
    "print(all_reviews[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = 'data/processed_for_chroma/research'\n",
    "FILENAMES = os.listdir(PATH_TO_DATA)\n",
    "research_data = dict()\n",
    "\n",
    "for filename in FILENAMES:\n",
    "    with open(f'{PATH_TO_DATA}/{filename}', 'r') as file:\n",
    "        research_data[os.path.splitext(os.path.basename(filename))[\n",
    "            0]] = json.load(file)\n",
    "\n",
    "\n",
    "# postprocessing\n",
    "for journal in research_data:\n",
    "    for i, d in enumerate(research_data[journal]['metadatas']):\n",
    "        if not 'doi' in d:\n",
    "            del research_data[journal]['metadatas'][i]\n",
    "            del research_data[journal]['documents'][i]\n",
    "            del research_data[journal]['ids'][i]\n",
    "\n",
    "for journal in research_data:\n",
    "    for d in research_data[journal]['metadatas']:\n",
    "\n",
    "        # Convert stringified list to list\n",
    "        d['reference'] = json.loads(d['reference'])\n",
    "        d['doi'] = json.loads(d['doi'])\n",
    "\n",
    "print(research_data.keys())\n",
    "research_data['Astro_Research'].keys()\n",
    "\n",
    "all_data = all_reviews + \\\n",
    "    [record for journal in review_data for record in review_data[journal]['metadatas']]\n",
    "print(f\"All data: {len(all_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2000A&A...363.1091B', '2000A&AS..142..467B', '2000MNRAS.311..535B', '2000MNRAS.312..116B']\n"
     ]
    }
   ],
   "source": [
    "# How many bibcodes are there in this paper's references that start with 1929 and end with 'R'?\n",
    "import re\n",
    "pattern = r'^2000.*B$'\n",
    "matches = [s for s in paper['reference'] if re.match(pattern, s)]\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re.compile(\"([A-Z][a-zA-ZÀ-ÖØ-öø-ÿ-]*(?:'[A-Z][a-zA-ZÀ-ÖØ-öø-ÿ-]*)?(?:,?\\\\s[A-Z][a-zA-ZÀ-ÖØ-öø-ÿ-]*(?:'[A-Z][a-zA-ZÀ-ÖØ-öø-ÿ-]*)?)*(?: et al.?)?)\\\\s*\\\\(?(\\\\d{4}[a-z]?)\\\\)?\")\n",
      "Results: 1\n",
      "1 ('Delbouille et al.', '1981')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the patterns\n",
    "lastname = r\"[A-Z][a-zA-ZÀ-ÖØ-öø-ÿ-]*(?:'[A-Z][a-zA-ZÀ-ÖØ-öø-ÿ-]*)?\"\n",
    "year = r\"\\(?(\\d{4}[a-z]?)\\)?\"\n",
    "name_sep = r\",?\\s\"\n",
    "INLINE_CITATION_PATTERN = fr\"({lastname}(?:{name_sep}{lastname})*(?: et al.?)?)\\s*{year}\"\n",
    "\n",
    "# Compile the regex pattern\n",
    "inline_regex = re.compile(INLINE_CITATION_PATTERN)\n",
    "\n",
    "print(inline_regex)\n",
    "\n",
    "test = \" Delbouille et al. 1981 the future (Section 5). 2. INGREDIENTS FOR SOLAR ABUNDANCE ANALYSIS 2.1. Observations Analyses of th\"\n",
    "\n",
    "# Find all matches using the compiled pattern\n",
    "matches = inline_regex.finditer(test)\n",
    "results = [match for match in matches]\n",
    "print(f\"Results: {len(results)}\")\n",
    "\n",
    "# Print the groups of each match\n",
    "for i, result in enumerate(results):\n",
    "    print(i+1, result.groups())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the inline citations from a paper's body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: 424\n",
      "[('Russell', '1929'),\n",
      " ('Suess Urey', '1956'),\n",
      " ('Goldberg, Müller Aller', '1960'),\n",
      " ('Anders Grevesse', '1989'),\n",
      " ('Grevesse Sauval', '1998')]\n"
     ]
    }
   ],
   "source": [
    "def get_inline_citations(record: dict) -> list[tuple[str, str]]:\n",
    "    return [match.groups() for match in inline_regex.finditer(record['body'])]\n",
    "\n",
    "# inline_citations = [match.groups() for match in inline_regex.finditer(paper['body'])]\n",
    "inline_citations = get_inline_citations(paper)\n",
    "print(f\"Results: {len(inline_citations)}\")\n",
    "pprint(inline_citations[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to resolve incline citation to bibcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: 206\n",
      "[(('Russell', '1929'), '1929ApJ....70...11R'), (('Suess Urey', '1956'), '1956RvMP...28...53S'), (('Goldberg, Müller Aller', '1960'), '1960ApJS....5....1G'), (('Anders Grevesse', '1989'), '1989GeCoA..53..197A'), (('Grevesse Sauval', '1998'), '1998SSRv...85..161G')]\n"
     ]
    }
   ],
   "source": [
    "# for each result, get the first letter of the first author's last name\n",
    "# get the year\n",
    "def bibcode_regex(author: str, year: str):\n",
    "    \"\"\"\n",
    "    Given first author and year, return a regex pattern for the\n",
    "    corresponding bibcode\n",
    "    \"\"\"\n",
    "    initial = author[0]\n",
    "    year = year[:4] # cut off any letters at the end\n",
    "    pattern = fr'^{year}.*{initial}$'\n",
    "    return re.compile(pattern)\n",
    "\n",
    "def bibcode_matches(inline_citation: tuple[str, str], references: list[str]) -> int:\n",
    "    \"\"\"\n",
    "    Given an inline citation and a list of references, return the number of\n",
    "    references that match the inline citation's bibcode regex pattern\n",
    "    \"\"\"\n",
    "    pattern = bibcode_regex(*inline_citation)\n",
    "    return [s for s in references if pattern.match(s)]\n",
    "\n",
    "def make_citation_bibcode_list(inline_citations: list[tuple[str, str]], references: list[str]) -> list[tuple[tuple[str, str], str]]:\n",
    "    \"\"\"\n",
    "    Given a paper's list of inline citations and list of references, return a list of\n",
    "    tuples where the first element is the inline citation and the second element\n",
    "    is the corresponding bibcode from the references list where there is exactly one match\n",
    "    \"\"\"\n",
    "    return [(citation, matches[0]) for citation in inline_citations \n",
    "            if len((matches := bibcode_matches(citation, references))) == 1]\n",
    "\n",
    "usable_citations = make_citation_bibcode_list(inline_citations, paper['reference'])\n",
    "print(f\"Results: {len(usable_citations)}\")\n",
    "print(usable_citations[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: 5944\n",
      "['1975E&PSL..26..207S', '1983E&PSL..64..295W', '1997E&PSL.148..243B', '1978E&PSL..40...25M', '1988E&PSL..90..297H']\n"
     ]
    }
   ],
   "source": [
    "def get_all_bibcodes_from_file(path: str) -> list[str]:\n",
    "    with open(path, 'r') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "bibcodes = get_all_bibcodes_from_file('data/bibcodes.json')\n",
    "print(f\"Results: {len(bibcodes)}\")\n",
    "print(bibcodes[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing records: 100%|██████████| 1/1 [00:00<00:00, 41.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In dataset: 12\n",
      "Out of dataset: 194\n",
      "[{'inline_citation': ('Russell', '1929'),\n",
      "  'reference_bibcode': '1929ApJ....70...11R',\n",
      "  'source_bibcode': '2009ARA&A..47..481A'},\n",
      " {'inline_citation': ('Suess Urey', '1956'),\n",
      "  'reference_bibcode': '1956RvMP...28...53S',\n",
      "  'source_bibcode': '2009ARA&A..47..481A'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def resolve_inline_references(records, bibcodes):\n",
    "    in_dataset, out_of_dataset = [], []\n",
    "    for record in tqdm(records, desc='Processing records'):\n",
    "        usable_citations = make_citation_bibcode_list(get_inline_citations(record), record['reference'])\n",
    "        for citation in usable_citations:\n",
    "            # Construct the citation dictionary\n",
    "            inline_citation, bibcode = citation\n",
    "            cite_dict = {'source_bibcode': record['bibcode'],\n",
    "                         'inline_citation': inline_citation, \n",
    "                         'reference_bibcode': bibcode}\n",
    "\n",
    "            # Determine if the referenced bibcode is in the dataset or not\n",
    "            in_dataset.append(cite_dict) if bibcode in bibcodes else out_of_dataset.append(cite_dict)\n",
    "    return in_dataset, out_of_dataset\n",
    "\n",
    "have, dont_have = resolve_inline_references(review_data['Astro_Reviews']['metadatas'][:1], bibcodes)\n",
    "print(f\"In dataset: {len(have)}\")\n",
    "print(f\"Out of dataset: {len(dont_have)}\")\n",
    "pprint(dont_have[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2984\n",
      "dict_keys(['bibcode', 'abstract', 'aff', 'author', 'bibstem', 'doctype', 'doi', 'id', 'keyword', 'pubdate', 'title', 'read_count', 'reference', 'citation_count', 'citation', 'body'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing records: 100%|██████████| 2984/2984 [00:33<00:00, 90.23it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique in dataset bibcodes: 2048\n",
      "Unique out of dataset bibcodes: 73055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Get all the review papers and get a list of their unresolved references\n",
    "\n",
    "\n",
    "\n",
    "have, dont_have = resolve_inline_references(all_reviews, bibcodes)\n",
    "bibcodes_out_of_dataset = set(\n",
    "    [cite['reference_bibcode'] for cite in dont_have])\n",
    "bibcodes_in_dataset = set([cite['reference_bibcode'] for cite in have])\n",
    "print(f\"Unique in dataset bibcodes: {len(bibcodes_in_dataset)}\")\n",
    "print(f\"Unique out of dataset bibcodes: {len(bibcodes_out_of_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysbd\n",
    "\n",
    "\n",
    "def get_inline_citations(text: str) -> list[tuple[str, str]]:\n",
    "    return [match.groups() for match in inline_regex.finditer(text)]\n",
    "\n",
    "# def make_evaluation_samples(record):\n",
    "#     # Split the body into sentences\n",
    "#     splitter = pysbd.Segmenter(language=\"en\", clean=False)\n",
    "#     sentences = splitter.segment(record['body'])\n",
    "#     examples = []\n",
    "#     for sentence in sentences:\n",
    "#         usable_sentence = True\n",
    "#         sentence_bibcodes = []\n",
    "#         inline_citations = get_inline_citations(sentence)\n",
    "#         for citation in inline_citations:\n",
    "#             bibcodes = bibcode_matches(citation, record['reference'])\n",
    "#             if len(bibcodes) != 1:\n",
    "#                 usable_sentence = False\n",
    "#                 break\n",
    "#             # Exactly one bibcode\n",
    "#             sentence_bibcodes.append(bibcodes[0])\n",
    "#         if usable_sentence:\n",
    "#             examples.append((record['doi'], sentence, sentence_bibcodes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_example(record, sentence):\n",
    "    \"\"\"\n",
    "    Takes all the inline citations of a sentence and if it can resolve them to dois\n",
    "    then it returns the \"\"\"\n",
    "    def citation_to_doi(citation):\n",
    "        \"\"\"\n",
    "        Takes a single inline citation as tuple of (author, year) and determines if there is a unique\n",
    "        matching bibcode in the record's references. If so, it continues to look for a unique\n",
    "        doi matching that bibcode in the entire dataset. It returns the doi if resolved, otherwise None.\n",
    "        \"\"\"\n",
    "        bibcodes = bibcode_matches(citation, record['reference'])\n",
    "        if len(bibcodes) != 1:\n",
    "            return None\n",
    "        matching_dois = [record['doi'][0] for record in all_research if record['bibcode'] == bibcodes[0]]\n",
    "        if len(matching_dois) != 1:\n",
    "            return None\n",
    "        return matching_dois[0]\n",
    "    \n",
    "    inline_citations = get_inline_citations(sentence)\n",
    "    citation_dois = []\n",
    "    for citation in inline_citations:\n",
    "        if not (doi := citation_to_doi(citation)):\n",
    "            break\n",
    "        citation_dois.append(doi)\n",
    "\n",
    "    # If all citations resolved to dois, return the example\n",
    "    if len(inline_citations) != len(citation_dois):\n",
    "        return None\n",
    "    return {\n",
    "            'source_doi': record['doi'][0],\n",
    "            'sentence': sentence,\n",
    "            'citation_dois': citation_dois\n",
    "           }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_examples_from_record(record):\n",
    "    splitter = pysbd.Segmenter(language=\"en\", clean=False)\n",
    "    sentences = splitter.segment(record['body'])\n",
    "    return [\n",
    "        example for sentence in sentences if (example := sentence_to_example(record, sentence))\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = create_examples_from_record(\n",
    "    review_data['Astro_Reviews']['metadatas'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source_doi': '10.1146/annurev-astro-081811-125615',\n",
       " 'sentence': '1. INTRODUCTION The origin and evolution of galaxies are among the most intriguing and complex chapters in the formation of cosmic structure, and observations in this field have accumulated at an astonishing pace. ',\n",
       " 'citation_dois': []}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/test_set.jsonl', 'a') as file:\n",
    "    for record in review_data['Astro_Reviews']['metadatas'][0:2]:\n",
    "        examples = create_examples_from_record(record)\n",
    "        for example in examples:\n",
    "            json.dump(example, file)\n",
    "            file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'source_doi': '10.1146/annurev.astro.46.060407.145222',\n",
       "  'sentence': '1. INTRODUCTION The origin and evolution of galaxies are among the most intriguing and complex chapters in the formation of cosmic structure, and observations in this field have accumulated at an astonishing pace. ',\n",
       "  'citation_dois': []},\n",
       " {'source_doi': '10.1146/annurev.astro.46.060407.145222',\n",
       "  'sentence': 'Photometric redshifts have become an unavoidable tool for placing faint galaxies onto a cosmic timeline. ',\n",
       "  'citation_dois': []},\n",
       " {'source_doi': '10.1146/annurev.astro.46.060407.145222',\n",
       "  'sentence': 'The Galaxy Evolution Explorer (GALEX) satellite has quantified the UV galaxy luminosity function (LF) of galaxies in the local Universe and its evolution at z ≲1. ',\n",
       "  'citation_dois': []}]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10.1146/annurev-astro-081811-125615', '10.48550/arXiv.1403.0007']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data['Astro_Reviews']['metadatas'][1]['doi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "citeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
