{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data/json/Astro_Reviews.json: 996/1000 have all required keys\n"
          ]
        }
      ],
      "source": [
        "from preprocessing import load_dataset\n",
        "import pandas as pd\n",
        "# 1. Get all the review papers loaded\n",
        "reviews = load_dataset('data/json/Astro_Reviews.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "for record in reviews:\n",
        "    if 'keyword' in record:\n",
        "        print(\"this has a keyword key\")\n",
        "# 2. Get all the research papers loaded\n",
        "# research = pd.read_json('data/preprocessed/research.jsonl', lines=True)\n",
        "\n",
        "\n",
        "\n",
        "    # it shouldn't be!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reviews['pubdate'].str.replace(r'-00$', '-01', regex=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "research = pd.read_json('data/preprocessed/research.jsonl', lines=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "small_bodies = research[research['body'].str.len() < 1000]\n",
        "small_bodies.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mid_bodies = research[(research['body'].str.len() >= 1000) & (research['body'].str.len() < 5000)]\n",
        "mid_bodies.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "research.iloc[10138]['pubdate']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# df = mid_bodies\n",
        "mid_bodies = research[(research['body'].str.len() >= 1000)\n",
        "                      & (research['body'].str.len() < 5000)]\n",
        "mid_bodies['pubdate'] = mid_bodies['pubdate'].str.replace(\n",
        "    r'-00', '-01', regex=True)\n",
        "# mid_bodies['pubdate'] = mid_bodies['pubdate'].str.replace(\n",
        "#     r'-00-', '-01', regex=True)\n",
        "mid_bodies['pubdate'] = pd.to_datetime(\n",
        "    mid_bodies['pubdate'], format='%Y-%m-%d', errors='coerce')\n",
        "mid_bodies['pubdate'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mid_bodies['pubdate'].dt.strftime('%Y-%m-%d').tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dupes = []\n",
        "for record in reviews.to_dict(orient='records'):\n",
        "    if record['doi'] in research.doi.values:\n",
        "        dupes.append(record)\n",
        "print(\"Done\")\n",
        "print(len(dupes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bert = EncoderEmbedder(model_name='bert-base-uncased', device='mps', normalize=False)\n",
        "bert.model.config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "from database.database import Database\n",
        "\n",
        "load_dotenv('.env', override=True)\n",
        "\n",
        "# Database setup\n",
        "db_params = {\n",
        "    'dbname': os.getenv('DB_NAME'),\n",
        "    'user': os.getenv('DB_USER'),\n",
        "    'password': os.getenv('DB_PASSWORD'),\n",
        "    'host': os.getenv('DB_HOST'),\n",
        "    'port': os.getenv('DB_PORT')\n",
        "}\n",
        "db = Database(db_params)\n",
        "db.test_connection()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from database.database import Database\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv()\n",
        "db_params = {\n",
        "    'dbname': os.getenv('DB_NAME'),\n",
        "    'user': os.getenv('DB_USER'),\n",
        "    'password': os.getenv('DB_PASSWORD'),\n",
        "    'host': os.getenv('DB_HOST'),\n",
        "    'port': os.getenv('DB_PORT'),\n",
        "}\n",
        "db = Database(db_params)\n",
        "\n",
        "db.test_connection()\n",
        "print(db.db_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import psycopg2\n",
        "from time import time\n",
        "conn = psycopg2.connect(**db.db_params)\n",
        "cursor = conn.cursor()\n",
        "\n",
        "cursor.execute('SELECT text FROM chunks;')\n",
        "embedding_times = []\n",
        "for i in range(30):\n",
        "    rows = [row[0] for row in cursor.fetchmany(1024)]\n",
        "    start = time()\n",
        "    embeddings = embedder(rows)\n",
        "    end = time()\n",
        "    embedding_times.append(end - start)\n",
        "    print(f'Batch {i+1} took {end - start:.2f} seconds. Shape: {embeddings.shape}')\n",
        "\n",
        "print(f'Average time: {sum(embedding_times) / len(embedding_times):.2f} seconds')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import psycopg2\n",
        "from time import time\n",
        "\n",
        "averages = []\n",
        "batch_size = 1\n",
        "while batch_size < 2_500_000:\n",
        "    try:\n",
        "        # Get chunks from the database\n",
        "        conn = psycopg2.connect(**db.db_params)\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute(\n",
        "            f\"SELECT text FROM chunks LIMIT {batch_size}\")\n",
        "        rows = cursor.fetchall()\n",
        "        conn.close()\n",
        "        chunks = [row[0] for row in rows]\n",
        "        print(f\"Got {len(chunks)} chunks\")\n",
        "\n",
        "        # Embed the chunks\n",
        "        start = time()\n",
        "        result = embedder(chunks)\n",
        "        duration = time() - start\n",
        "        print(f\"Result shape: {result.shape}\")\n",
        "        averages.append(duration/batch_size)\n",
        "        print(f\"Batch size {batch_size} took {duration} seconds ({duration/batch_size} per chunk)\")\n",
        "        batch_size *= 2\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(chunks[234])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results = db.query_vector_table('bge', query_vector=embeddings[0], metric='vector_cosine_ops', top_k=5)\n",
        "for result in results:\n",
        "    print(result.similarity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ip_results = db.query_vector_table('bge', query_vector=embeddings[0], metric='vector_ip_ops', top_k=5)\n",
        "for result in ip_results:\n",
        "    print(result.similarity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from random import random\n",
        "\n",
        "plt.figure()\n",
        "x = [random() for _ in range(100)]\n",
        "y = [-x_i for x_i in x]\n",
        "plt.plot(x, y, marker='o', label='Average Score')\n",
        "plt.xlabel('Distance (n = 123)')\n",
        "plt.grid(True)\n",
        "plt.text(0.95, 0.05, \"n = 123\", horizontalalignment='right', verticalalignment='bottom')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "citeline",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
