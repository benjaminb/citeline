{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b279793a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of DataFrame: 416\n",
      "Number of unique DOIs to process: 144\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(\"logs/long_papers.jsonl\", lines=True)\n",
    "print(f\"Length of DataFrame: {len(df)}\")\n",
    "dois_long = set(df[\"doi\"].tolist())\n",
    "print(f\"Number of unique DOIs to process: {len(dois_long)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca015372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================CONFIG=================================\n",
      "Database         User             Host                             Port            \n",
      "citelinedb       bbasseri         localhost                        5432            \n",
      "========================================================================\n",
      "Database version: ('PostgreSQL 17.5 (Homebrew) on aarch64-apple-darwin24.4.0, compiled by Apple clang version 17.0.0 (clang-1700.0.13.3), 64-bit',)\n",
      "Number of unique DOIs in contributions: 4864\n"
     ]
    }
   ],
   "source": [
    "# Get the unique dois from the database contribution table\n",
    "from database.database import Database\n",
    "\n",
    "db = Database()\n",
    "db.test_connection()\n",
    "\n",
    "results = db.query(\"SELECT DISTINCT doi FROM contributions\")\n",
    "existing_dois = {row[0] for row in results}\n",
    "print(f\"Number of unique DOIs in contributions: {len(existing_dois)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1de6844",
   "metadata": {},
   "outputs": [],
   "source": [
    "research = pd.read_json(\"data/preprocessed/research.jsonl\", lines=True)\n",
    "\n",
    "\n",
    "def reconstruct_paper(example: pd.Series) -> str:\n",
    "    return f\"{example['title']}\\n\\nAbstract: {example['abstract']}\\n\\n{example['body']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25cc5e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed paper text:\n",
      "Evidence for Ubiquitous Collimated Galactic-scale Outflows along the Star-forming Sequence at z ~ 0.5\n",
      "\n",
      "Abstract: We analyze Mg II λλ2796, 2803 and Fe II λλ2586, 2600 absorption profiles in individual spectra of 105 galaxies at 0.3 &lt; z &lt; 1.4. The galaxies, drawn from redshift surveys of the GOODS fields and the Extended Groth Strip, sample the range in star formation rates (SFRs) occupied by the star-forming sequence with stellar masses log M <SUB>*</SUB>/M <SUB>⊙</SUB> &gt;~ 9.6 down to SFR gsim 2 M <SUB>⊙</SUB> yr<SUP>-1</SUP> at 0.3 &lt; z &lt; 0.7. Using the Doppler shifts of Mg II and Fe II absorption as tracers of cool gas kinematics, we detect large-scale winds in 66 ± 5% of the galaxies. Hubble Space Telescope Advanced Camera for Surveys imaging and our spectral analysis indicate that the outflow detection rate depends primarily on galaxy orientation: winds are detected in ~89% of galaxies having inclinations (i) &lt;30° (face-on), while the wind detection rate is ~45% in \n"
     ]
    }
   ],
   "source": [
    "long_paper = research[research[\"doi\"] == \"10.1088/0004-637X/794/2/156\"].iloc[0]\n",
    "long_paper_text = reconstruct_paper(long_paper)\n",
    "print(f\"Reconstructed paper text:\\n{long_paper_text[:1000]}\")\n",
    "\n",
    "with open(\"long_Paper\", \"w\") as f:\n",
    "    f.write(long_paper_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f51367d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=\"logs/deeseek_long.log\",\n",
    "    filemode=\"w\",\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56512ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the deepseek api, which copies the openai api\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from llm.models import Findings\n",
    "\n",
    "load_dotenv()\n",
    "MAX_PAPER_LEN = 195000  # ~65k tokens, leaving ~500 tokens for the response\n",
    "\n",
    "\n",
    "def deepseek_client():\n",
    "    assert \"DEEPSEEK_API_KEY\" in os.environ, \"DEEPSEEK_API_KEY must be set in environment variables\"\n",
    "    client = OpenAI(\n",
    "        api_key=os.environ[\"DEEPSEEK_API_KEY\"],\n",
    "        base_url=\"https://api.deepseek.com\",\n",
    "    )\n",
    "    return client\n",
    "\n",
    "\n",
    "client = deepseek_client()\n",
    "\n",
    "with open(\"llm/prompts/original_contributions.txt\", \"r\") as f:\n",
    "    SYSTEM_PROMPT = f.read()\n",
    "\n",
    "\n",
    "def get_deepseek_response(paper: str):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"deepseek-chat\",\n",
    "            messages=[{\"role\": \"system\", \"content\": SYSTEM_PROMPT}, {\"role\": \"user\", \"content\": paper}],\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "            stream=False,\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "\n",
    "\n",
    "def get_contributions_from_paper(record: pd.Series) -> list[str]:\n",
    "    paper = reconstruct_paper(record)\n",
    "\n",
    "\n",
    "    # Get the deepseek API response\n",
    "    json_response = None\n",
    "    try:\n",
    "        json_response = get_deepseek_response(paper)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error parsing JSON for DOI {record['doi']}: {e}\")\n",
    "        return []\n",
    "\n",
    "    # Parse the JSON response, log any error\n",
    "    try:\n",
    "        data = json.loads(json_response)\n",
    "        findings_obj = Findings.model_validate(data)\n",
    "        return findings_obj.findings  # This is your list of strings\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error parsing JSON for DOI {record['doi']}: {e}\")\n",
    "        print(f\"JSON parse error for DOI {record['doi']}\")\n",
    "        return []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "citeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
