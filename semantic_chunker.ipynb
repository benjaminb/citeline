{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3429e4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_json(\"data/preprocessed/research.jsonl\", lines=True, nrows=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb6d2e71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bibcode</th>\n",
       "      <th>abstract</th>\n",
       "      <th>aff</th>\n",
       "      <th>author</th>\n",
       "      <th>bibstem</th>\n",
       "      <th>doctype</th>\n",
       "      <th>doi</th>\n",
       "      <th>id</th>\n",
       "      <th>pubdate</th>\n",
       "      <th>title</th>\n",
       "      <th>read_count</th>\n",
       "      <th>reference</th>\n",
       "      <th>data</th>\n",
       "      <th>citation_count</th>\n",
       "      <th>citation</th>\n",
       "      <th>body</th>\n",
       "      <th>dois</th>\n",
       "      <th>keywords</th>\n",
       "      <th>loaded_from</th>\n",
       "      <th>body_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>1984E&amp;PSL..71..229C</td>\n",
       "      <td>The oxygen isotopic composition of modern soil...</td>\n",
       "      <td>[Department of Geology and Geophysics, Univers...</td>\n",
       "      <td>[Cerling, Thure E.]</td>\n",
       "      <td>[E&amp;PSL, E&amp;PSL..71]</td>\n",
       "      <td>article</td>\n",
       "      <td>10.1016/0012-821X(84)90089-X</td>\n",
       "      <td>1564195</td>\n",
       "      <td>1984-12-01</td>\n",
       "      <td>The stable isotopic composition of modern soil...</td>\n",
       "      <td>0</td>\n",
       "      <td>[1953GeCoA...3...53C, 1963SoilS..96..239A, 196...</td>\n",
       "      <td>None</td>\n",
       "      <td>996</td>\n",
       "      <td>[1986GeCoA..50.1213Y, 1986QuRes..25...63C, 198...</td>\n",
       "      <td>Earth and Planetary Science Letters, 71 (1984)...</td>\n",
       "      <td>[10.1016/0012-821X(84)90089-X]</td>\n",
       "      <td>None</td>\n",
       "      <td>data/json/Earth_Science_Research.json</td>\n",
       "      <td>[Earth and Planetary Science Letters, 71 (1984...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>2009E&amp;PSL.288..115D</td>\n",
       "      <td>We test the research strategy of using younges...</td>\n",
       "      <td>[University of Arizona, Department of Geoscien...</td>\n",
       "      <td>[Dickinson, William R., Gehrels, George E.]</td>\n",
       "      <td>[E&amp;PSL, E&amp;PSL.288]</td>\n",
       "      <td>article</td>\n",
       "      <td>10.1016/j.epsl.2009.09.013</td>\n",
       "      <td>2589801</td>\n",
       "      <td>2009-10-01</td>\n",
       "      <td>Use of U-Pb ages of detrital zircons to infer ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[1987CaJES..24..866G, 1998GeCoA..62.2823W, 199...</td>\n",
       "      <td>None</td>\n",
       "      <td>1020</td>\n",
       "      <td>[2010ChGeo.271...13W, 2010Geo....38..459A, 201...</td>\n",
       "      <td>1 Introduction The youngest UPb ages of zircon...</td>\n",
       "      <td>[10.1016/j.epsl.2009.09.013]</td>\n",
       "      <td>None</td>\n",
       "      <td>data/json/Earth_Science_Research.json</td>\n",
       "      <td>[1 Introduction The youngest UPb ages of zirco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>1982E&amp;PSL..57..421H</td>\n",
       "      <td>We propose the following model for the origin ...</td>\n",
       "      <td>[Carnegie Institution of Washington, Departmen...</td>\n",
       "      <td>[Hofmann, Albrecht W., White, William M.]</td>\n",
       "      <td>[E&amp;PSL, E&amp;PSL..57]</td>\n",
       "      <td>article</td>\n",
       "      <td>10.1016/0012-821X(82)90161-3</td>\n",
       "      <td>1505418</td>\n",
       "      <td>1982-02-01</td>\n",
       "      <td>Mantle plumes from ancient oceanic crust</td>\n",
       "      <td>0</td>\n",
       "      <td>[1963CaJPh..41..863W, 1963JGR....68.4209G, 196...</td>\n",
       "      <td>None</td>\n",
       "      <td>1145</td>\n",
       "      <td>[1982JGR....87.4682S, 1982Natur.296..821W, 198...</td>\n",
       "      <td>- ~ #12; 422 ventional petrological thinking. ...</td>\n",
       "      <td>[10.1016/0012-821X(82)90161-3]</td>\n",
       "      <td>None</td>\n",
       "      <td>data/json/Earth_Science_Research.json</td>\n",
       "      <td>[- ~ #12; 422 ventional petrological thinking....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>1983E&amp;PSL..64..295W</td>\n",
       "      <td>Hydrothermal experiments in the temperature ra...</td>\n",
       "      <td>[Department of Geology, Rensselaer Polytechnic...</td>\n",
       "      <td>[Watson, E. Bruce, Harrison, T. Mark]</td>\n",
       "      <td>[E&amp;PSL, E&amp;PSL..64]</td>\n",
       "      <td>article</td>\n",
       "      <td>10.1016/0012-821X(83)90211-X</td>\n",
       "      <td>1534069</td>\n",
       "      <td>1983-08-01</td>\n",
       "      <td>Zircon saturation revisited: temperature and c...</td>\n",
       "      <td>0</td>\n",
       "      <td>[1951GeCoA...1..129W, 1969GeCoA..33..357B, 197...</td>\n",
       "      <td>None</td>\n",
       "      <td>2877</td>\n",
       "      <td>[1983CoMP...84...66H, 1984BVol...47..153T, 198...</td>\n",
       "      <td>Earth and Planetary Science Letters, 64 (1983)...</td>\n",
       "      <td>[10.1016/0012-821X(83)90211-X]</td>\n",
       "      <td>None</td>\n",
       "      <td>data/json/Earth_Science_Research.json</td>\n",
       "      <td>[Earth and Planetary Science Letters, 64 (1983...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>2002E&amp;PSL.196...17S</td>\n",
       "      <td>We developed a plate tectonic model for the Pa...</td>\n",
       "      <td>[Institute of Geology and Paleontology, Univer...</td>\n",
       "      <td>[Stampfli, G. M., Borel, G. D.]</td>\n",
       "      <td>[E&amp;PSL, E&amp;PSL.196]</td>\n",
       "      <td>article</td>\n",
       "      <td>10.1016/S0012-821X(01)00588-X</td>\n",
       "      <td>2244021</td>\n",
       "      <td>2002-02-01</td>\n",
       "      <td>A plate tectonic model for the Paleozoic and M...</td>\n",
       "      <td>0</td>\n",
       "      <td>[1985E&amp;PSL..74..103Z, 1992JSG....14..669D, 199...</td>\n",
       "      <td>None</td>\n",
       "      <td>1861</td>\n",
       "      <td>[2002Litho..64..155O, 2002Tectp.359..117R, 200...</td>\n",
       "      <td>1 Introduction Recent syntheses by IGCP and Te...</td>\n",
       "      <td>[10.1016/S0012-821X(01)00588-X]</td>\n",
       "      <td>None</td>\n",
       "      <td>data/json/Earth_Science_Research.json</td>\n",
       "      <td>[1 Introduction Recent syntheses by IGCP and T...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 bibcode                                           abstract  \\\n",
       "995  1984E&PSL..71..229C  The oxygen isotopic composition of modern soil...   \n",
       "996  2009E&PSL.288..115D  We test the research strategy of using younges...   \n",
       "997  1982E&PSL..57..421H  We propose the following model for the origin ...   \n",
       "998  1983E&PSL..64..295W  Hydrothermal experiments in the temperature ra...   \n",
       "999  2002E&PSL.196...17S  We developed a plate tectonic model for the Pa...   \n",
       "\n",
       "                                                   aff  \\\n",
       "995  [Department of Geology and Geophysics, Univers...   \n",
       "996  [University of Arizona, Department of Geoscien...   \n",
       "997  [Carnegie Institution of Washington, Departmen...   \n",
       "998  [Department of Geology, Rensselaer Polytechnic...   \n",
       "999  [Institute of Geology and Paleontology, Univer...   \n",
       "\n",
       "                                          author             bibstem  doctype  \\\n",
       "995                          [Cerling, Thure E.]  [E&PSL, E&PSL..71]  article   \n",
       "996  [Dickinson, William R., Gehrels, George E.]  [E&PSL, E&PSL.288]  article   \n",
       "997    [Hofmann, Albrecht W., White, William M.]  [E&PSL, E&PSL..57]  article   \n",
       "998        [Watson, E. Bruce, Harrison, T. Mark]  [E&PSL, E&PSL..64]  article   \n",
       "999              [Stampfli, G. M., Borel, G. D.]  [E&PSL, E&PSL.196]  article   \n",
       "\n",
       "                               doi       id     pubdate  \\\n",
       "995   10.1016/0012-821X(84)90089-X  1564195  1984-12-01   \n",
       "996     10.1016/j.epsl.2009.09.013  2589801  2009-10-01   \n",
       "997   10.1016/0012-821X(82)90161-3  1505418  1982-02-01   \n",
       "998   10.1016/0012-821X(83)90211-X  1534069  1983-08-01   \n",
       "999  10.1016/S0012-821X(01)00588-X  2244021  2002-02-01   \n",
       "\n",
       "                                                 title  read_count  \\\n",
       "995  The stable isotopic composition of modern soil...           0   \n",
       "996  Use of U-Pb ages of detrital zircons to infer ...           0   \n",
       "997           Mantle plumes from ancient oceanic crust           0   \n",
       "998  Zircon saturation revisited: temperature and c...           0   \n",
       "999  A plate tectonic model for the Paleozoic and M...           0   \n",
       "\n",
       "                                             reference  data  citation_count  \\\n",
       "995  [1953GeCoA...3...53C, 1963SoilS..96..239A, 196...  None             996   \n",
       "996  [1987CaJES..24..866G, 1998GeCoA..62.2823W, 199...  None            1020   \n",
       "997  [1963CaJPh..41..863W, 1963JGR....68.4209G, 196...  None            1145   \n",
       "998  [1951GeCoA...1..129W, 1969GeCoA..33..357B, 197...  None            2877   \n",
       "999  [1985E&PSL..74..103Z, 1992JSG....14..669D, 199...  None            1861   \n",
       "\n",
       "                                              citation  \\\n",
       "995  [1986GeCoA..50.1213Y, 1986QuRes..25...63C, 198...   \n",
       "996  [2010ChGeo.271...13W, 2010Geo....38..459A, 201...   \n",
       "997  [1982JGR....87.4682S, 1982Natur.296..821W, 198...   \n",
       "998  [1983CoMP...84...66H, 1984BVol...47..153T, 198...   \n",
       "999  [2002Litho..64..155O, 2002Tectp.359..117R, 200...   \n",
       "\n",
       "                                                  body  \\\n",
       "995  Earth and Planetary Science Letters, 71 (1984)...   \n",
       "996  1 Introduction The youngest UPb ages of zircon...   \n",
       "997  - ~ #12; 422 ventional petrological thinking. ...   \n",
       "998  Earth and Planetary Science Letters, 64 (1983)...   \n",
       "999  1 Introduction Recent syntheses by IGCP and Te...   \n",
       "\n",
       "                                dois keywords  \\\n",
       "995   [10.1016/0012-821X(84)90089-X]     None   \n",
       "996     [10.1016/j.epsl.2009.09.013]     None   \n",
       "997   [10.1016/0012-821X(82)90161-3]     None   \n",
       "998   [10.1016/0012-821X(83)90211-X]     None   \n",
       "999  [10.1016/S0012-821X(01)00588-X]     None   \n",
       "\n",
       "                               loaded_from  \\\n",
       "995  data/json/Earth_Science_Research.json   \n",
       "996  data/json/Earth_Science_Research.json   \n",
       "997  data/json/Earth_Science_Research.json   \n",
       "998  data/json/Earth_Science_Research.json   \n",
       "999  data/json/Earth_Science_Research.json   \n",
       "\n",
       "                                        body_sentences  \n",
       "995  [Earth and Planetary Science Letters, 71 (1984...  \n",
       "996  [1 Introduction The youngest UPb ages of zirco...  \n",
       "997  [- ~ #12; 422 ventional petrological thinking....  \n",
       "998  [Earth and Planetary Science Letters, 64 (1983...  \n",
       "999  [1 Introduction Recent syntheses by IGCP and T...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5323dbba",
   "metadata": {},
   "source": [
    "## Tuning the Semantic Chunker\n",
    "\n",
    "Goal: find the semantic chunker settings that will most accurately chunk research papers to **paragraphs**\n",
    "\n",
    "Methodology: \n",
    "1. Pick 3 random papers\n",
    "2. Using their original papers, identify the paragraph boundaries\n",
    "3. Grid Search over SemanticChunker parameters to find the one that most aligns with paragraph chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a84f651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bibcode</th>\n",
       "      <th>abstract</th>\n",
       "      <th>aff</th>\n",
       "      <th>author</th>\n",
       "      <th>bibstem</th>\n",
       "      <th>doctype</th>\n",
       "      <th>doi</th>\n",
       "      <th>id</th>\n",
       "      <th>pubdate</th>\n",
       "      <th>title</th>\n",
       "      <th>read_count</th>\n",
       "      <th>reference</th>\n",
       "      <th>data</th>\n",
       "      <th>citation_count</th>\n",
       "      <th>citation</th>\n",
       "      <th>body</th>\n",
       "      <th>dois</th>\n",
       "      <th>keywords</th>\n",
       "      <th>loaded_from</th>\n",
       "      <th>body_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>2007MNRAS.379.1599L</td>\n",
       "      <td>We describe the goals, design, implementation,...</td>\n",
       "      <td>[Institute for Astronomy, SUPA (Scottish Unive...</td>\n",
       "      <td>[Lawrence, A., Warren, S. J., Almaini, O., Edg...</td>\n",
       "      <td>[MNRAS, MNRAS.379]</td>\n",
       "      <td>article</td>\n",
       "      <td>10.1111/j.1365-2966.2007.12040.x</td>\n",
       "      <td>2344762</td>\n",
       "      <td>2007-08-01</td>\n",
       "      <td>The UKIRT Infrared Deep Sky Survey (UKIDSS)</td>\n",
       "      <td>99</td>\n",
       "      <td>[1973asqu.book.....A, 1983ApJ...264..337S, 198...</td>\n",
       "      <td>[CDS:3, IRSA:1, SIMBAD:22]</td>\n",
       "      <td>2215</td>\n",
       "      <td>[2006MNRAS.371.1722D, 2006MNRAS.372..357M, 200...</td>\n",
       "      <td>1 INTRODUCTION The UKIRT Infrared Deep Sky Sur...</td>\n",
       "      <td>[10.1111/j.1365-2966.2007.12040.x, 10.48550/ar...</td>\n",
       "      <td>[surveys, infrared: general, Astrophysics]</td>\n",
       "      <td>data/json/Astro_Research.json</td>\n",
       "      <td>[1 INTRODUCTION The UKIRT Infrared Deep Sky Su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818</th>\n",
       "      <td>1994MNRAS.271..676L</td>\n",
       "      <td>We have made a detailed comparison of the resu...</td>\n",
       "      <td>[-, -]</td>\n",
       "      <td>[Lacey, C., Cole, S.]</td>\n",
       "      <td>[MNRAS, MNRAS.271]</td>\n",
       "      <td>article</td>\n",
       "      <td>10.1093/mnras/271.3.676</td>\n",
       "      <td>3121109</td>\n",
       "      <td>1994-12-01</td>\n",
       "      <td>Merger Rates in Hierarchical Models of Galaxy ...</td>\n",
       "      <td>46</td>\n",
       "      <td>[1974ApJ...187..425P, 1977ApJS...34..425D, 197...</td>\n",
       "      <td>None</td>\n",
       "      <td>678</td>\n",
       "      <td>[1994MNRAS.271..781C, 1994astro.ph.12088M, 199...</td>\n",
       "      <td>19 94MNRAS.271. .676L Mon. Not. R. Astron. Soc...</td>\n",
       "      <td>[10.1093/mnras/271.3.676, 10.48550/arXiv.astro...</td>\n",
       "      <td>[Astrophysics]</td>\n",
       "      <td>data/json/Astro_Research.json</td>\n",
       "      <td>[19 94MNRAS.271.  .676L Mon.  Not.  R. Astron....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>2020ApJ...900..179K</td>\n",
       "      <td>To reach a deeper understanding of the origin ...</td>\n",
       "      <td>[Centre for Astrophysics Research, Department ...</td>\n",
       "      <td>[Kobayashi, Chiaki, Karakas, Amanda I., Lugaro...</td>\n",
       "      <td>[ApJ, ApJ...900]</td>\n",
       "      <td>article</td>\n",
       "      <td>10.3847/1538-4357/abae65</td>\n",
       "      <td>19402707</td>\n",
       "      <td>2020-09-01</td>\n",
       "      <td>The Origin of Elements from Carbon to Uranium</td>\n",
       "      <td>274</td>\n",
       "      <td>[1955ApJ...121..161S, 1957RvMP...29..547B, 196...</td>\n",
       "      <td>[SIMBAD:17]</td>\n",
       "      <td>490</td>\n",
       "      <td>[2020A&amp;A...642A..62A, 2020A&amp;A...643A..49H, 202...</td>\n",
       "      <td>1. Introduction Since the time of Burbidge et ...</td>\n",
       "      <td>[10.3847/1538-4357/abae65, 10.48550/arXiv.2008...</td>\n",
       "      <td>[Galaxy abundances, Stellar abundances, Chemic...</td>\n",
       "      <td>data/json/Astro_Research.json</td>\n",
       "      <td>[1. Introduction Since the time of Burbidge et...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 bibcode                                           abstract  \\\n",
       "507  2007MNRAS.379.1599L  We describe the goals, design, implementation,...   \n",
       "818  1994MNRAS.271..676L  We have made a detailed comparison of the resu...   \n",
       "452  2020ApJ...900..179K  To reach a deeper understanding of the origin ...   \n",
       "\n",
       "                                                   aff  \\\n",
       "507  [Institute for Astronomy, SUPA (Scottish Unive...   \n",
       "818                                             [-, -]   \n",
       "452  [Centre for Astrophysics Research, Department ...   \n",
       "\n",
       "                                                author             bibstem  \\\n",
       "507  [Lawrence, A., Warren, S. J., Almaini, O., Edg...  [MNRAS, MNRAS.379]   \n",
       "818                              [Lacey, C., Cole, S.]  [MNRAS, MNRAS.271]   \n",
       "452  [Kobayashi, Chiaki, Karakas, Amanda I., Lugaro...    [ApJ, ApJ...900]   \n",
       "\n",
       "     doctype                               doi        id     pubdate  \\\n",
       "507  article  10.1111/j.1365-2966.2007.12040.x   2344762  2007-08-01   \n",
       "818  article           10.1093/mnras/271.3.676   3121109  1994-12-01   \n",
       "452  article          10.3847/1538-4357/abae65  19402707  2020-09-01   \n",
       "\n",
       "                                                 title  read_count  \\\n",
       "507        The UKIRT Infrared Deep Sky Survey (UKIDSS)          99   \n",
       "818  Merger Rates in Hierarchical Models of Galaxy ...          46   \n",
       "452      The Origin of Elements from Carbon to Uranium         274   \n",
       "\n",
       "                                             reference  \\\n",
       "507  [1973asqu.book.....A, 1983ApJ...264..337S, 198...   \n",
       "818  [1974ApJ...187..425P, 1977ApJS...34..425D, 197...   \n",
       "452  [1955ApJ...121..161S, 1957RvMP...29..547B, 196...   \n",
       "\n",
       "                           data  citation_count  \\\n",
       "507  [CDS:3, IRSA:1, SIMBAD:22]            2215   \n",
       "818                        None             678   \n",
       "452                 [SIMBAD:17]             490   \n",
       "\n",
       "                                              citation  \\\n",
       "507  [2006MNRAS.371.1722D, 2006MNRAS.372..357M, 200...   \n",
       "818  [1994MNRAS.271..781C, 1994astro.ph.12088M, 199...   \n",
       "452  [2020A&A...642A..62A, 2020A&A...643A..49H, 202...   \n",
       "\n",
       "                                                  body  \\\n",
       "507  1 INTRODUCTION The UKIRT Infrared Deep Sky Sur...   \n",
       "818  19 94MNRAS.271. .676L Mon. Not. R. Astron. Soc...   \n",
       "452  1. Introduction Since the time of Burbidge et ...   \n",
       "\n",
       "                                                  dois  \\\n",
       "507  [10.1111/j.1365-2966.2007.12040.x, 10.48550/ar...   \n",
       "818  [10.1093/mnras/271.3.676, 10.48550/arXiv.astro...   \n",
       "452  [10.3847/1538-4357/abae65, 10.48550/arXiv.2008...   \n",
       "\n",
       "                                              keywords  \\\n",
       "507         [surveys, infrared: general, Astrophysics]   \n",
       "818                                     [Astrophysics]   \n",
       "452  [Galaxy abundances, Stellar abundances, Chemic...   \n",
       "\n",
       "                       loaded_from  \\\n",
       "507  data/json/Astro_Research.json   \n",
       "818  data/json/Astro_Research.json   \n",
       "452  data/json/Astro_Research.json   \n",
       "\n",
       "                                        body_sentences  \n",
       "507  [1 INTRODUCTION The UKIRT Infrared Deep Sky Su...  \n",
       "818  [19 94MNRAS.271.  .676L Mon.  Not.  R. Astron....  \n",
       "452  [1. Introduction Since the time of Burbidge et...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "sample_df = df.sample(3)\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42211ed8",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "1. Get all the first sentences of each paragraph / section\n",
    "1. Ensure the sentence strings appear in the body of each paper **exactly**\n",
    "1. Ensure the sentence strings appear in order wrt the body text on file (some papers jumble sections after OCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "635883b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Paper 0] all paragraph sentences are in the paper body: True\n",
      "[Paper 1] all paragraph sentences are in the paper body: True\n",
      "[Paper 2] all paragraph sentences are in the paper body: True\n"
     ]
    }
   ],
   "source": [
    "from data.etc.first_paragraph_sentences import ukirt, lacey, kobayashi\n",
    "\n",
    "def get_truncated_sentences(lst: list[str], max_length: int = 75) -> list:\n",
    "    return [sentence[:max_length] for sentence in lst]\n",
    "\n",
    "first_paragraph_sentences = [\n",
    "    get_truncated_sentences(ukirt),\n",
    "    get_truncated_sentences(lacey),\n",
    "    get_truncated_sentences(kobayashi)\n",
    "]\n",
    "\n",
    "def are_sentences_in_body(idx) -> bool:\n",
    "    \"\"\"\n",
    "    Check if all sentences in the list are present in the body of the text.\n",
    "    \"\"\"\n",
    "    sentences = first_paragraph_sentences[idx]\n",
    "    paper_body = sample_df.iloc[idx]['body']\n",
    "    all_there = True\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        if sentence not in paper_body:\n",
    "            print(f\"Sentence {i} not found: {sentence}\")\n",
    "            all_there = False\n",
    "    return all_there\n",
    "\n",
    "for i in range(3):\n",
    "    print(f\"[Paper {i}] all paragraph sentences are in the paper body: {are_sentences_in_body(i)}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98d4080a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence indices: [[0, 1518, 2433, 3474, 4807, 5572, 6128, 6581, 7032, 7492, 8671, 9511, 10355, 11150, 11539, 12795, 13083, 14451, 15190, 15713, 17247, 18497, 20756, 21155, 22092, 22695, 24036, 25058, 25420, 27316, 28635, 29299, 29471, 29834, 31209, 32980, 33903, 35312, 35545, 36620, 37913, 38559, 39634, 39750, 41115, 41547, 42333, 45087, 45358, 45857, 45963, 47115, 47314, 48104, 48556, 49693, 50098, 51077, 51793, 52202, 52793, 54029, 55336, 55892, 56568, 57092, 57640, 58888, 59737, 60862, 61401, 61684, 62789, 63205, 63782, 64646, 65845, 66166, 67143, 67920, 68992, 69850, 70443, 70930, 71435, 71584, 72859, 73520, 74424, 75058, 75665, 76049, 76453, 77503, 78061, 79440, 80209, 81690, 82623, 83937, 85140, 86232], [0, 2355, 4217, 7240, 7796, 8573, 10416, 11689, 12200, 12967, 13461, 13992, 15168, 15423, 16484, 18438, 19409, 19653, 20356, 22039, 23560, 24007, 24469, 26148, 26857, 28526, 30410, 31334, 33231, 34643, 35568, 36348, 37572, 38863, 39252, 39948, 42920, 44474, 45196, 47027, 49634, 50188, 52985, 54336, 55613, 56112, 56754, 58011, 59094, 59858, 62232, 63465, 64624, 66540, 67709, 68764], [0, 1125, 2058, 3741, 5369, 6138, 7206, 8394, 10113, 10407, 11540, 12459, 13377, 15127, 16444, 16812, 17751, 18263, 18962, 20050, 20492, 21225, 22897, 24268, 24778, 26111, 27077, 27943, 29186, 29868, 30291, 30909, 31993, 33211, 33981, 35223, 36484, 37345, 38269, 38785, 39308, 39871, 41586, 42260, 42693, 43552, 43835, 44378, 45810, 46997, 47702, 48393, 48877, 51599, 52815, 53364, 54090, 56268, 57248, 58508, 59027, 69722, 71398, 72940, 73947, 74497, 75134, 76201, 77023, 77532, 78167, 78971, 80413, 80784, 81660, 82450, 83191, 83837, 84614, 85338, 86309, 87869, 89160, 90333, 90953, 91369, 92670, 93340, 94367, 95247, 95908, 97877, 98384, 99177, 100731, 104921, 106077, 107122, 107779, 108166, 108893, 111579, 113594, 114584, 115032, 115462, 116103, 117325, 118158, 119447, 120935, 121391, 121753, 122182, 122577, 122763, 123229, 123828, 124446, 125181, 126372, 127634, 128623, 131018, 131818, 133695, 134502, 135462, 136109, 136593, 137213, 137840, 138524, 139047, 139544, 140451]]\n"
     ]
    }
   ],
   "source": [
    "sentence_indices = []\n",
    "for i in range(3):\n",
    "    sentences = first_paragraph_sentences[i]\n",
    "    paper_body = sample_df.iloc[i]['body']\n",
    "    sentence_indices.append([paper_body.index(sentence) for sentence in sentences])\n",
    "print(f\"Sentence indices: {sentence_indices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "778206ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All sentences are in order.\n"
     ]
    }
   ],
   "source": [
    "# Confirm the sentences are all in order\n",
    "all_in_order = True\n",
    "for i in range(3):\n",
    "    sent_idx_list = sentence_indices[i]\n",
    "    for j in range(1, len(sent_idx_list)):\n",
    "        if sent_idx_list[j] <= sent_idx_list[j - 1]:\n",
    "            print(f\"Paper {i} Sentence {j} is out of order in paper {i}: {sent_idx_list}\")\n",
    "            print(f\"  Index of sentence {j}: {sent_idx_list[j]}\")\n",
    "            print(f\"  Index of sentence {j-1}: {sent_idx_list[j-1]}\")\n",
    "            all_in_order = False\n",
    "\n",
    "if all_in_order:\n",
    "    print(\"All sentences are in order.\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f3d71b",
   "metadata": {},
   "source": [
    "## Computing Boundary Similarities\n",
    "\n",
    "1. Package the vetted paragraph start sentences as reference lists\n",
    "1. Create a grid over SemanticChunker parameters for search\n",
    "1. For each parameter config, compare SemanticChunker boundary similarity to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8fc1ae15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference lengths: [[1518, 915, 1041, 1333, 765, 556, 453, 451, 460, 1179, 840, 844, 795, 389, 1256, 288, 1368, 739, 523, 1534, 1250, 2259, 399, 937, 603, 1341, 1022, 362, 1896, 1319, 664, 172, 363, 1375, 1771, 923, 1409, 233, 1075, 1293, 646, 1075, 116, 1365, 432, 786, 2754, 271, 499, 106, 1152, 199, 790, 452, 1137, 405, 979, 716, 409, 591, 1236, 1307, 556, 676, 524, 548, 1248, 849, 1125, 539, 283, 1105, 416, 577, 864, 1199, 321, 977, 777, 1072, 858, 593, 487, 505, 149, 1275, 661, 904, 634, 607, 384, 404, 1050, 558, 1379, 769, 1481, 933, 1314, 1203, 1092, 92271], [2355, 1862, 3023, 556, 777, 1843, 1273, 511, 767, 494, 531, 1176, 255, 1061, 1954, 971, 244, 703, 1683, 1521, 447, 462, 1679, 709, 1669, 1884, 924, 1897, 1412, 925, 780, 1224, 1291, 389, 696, 2972, 1554, 722, 1831, 2607, 554, 2797, 1351, 1277, 499, 642, 1257, 1083, 764, 2374, 1233, 1159, 1916, 1169, 1055, 4311], [1125, 933, 1683, 1628, 769, 1068, 1188, 1719, 294, 1133, 919, 918, 1750, 1317, 368, 939, 512, 699, 1088, 442, 733, 1672, 1371, 510, 1333, 966, 866, 1243, 682, 423, 618, 1084, 1218, 770, 1242, 1261, 861, 924, 516, 523, 563, 1715, 674, 433, 859, 283, 543, 1432, 1187, 705, 691, 484, 2722, 1216, 549, 726, 2178, 980, 1260, 519, 10695, 1676, 1542, 1007, 550, 637, 1067, 822, 509, 635, 804, 1442, 371, 876, 790, 741, 646, 777, 724, 971, 1560, 1291, 1173, 620, 416, 1301, 670, 1027, 880, 661, 1969, 507, 793, 1554, 4190, 1156, 1045, 657, 387, 727, 2686, 2015, 990, 448, 430, 641, 1222, 833, 1289, 1488, 456, 362, 429, 395, 186, 466, 599, 618, 735, 1191, 1262, 989, 2395, 800, 1877, 807, 960, 647, 484, 620, 627, 684, 523, 497, 907, 960]]\n",
      "Paper 0 min length: 106, total length: 178503, original body length: 178503\n",
      "Paper 1 min length: 244, total length: 73075, original body length: 73075\n",
      "Paper 2 min length: 186, total length: 141411, original body length: 141411\n"
     ]
    }
   ],
   "source": [
    "\n",
    "reference_lengths = []\n",
    "for i in range(3):\n",
    "    # Compute all the paragraph lengths\n",
    "    idx_list = sentence_indices[i]\n",
    "    paragraph_lengths = [\n",
    "        idx_list[j] - idx_list[j - 1] for j in range(1, len(idx_list))\n",
    "    ]\n",
    "\n",
    "    # Add the last paragraph length\n",
    "    paragraph_lengths.append(len(sample_df.iloc[i]['body']) - idx_list[-1])\n",
    "    reference_lengths.append(paragraph_lengths)\n",
    "\n",
    "print(f\"Reference lengths: {reference_lengths}\")\n",
    "for i in range(3):\n",
    "    print(f\"Paper {i} min length: {min(reference_lengths[i])}, total length: {sum(reference_lengths[i])}, original body length: {len(sample_df.iloc[i]['body'])}\")\n",
    "    assert sum(reference_lengths[i]) == len(sample_df.iloc[i]['body']), f\"Paper {i} length mismatch: {sum(reference_lengths[i])} != {len(sample_df.iloc[i]['body'])}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06379bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name bert-base-uncased. Creating a new one with mean pooling.\n"
     ]
    }
   ],
   "source": [
    "from segeval import boundary_similarity\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import torch\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "hf_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs={\"device\": \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": False},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b02725c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunked length: 178503\n",
      "Original length: 178503\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "class LengthPreservingChunker(SemanticChunker):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embeddings,\n",
    "        buffer_size: int = 1,\n",
    "        breakpoint_threshold_type: str = \"percentile\",\n",
    "        breakpoint_threshold_amount: float = 95,\n",
    "        number_of_chunks: int = None,\n",
    "        sentence_split_regex: str = r\"(?<=[.?!]\\s)(?=\\S)\",\n",
    "        min_chunk_size: int = 64,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            embeddings=embeddings,\n",
    "            buffer_size=buffer_size,\n",
    "            breakpoint_threshold_type=breakpoint_threshold_type,\n",
    "            breakpoint_threshold_amount=breakpoint_threshold_amount,\n",
    "            number_of_chunks=number_of_chunks,\n",
    "            sentence_split_regex=sentence_split_regex,\n",
    "            min_chunk_size=min_chunk_size,\n",
    "        )\n",
    "        # Warn user if regex consumes chars\n",
    "        zero_width_pattern = re.compile(\n",
    "            r\"\"\"^(?:\n",
    "            \\(\\?<=[^)]*\\)   # positive lookbehind\n",
    "            | \\(\\?<![^)]*\\)   # negative lookbehind\n",
    "            | \\(\\?=[^)]*\\)    # positive lookahead\n",
    "            | \\(\\?![^)]*\\)    # negative lookahead\n",
    "            | [\\^$]           # start/end anchors\n",
    "            | \\\\[bBAZz]       # \\b, \\B, \\A, \\Z, \\z\n",
    "            )*$\"\"\",\n",
    "            re.VERBOSE\n",
    "        )\n",
    "        if not zero_width_pattern.match(sentence_split_regex):\n",
    "            print(\n",
    "                \"Warning: The sentence_split_regex pattern may consume characters. \"\n",
    "                \"This may modify total text length after splitting text.\"\n",
    "            )\n",
    "\n",
    "    def split_text(self, text: str) -> list[str]:\n",
    "        # Warn user if regex consumes chars\n",
    "\n",
    "        single_sentences = re.split(self.sentence_split_regex, text)\n",
    "        # 2. everything else the same up through finding breakpoints...\n",
    "        distances, sentences = self._calculate_sentence_distances(single_sentences)\n",
    "        if self.number_of_chunks is None:\n",
    "            threshold, dist_array = self._calculate_breakpoint_threshold(distances)\n",
    "        else:\n",
    "            threshold = self._threshold_from_clusters(distances)\n",
    "            dist_array = distances\n",
    "\n",
    "        breakpoints = {i for i, d in enumerate(dist_array) if d > threshold}\n",
    "\n",
    "        # 3. build your chunks **without** injecting extra spaces\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        for bp in sorted(breakpoints):\n",
    "            group = sentences[start : bp + 1]\n",
    "            # ← ← ← here’s the only change\n",
    "            combined = \"\".join(d[\"sentence\"] for d in group)\n",
    "            chunks.append(combined)\n",
    "            start = bp + 1\n",
    "\n",
    "        # last tail\n",
    "        if start < len(sentences):\n",
    "            tail = \"\".join(d[\"sentence\"] for d in sentences[start:])\n",
    "            chunks.append(tail)\n",
    "\n",
    "        return chunks\n",
    "\n",
    "\n",
    "# usage\n",
    "chunker = LengthPreservingChunker(\n",
    "    embeddings=hf_embeddings,\n",
    "    breakpoint_threshold_type=\"percentile\",\n",
    "    breakpoint_threshold_amount=95,\n",
    "    min_chunk_size=64,  # in chars\n",
    "    sentence_split_regex=r\"(?<=[.?!]\\s)(?=\\S)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e546cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_params(\n",
    "        model_name: str,\n",
    "        breakpoint_threshold_type: str,\n",
    "        breakpoint_threshold_amount: float,\n",
    "        min_chunk_size: int,):\n",
    "    \"\"\"\n",
    "    Instantiates a chunker with the given parameters and evaluates its boundary similarity on the reference chunks\n",
    "    \"\"\"\n",
    "    # Set up the chunker\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=model_name,\n",
    "        model_kwargs={\"device\": \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\"},\n",
    "        encode_kwargs={\"normalize_embeddings\": False},\n",
    "    )\n",
    "    chunker = LengthPreservingChunker(\n",
    "        embeddings=embeddings,\n",
    "        breakpoint_threshold_type=breakpoint_threshold_type,\n",
    "        breakpoint_threshold_amount=breakpoint_threshold_amount,\n",
    "        min_chunk_size=min_chunk_size,  # in chars\n",
    "    )\n",
    "\n",
    "    scores = []\n",
    "    for i in range(3):\n",
    "        # Get the predicted chunks\n",
    "        chunks = chunker.split_text(sample_df.iloc[i]['body'])\n",
    "        chunk_lengths = [len(chunk) for chunk in chunks]\n",
    "\n",
    "        # Compute the boundary similarity score\n",
    "        score = boundary_similarity(reference_lengths[i], chunk_lengths)\n",
    "        scores.append(float(score))\n",
    "        print(f\"[{model_name}]:({breakpoint_threshold_type}:{breakpoint_threshold_amount}), min chunk size {min_chunk_size}: {score}\")\n",
    "    print(\"=======\")\n",
    "\n",
    "    scores = [float(score) for score in scores]\n",
    "    average_score = sum(scores) / len(scores)\n",
    "    with open(\"data/etc/chunker_scores.csv\", \"a\") as f:\n",
    "        f.write(f\"{model_name},{breakpoint_threshold_type},{breakpoint_threshold_amount},{min_chunk_size},{average_score},{scores}\\n\")\n",
    "    return average_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240f5aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name adsabs/astroBERT. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[adsabs/astroBERT]:(standard_deviation:2.0), min chunk size 64: 0.01041666666666666666666666667\n",
      "[adsabs/astroBERT]:(standard_deviation:2.0), min chunk size 64: 0.01204819277108433734939759036\n",
      "[adsabs/astroBERT]:(standard_deviation:2.0), min chunk size 64: 0.04081632653061224489795918367\n",
      "=======\n",
      "0.02109372865612108\n"
     ]
    }
   ],
   "source": [
    "# scores = evaluate_params(\n",
    "#     model_name=\"adsabs/astroBERT\",\n",
    "#     breakpoint_threshold_type=\"standard_deviation\",\n",
    "#     breakpoint_threshold_amount=2.0,\n",
    "#     min_chunk_size=64,\n",
    "# )\n",
    "# print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b93d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total configurations generated: 1440\n",
      "First 3 configurations:\n",
      "[{'breakpoint_threshold_amount': 1,\n",
      "  'breakpoint_threshold_type': 'percentile',\n",
      "  'min_chunk_size': 50,\n",
      "  'model_name': 'all-MiniLM-L6-v2'},\n",
      " {'breakpoint_threshold_amount': 1,\n",
      "  'breakpoint_threshold_type': 'percentile',\n",
      "  'min_chunk_size': 100,\n",
      "  'model_name': 'all-MiniLM-L6-v2'},\n",
      " {'breakpoint_threshold_amount': 4,\n",
      "  'breakpoint_threshold_type': 'percentile',\n",
      "  'min_chunk_size': 50,\n",
      "  'model_name': 'all-MiniLM-L6-v2'}]\n"
     ]
    }
   ],
   "source": [
    "MODELS = [\n",
    "    # \"all-MiniLM-L6-v2\",\n",
    "    # \"bert-base-uncased\",\n",
    "    \"adsabs/astroBERT\",\n",
    "    \"BAAI/bge-large-en-v1.5\",\n",
    "    \"nasa-impact/nasa-ibm-st.38m\",\n",
    "]\n",
    "breakpoints = {\n",
    "    \"percentile\": [n for n in range(1, 100, 3)],\n",
    "    \"gradient\": [n for n in range(1, 100, 3)],\n",
    "    \"standard_deviation\": [0.1 * n for n in range(1, 40)],\n",
    "    \"interquartile\": [0.1 * n for n in range(1, 40)],\n",
    "}\n",
    "MIN_CHUNK_SIZES = [50, 100]\n",
    "\n",
    "all_configs_kwargs = []\n",
    "\n",
    "# Iterate through models\n",
    "for model in MODELS:\n",
    "    for bp_type, bp_amounts in breakpoints.items():\n",
    "        for bp_amount in bp_amounts:\n",
    "            for chunk_size in MIN_CHUNK_SIZES:\n",
    "                # Create the kwargs dictionary\n",
    "                config_kwargs = {\n",
    "                    \"model_name\": model,\n",
    "                    \"breakpoint_threshold_type\": bp_type,\n",
    "                    \"breakpoint_threshold_amount\": bp_amount,\n",
    "                    \"min_chunk_size\": chunk_size,\n",
    "                }\n",
    "                all_configs_kwargs.append(config_kwargs)\n",
    "\n",
    "# Print the first few configurations to verify\n",
    "print(f\"Total configurations generated: {len(all_configs_kwargs)}\")\n",
    "print(\"First 3 configurations:\")\n",
    "pprint(all_configs_kwargs[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe707214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37d3bdae00b34060a4a53484d25989ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[all-MiniLM-L6-v2]:(percentile:1), min chunk size 50: 0.05423728813559322033898305085\n",
      "[all-MiniLM-L6-v2]:(percentile:1), min chunk size 50: 0.08623853211009174311926605505\n",
      "[all-MiniLM-L6-v2]:(percentile:1), min chunk size 50: 0.09343434343434343434343434343\n",
      "=======\n",
      "0.07797005456000948\n",
      "[all-MiniLM-L6-v2]:(percentile:1), min chunk size 100: 0.05423728813559322033898305085\n",
      "[all-MiniLM-L6-v2]:(percentile:1), min chunk size 100: 0.08623853211009174311926605505\n",
      "[all-MiniLM-L6-v2]:(percentile:1), min chunk size 100: 0.09343434343434343434343434343\n",
      "=======\n",
      "0.07797005456000948\n",
      "[all-MiniLM-L6-v2]:(percentile:4), min chunk size 50: 0.05406976744186046511627906977\n",
      "[all-MiniLM-L6-v2]:(percentile:4), min chunk size 50: 0.08679245283018867924528301887\n",
      "[all-MiniLM-L6-v2]:(percentile:4), min chunk size 50: 0.09553903345724907063197026022\n",
      "=======\n",
      "0.07880041790976607\n"
     ]
    }
   ],
   "source": [
    "for config in all_configs_kwargs:\n",
    "    score = evaluate_params(**config)\n",
    "    print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea28df5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "citeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
