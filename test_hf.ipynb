{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1356ad8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "assert os.getenv(\"HF_ACCESS_TOKEN\") is not None, \"Please set the HF_TOKEN environment variable.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e9882b",
   "metadata": {},
   "source": [
    "Load a CausalLM tokenizer and model. Be sure to set the model to your device and set it to eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0e4e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "model = model.to(\"mps\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26284089",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inputs = tokenizer(\"Tell me about yourself!\\n\\n\", return_tensors=\"pt\").to(\"mps\")\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, num_beams=2, max_new_tokens=1024)\n",
    "\n",
    "decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77f8a48",
   "metadata": {},
   "source": [
    "Try to use a chat interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb33c0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\"role\": \"user\", \"content\": \"Explain LLMs to me\"}]\n",
    "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"mps\")\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, num_beams=4, max_new_tokens=100)\n",
    "decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4b51f77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "653f486ac6f94edbb0e657660a4fa402",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps:0\n",
      "Model pad_token_id: None\n",
      "Setting pad_token to eos_token...\n",
      "Model pad_token_id: 128009\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# First instantiate the tokenizer and model\n",
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ").eval()\n",
    "\n",
    "print(model.device)\n",
    "\n",
    "# Set the pad token on the model\n",
    "print(f\"Model pad_token_id: {model.config.pad_token_id}\")\n",
    "if tokenizer.pad_token is None:\n",
    "    print(\"Setting pad_token to eos_token...\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "print(f\"Model pad_token_id: {model.config.pad_token_id}\")\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=512, do_sample=False)\n",
    "llm_pipeline = HuggingFacePipeline(pipeline=pipe)\n",
    "chat_model = ChatHuggingFace(llm=llm_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5b8d6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/benjaminbasseri/miniforge3/envs/citeline/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/Users/benjaminbasseri/miniforge3/envs/citeline/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Explain LLMs to me\"),\n",
    "]\n",
    "response = chat_model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a431ce10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 17 Apr 2025\\n\\nYou are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nExplain LLMs to me<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nI'd be happy to explain Large Language Models (LLMs) to you.\\n\\n**What are Large Language Models (LLMs)?**\\n\\nLarge Language Models (LLMs) are a type of artificial intelligence (AI) model that are designed to process and understand human language. They are a subset of a broader category of machine learning models known as neural networks.\\n\\n**How do LLMs work?**\\n\\nLLMs are trained on vast amounts of text data, which allows them to learn patterns and relationships within language. This training data can come from various sources, such as books, articles, conversations, and even social media posts. The model's architecture is typically based on a transformer encoder-decoder structure, which is inspired by the human brain's language processing abilities.\\n\\nThe encoder part of the model takes in a sequence of words or tokens and converts them into a numerical representation, called a vector. This vector is then fed into the decoder, which generates text based on the input vector. The decoder uses this vector to predict the next word in a sequence, and this process is repeated until a desired output is reached.\\n\\n**Key characteristics of LLMs:**\\n\\n1. **Scalability**: LLMs can process and understand vast amounts of text data, making them suitable for a wide range of applications.\\n2. **Contextual understanding**: LLMs can understand the context of a conversation or text, allowing them to generate more coherent and relevant responses.\\n3. **Flexibility**: LLMs can be fine-tuned for specific tasks, such as language translation, text summarization, or question-answering.\\n4. **Generative capabilities**: LLMs can generate text that is similar in style and structure to human-written text.\\n\\n**Applications of LLMs:**\\n\\n1. **Virtual assistants**: LLMs are used in virtual assistants like Siri, Alexa, and Google Assistant to understand voice commands and respond accordingly.\\n2. **Language translation**: LLMs are used to translate text from one language to another.\\n3. **Text summarization**: LLMs can summarize long pieces of text into shorter, more digestible versions.\\n4. **Content generation**: LLMs can generate text, such as articles, social media posts, or even entire books.\\n5. **Chatbots**: LLMs are used in chatbots to understand user queries and respond accordingly.\\n\\n**Challenges and limitations of LLMs:**\\n\\n1. **Data quality**: LLMs are only as good as the data they are trained on.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e922125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BitsAndBytesConfig\n",
    "\n",
    "# quantization_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=\"float16\",\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "# )\n",
    "\n",
    "# llm = HuggingFacePipeline.from_model_id(\n",
    "#     model_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
    "#     task=\"text-generation\",\n",
    "#     pipeline_kwargs=dict(\n",
    "#         max_new_tokens=512,\n",
    "#         do_sample=False,\n",
    "#         repetition_penalty=1.03,\n",
    "#         return_full_text=False,\n",
    "#     ),\n",
    "#     model_kwargs={\"quantization_config\": quantization_config},\n",
    "# )\n",
    "\n",
    "# chat_model = ChatHuggingFace(llm=llm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "citeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
